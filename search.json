[
  {
    "objectID": "unidad_6/04_reg_cox.html",
    "href": "unidad_6/04_reg_cox.html",
    "title": "Regresión de Cox",
    "section": "",
    "text": "Hasta el momento hemos visto cómo efectuar estimaciones paramétricas y no paramétricas de la función de supervivencia y de la función de riesgo, que permiten describir la evolución de la “supervivencia” de una población. También hemos visto cómo comparar curvas de supervivencia, al estratificar por las distintas categorías de una variable categórica.\n\n\n\n\n\n\n\n\nEl análisis de supervivencia dispone también de modelos de regresión, que permiten identificar y evaluar la relación entre un conjunto de variables explicativas y el tiempo de supervivencia (variable respuesta), por lo que se la considera parte de los modelos lineales generalizados. A continuación, presentaremos uno de los modelos más usados en salud: el modelo de regresión de Cox, llamado así en honor a su creador (Cox 1972).\nLa regresión de Cox, también llamada modelo de riesgos proporcionales (proportional hazards model) es una técnica muy difundida. Está indicado su uso cuando la variable dependiente esté relacionada con la supervivencia de un grupo de sujetos o, en general, con el tiempo que trascurre hasta que se produce en ellos un suceso o evento. Como ya dijimos, el evento de interés no tiene por qué ser la muerte, puede ser otro tipo de suceso, por ejemplo, el fallo de una prótesis, la incidencia de una enfermedad o la ocurrencia de una complicación en quien ya tiene una patología de base. Se usa para valorar simultáneamente el efecto independiente de una serie de variables explicativas o factores pronósticos sobre esta supervivencia (es decir, sobre la tasa de mortalidad) o sobre la tasa de ocurrencia de otro fenómeno que vaya ocurriendo tras un periodo de tiempo variable en cada sujeto. Como en cualquier modelo de regresión, las variables explicativas pueden ser tanto cuantitativas como categóricas.\nLa regresión de Cox es la extensión multivariada del análisis de supervivencia para evaluar de manera general variables dependientes del tipo “tiempo hasta un suceso o evento” y usa modelos de regresión, próximos al modelo de regresión logística. Es una técnica que permite identificar y evaluar la relación entre un conjunto de variables explicativas y la tasa de ocurrencia del suceso de interés.\nLa mayor utilidad del modelo de regresión de Cox radica en que permite predecir las probabilidades de supervivencia (o, en general de permanencia libre del evento) para un determinado sujeto a partir del patrón de valores que presenten sus variables pronósticas.\nAl igual que en la regresión logística, en la regresión de Cox también es necesaria una función matemática que transforme el desenlace y permita entender la relación entre las variables por medio de un modelo similar al de la línea recta. Y en esto nos concentraremos a continuación."
  },
  {
    "objectID": "unidad_6/04_reg_cox.html#introducción",
    "href": "unidad_6/04_reg_cox.html#introducción",
    "title": "Regresión de Cox",
    "section": "",
    "text": "Hasta el momento hemos visto cómo efectuar estimaciones paramétricas y no paramétricas de la función de supervivencia y de la función de riesgo, que permiten describir la evolución de la “supervivencia” de una población. También hemos visto cómo comparar curvas de supervivencia, al estratificar por las distintas categorías de una variable categórica.\n\n\n\n\n\n\n\n\nEl análisis de supervivencia dispone también de modelos de regresión, que permiten identificar y evaluar la relación entre un conjunto de variables explicativas y el tiempo de supervivencia (variable respuesta), por lo que se la considera parte de los modelos lineales generalizados. A continuación, presentaremos uno de los modelos más usados en salud: el modelo de regresión de Cox, llamado así en honor a su creador (Cox 1972).\nLa regresión de Cox, también llamada modelo de riesgos proporcionales (proportional hazards model) es una técnica muy difundida. Está indicado su uso cuando la variable dependiente esté relacionada con la supervivencia de un grupo de sujetos o, en general, con el tiempo que trascurre hasta que se produce en ellos un suceso o evento. Como ya dijimos, el evento de interés no tiene por qué ser la muerte, puede ser otro tipo de suceso, por ejemplo, el fallo de una prótesis, la incidencia de una enfermedad o la ocurrencia de una complicación en quien ya tiene una patología de base. Se usa para valorar simultáneamente el efecto independiente de una serie de variables explicativas o factores pronósticos sobre esta supervivencia (es decir, sobre la tasa de mortalidad) o sobre la tasa de ocurrencia de otro fenómeno que vaya ocurriendo tras un periodo de tiempo variable en cada sujeto. Como en cualquier modelo de regresión, las variables explicativas pueden ser tanto cuantitativas como categóricas.\nLa regresión de Cox es la extensión multivariada del análisis de supervivencia para evaluar de manera general variables dependientes del tipo “tiempo hasta un suceso o evento” y usa modelos de regresión, próximos al modelo de regresión logística. Es una técnica que permite identificar y evaluar la relación entre un conjunto de variables explicativas y la tasa de ocurrencia del suceso de interés.\nLa mayor utilidad del modelo de regresión de Cox radica en que permite predecir las probabilidades de supervivencia (o, en general de permanencia libre del evento) para un determinado sujeto a partir del patrón de valores que presenten sus variables pronósticas.\nAl igual que en la regresión logística, en la regresión de Cox también es necesaria una función matemática que transforme el desenlace y permita entender la relación entre las variables por medio de un modelo similar al de la línea recta. Y en esto nos concentraremos a continuación."
  },
  {
    "objectID": "unidad_6/04_reg_cox.html#modelo-de-cox",
    "href": "unidad_6/04_reg_cox.html#modelo-de-cox",
    "title": "Regresión de Cox",
    "section": "Modelo de Cox",
    "text": "Modelo de Cox\nEl modelo de Cox es un modelo semi-paramétrico que postula que:\n\\[\nh(t,x) = h_o(t) exp (x_1\\beta_1+x_2\\beta_2+\\dots+x_n\\beta_n)\n\\]\ndonde,\n\\(h (t,x)\\) : representa el riesgo de morir al tiempo \\(t\\) de un sujeto que tiene un patrón determinado de las variables \\(x\\).\n\\(h_0(t)\\): representa el riesgo basal dependiente del \\(t\\).\nEn este caso, como podemos ver, la función que es “transformada” es la función denominada Hazard, que puede simplificarse en términos prácticos como una medida del riesgo instantáneo de ocurrencia de dicho evento.\nComo podemos ver, este modelo asume un presupuesto importante: las covariables tienen un efecto multiplicativo sobre la función de riesgo. ¿Qué significa esto? Esto significa que la razón entre el riesgo de ocurrencia del evento para dos individuos: individuo i e individuo j, por ejemplo, con covariables:\n\\(xi= (xi_1, xi_2, xi_3,\\dots,xi_n)\\) (es el conjunto de covariables del individuo i) \\(xj= (xj_1, xj_2, xj_3,\\dots,xj_n)\\) (es el conjunto de covariables del individuo j) será: \\[\n\\frac {hi(t/xi)} {hj(t/xj)}= \\frac{ho(t)exp(xi_1\\beta_1+xi_2\\beta_2+\\dots xi_n \\beta_n)}{ho(t)exp(xj_1\\beta_1+xj_2 \\beta_2+\\dots xj_n \\beta_n)}\n\\] Entonces:\n\\[\n\\frac {hi(t/xi)} {hj(t/xj)}= \\frac{\\cancel{ho(t)} exp(xi_1\\beta_1+xi_2\\beta_2+\\dots xi_n \\beta_n)}{\\cancel{ho(t)}exp(xj_1\\beta_1+xj_2 \\beta_2+\\dots xj_n \\beta_n)}\n\\]\nO sea que este cociente de riesgos, como podemos ver, es independiente del tiempo. Por este motivo, el modelo de Cox se lo conoce también como Modelo de Riesgos Proporcionales. Es un modelo semi-paramétrico, pues no asume ninguna distribución para la función de riesgo basal \\(ho(t)\\), la única asunción es, como ya dijimos antes, que las covariables tienen un efecto multiplicativo sobre la función de riesgo, y esto constituye la parte paramétrica del modelo.\nEn la regresión de Cox, por lo tanto, el efecto de las variables independientes se presenta como un Hazard Relativo (HR), y expresa la magnitud en la que una variable aumenta o disminuye el riesgo de ocurrencia de un desenlace en el tiempo.\nComo en todo modelo, tendremos que estimar los coeficientes (\\(\\beta\\)) y aprender a interpretarlos. A continuación aprenderemos a construir un modelo de Cox, fijaremos criterios para seleccionar el mejor modelo, evaluaremos el ajuste de dicho modelo y veremos algo de análisis de residuos. Allá vamos!\nEstimación e interpretación de los coeficientes del modelo de Cox\nLos coeficientes del modelo de Cox son estimados por el método de verosimilitud parcial, pero su desarrollo lo omitiremos, dado que los paquetes estadísticos, los calcularán por nosotros. Para comprender su interpretación, consideraremos el modelo más simple, con una única covariable:\n\\[\nh(t,x)= ho(t) e^{\\beta x}\n\\] Si se calcula el cociente entre la tasa de riesgo instantáneo para una variable que toma los valores \\(x=0\\) y \\(x=1\\): \\[\nHR= \\frac{e^{(\\beta*1)}}{e^{(\\beta*0)}}= e^\\beta\n\\]\n¿Cómo interpretamos esto? Sé que ya todos tienen mucha experiencia en interpretar correctamente expresiones de este tipo (notarán ciertas similitudes con la regresión logística), pero nunca está de más reforzar.\n\nSi la variable es dicotómica, por ejemplo consideremos sexo, donde \\(x=1\\) corresponde a masculino y \\(x= 0\\) a femenino, diremos que los varones ven incrementado su riesgo en \\(e\\beta\\) respecto de las mujeres.\nSi la variable fuera continua, como por ejemplo la edad, diremos que un incremento de un año en la edad, aumenta el riesgo en \\(e\\beta\\). Si calculamos el cambio en \\(h\\) para \\(n\\) variables explicativas, y observamos el coeficiente para cuando cada \\(xi\\) se incrementa en una unidad, manteniendo constante al resto, observaremos algo similar.\n\nSi aplicamos ln a la expresión del HR, tendremos que: \\[\nlnHR= \\beta\n\\]\nComo acabamos de ver, la interpretación del modelo de Cox no se hace directamente a través de su coeficiente estimado \\((\\beta)\\) sino del exponencial del coeficiente estimado, \\(exp(\\beta)\\).\nPara variables dicotómicas \\(exp(\\beta)\\) es un estimador de la razón de riesgos (Hazard Ratio) y se interpretará en forma análoga al RR: valores mayores que 1 indican sobreriesgo para el grupo “expuesto”, valores menores a 1 indican factor de protección. Para variables continuas, su interpretación es más difícil ya que no es estrictamente un RR, pero lo podemos pensar como algo similar a un RR para un incremento de la variable continua. Para comprender mejor lo que hemos visto hasta aquí y lo que seguiremos viendo, vamos a recurrir a un ejemplo."
  },
  {
    "objectID": "unidad_6/04_reg_cox.html#ejemplo-práctico-en-r",
    "href": "unidad_6/04_reg_cox.html#ejemplo-práctico-en-r",
    "title": "Regresión de Cox",
    "section": "Ejemplo práctico en R",
    "text": "Ejemplo práctico en R\nRetomaremos el ejemplo sobre trasplante de médula ósea (TMO), que vimos cuando abordamos la estimación de Kaplan Meier. Recordemos que los datos del archivo “tmo.txt“ provienen de esa cohorte de 96 pacientes sometidos a trasplante de médula ósea. Las covariables registradas para cada paciente se describen en la siguiente tabla:\nLas variables presentes en el conjunto de datos son las siguientes:\n\nid: identificador único del paciente\nsexo: sexo biológico (M = masculino, F = femenino)\nedad: edad en años al momento del trasplante\nstatus: estado al final del seguimiento (1 = fallecido, 0 = censurado)\ntiempo: tiempo hasta la fecha de óbito o censura\ndeag: presencia de enfermedad injerto-huesped aguda\ndecr: presencia de enfermedad injerto-huesped crónica\nfase: fase de la LMC (aguda, crónica, crisis blástica)\n\nComenzaremos por activar los paquetes necesarios:\n\nlibrary(survival)\nlibrary(survminer)\nlibrary(easystats)\nlibrary(tidyverse)\n\nLeemos el archivo de datos\n\n# Cargar datos\ndatos &lt;- read_csv2(\"datos/tmo.txt\")\n\n# Modificar datos\ndatos &lt;- datos |&gt; \n  mutate(across(.cols = c(sexo, deag, decr, fase),\n                .fns = ~factor(.x)))\n\nEn general, no se realiza la regresión de Cox con cada una de las variables para decidir cuál incluir en el modelo (puede hacerse para ver el cambio de los coeficientes), sino que esta decisión se toma en base a las curvas de KM, considerando las variables en las que se observan diferencias. Como el modelo de Cox, en general se usa con fines predictivos, pueden ser incluidas otras variables que clínicamente se consideren relevantes.\nVamos a considerar un modelo inicial que incluya todas las variables (m1) y luego usaremos un procedimiento “hacia atrás” manual. El modelo semiparamétrico de riesgo proporcionales de Cox es realizado con la función coxph() del paquete survival:\n\n# Ajustar modelo\nm1 &lt;- coxph(Surv(tiempo, status) ~  fase + decr + deag + sexo + edad, \n            data = datos)\n\n# Salida del modelo\nsummary(m1)\n\nCall:\ncoxph(formula = Surv(tiempo, status) ~ fase + decr + deag + sexo + \n    edad, data = datos)\n\n  n= 96, number of events= 49 \n\n                 coef exp(coef)  se(coef)      z Pr(&gt;|z|)    \nfasecb       0.344438  1.411197  0.542408  0.635 0.525417    \nfasecrónica -0.593973  0.552129  0.371427 -1.599 0.109784    \ndecrsi      -1.061750  0.345850  0.338452 -3.137 0.001706 ** \ndeagsi       1.190381  3.288332  0.327485  3.635 0.000278 ***\nsexoM        0.271984  1.312567  0.332115  0.819 0.412816    \nedad        -0.005019  0.994993  0.014912 -0.337 0.736415    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n            exp(coef) exp(-coef) lower .95 upper .95\nfasecb         1.4112     0.7086    0.4874    4.0859\nfasecrónica    0.5521     1.8112    0.2666    1.1434\ndecrsi         0.3459     2.8914    0.1782    0.6714\ndeagsi         3.2883     0.3041    1.7307    6.2479\nsexoM          1.3126     0.7619    0.6846    2.5166\nedad           0.9950     1.0050    0.9663    1.0245\n\nConcordance= 0.768  (se = 0.03 )\nLikelihood ratio test= 40.67  on 6 df,   p=3e-07\nWald test            = 38.46  on 6 df,   p=9e-07\nScore (logrank) test = 47.62  on 6 df,   p=1e-08\n\n\n\nLa columna “coef” indica los valores de \\(\\beta\\) .\nLa columna “exp (coef)” indica los valores de \\(HR\\).\nLa columna “se (coef)”: desvío estándar de los coeficientes\nZ y Pr (&lt;|Z|): estadístico del Test de Wald y su p-valor.\nlower.95 y upper.95: 95% IC para los \\(HR\\).\n\nEn la parte inferior, destacamos el LR test del modelo.\nAdemás de lo señalado, se agregan el valor de Concordancia así como los estadísticos de prueba, grados de libertad y p-valores de las pruebas de hipótesis de Razón de Verosimilitud, Prueba de Wald y Prueba de Puntajes (score test).\nRepasemos:\n\nEl test de Wald : Testea solo un parámetro, por ejemplo la \\(Ho: \\beta_3=0\\). El test estadístico es \\(Z=\\frac{\\beta}{EE\\beta}\\) que sigue una distribución aproximadamente \\(N(0,1)\\). \\(LR\\) y \\(Z\\) son aproximadamente iguales en grandes muestras, pero pueden diferir en muestras pequeñas.\nLa Deviance puede ser utilizada tanto para obtener una estadística global de ajuste del modelo, como para comparar modelos. Recordemos que: \\(D= 2 (log\\ Likelihood\\ modelo- log\\ Likelihood\\ modelo\\ nulo)\\), donde el Modelo nulo es un modelo teórico sin covariables.\n\nPodemos calcular el Criterio de Información de Akaike (AIC) y también el Criterio de Información Bayesiano (BIC). El AIC sirve para comparar la plausibilidad relativa de un conjunto de modelos. Es decir, dado un conjunto de modelos construidos con los mismos datos, el AIC los ordena según su verosimilitud dados los datos con que se construyen. Este criterio tiene en cuenta tanto el ajuste del modelo como su complejidad de acuerdo a la fórmula: \\(-2*log(-Likelihood) + k*npar\\) donde el primer término mide el ajuste (es la deviance) y el segundo la complejidad (\\(npar\\) es el número de parámetros y \\(k = 2\\)).\nAIC mide lo lejos que está el modelo de la realidad (de un ‘modelo perfecto’), por lo que cuanto menor es el valor, más plausible es el modelo. Existen otros criterios parecidos como el BIC (en BIC \\(k = log(n)\\)).\nAIC no equivale a significación; de hecho este criterio se plantea dentro de una concepción de la estadística distinta –y alternativa– a la fundamentada en testado de hipótesis. El comando es simplemente:\n\n# Criterio de información de Akaike\nAIC(m1)\n\n[1] 378.1393\n\n# Criterio de información Bayesiano\nBIC(m1)\n\n[1] 389.4903\n\n\nEl Likelihood nos da una evaluación cuantitativa de la compatibilidad de los datos con determinado valor del parámetro. \\[\nLikelihood\\ Ratio= \\frac {Likelihood\\ del\\ modelo}{Máximo\\ de\\ Likelihood}\n\\]\n\nEl Likelihood ratio da una medida más conveniente del soporte de los datos para determinado valor de probabilidades que el likelihood, ya que relativiza los valores de los likelihood al valor del máximo likelihood.\nCuanto más cerca de 1 está un Likelihood ratio significa que ese valor de p está más “sustentado” por los datos.\nEl LR test compara dos modelos anidados: modelo mayor vs. modelo reducido. La \\(H_0\\) plantea que algunos parámetros en el modelo completo son igual a 0. Este LR tiene aproximadamente una distribución chi-cuadrado bajo la hipótesis nula.\n\nAnalicemos nuestro modelo\nSolamente los coeficientes que acompañan a las variables deag y decr resultaron significativos según el test de Wald. Esto es lo mismo que observamos en las curvas de KM.\nRespecto de la variable fase, que tiene 3 categorías, observemos que fue transformada en una variable dummy: el modelo ha tomado como categoría de referencia “aguda” y los valores de HR son en referencia a esta categoría: “crisis blástica” respecto a “aguda” y “crónica” respecto a “aguda”.\nLa columna Coefficient representa los valores de \\(\\beta\\) estimados por verosimilitud parcial. Valores positivos de \\(\\beta\\) indican variables que contribuyen a un incremento de riesgo, en tanto que valores negativos de \\(\\beta\\) indican variables que contribuyen a reducir el riesgo. Si miramos los HR, vemos que decr resultó un factor protector importante.\nProbaremos un modelo con cuatro variables ¿Cuál saco del modelo: edad, sexo o fase? Probemos distintos modelos:\n\n\nModelo 2: fase + decr + deag + edad (sacando sexo)\n\n\n# Ajustar modelo\nm2 &lt;- coxph(formula = Surv(tiempo, status) ~ fase + decr + deag + edad, \n            data = datos)\n\n# Salida modelo \nsummary(m2)\n\nCall:\ncoxph(formula = Surv(tiempo, status) ~ fase + decr + deag + edad, \n    data = datos)\n\n  n= 96, number of events= 49 \n\n                coef exp(coef) se(coef)      z Pr(&gt;|z|)    \nfasecb       0.34930   1.41808  0.54010  0.647 0.517801    \nfasecrónica -0.66770   0.51289  0.35813 -1.864 0.062262 .  \ndecrsi      -1.04100   0.35310  0.33283 -3.128 0.001762 ** \ndeagsi       1.15232   3.16554  0.32110  3.589 0.000332 ***\nedad        -0.00272   0.99728  0.01452 -0.187 0.851418    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n            exp(coef) exp(-coef) lower .95 upper .95\nfasecb         1.4181     0.7052    0.4920     4.087\nfasecrónica    0.5129     1.9498    0.2542     1.035\ndecrsi         0.3531     2.8321    0.1839     0.678\ndeagsi         3.1655     0.3159    1.6870     5.940\nedad           0.9973     1.0027    0.9693     1.026\n\nConcordance= 0.758  (se = 0.031 )\nLikelihood ratio test= 39.98  on 5 df,   p=2e-07\nWald test            = 38.28  on 5 df,   p=3e-07\nScore (logrank) test = 47.16  on 5 df,   p=5e-09\n\n# AIC\nAIC(m2)\n\n[1] 376.825\n\n\n\n\nModelo 3: fase + decr + deag + sexo (sacando edad)\n\n\n# Ajustar modelo\nm3 &lt;- coxph(formula = Surv(tiempo, status) ~ fase + decr + deag + sexo, \n            data = datos)\n\n# Salida modelo \nsummary(m3)\n\nCall:\ncoxph(formula = Surv(tiempo, status) ~ fase + decr + deag + sexo, \n    data = datos)\n\n  n= 96, number of events= 49 \n\n               coef exp(coef) se(coef)      z Pr(&gt;|z|)    \nfasecb       0.3378    1.4019   0.5429  0.622 0.533779    \nfasecrónica -0.5715    0.5647   0.3647 -1.567 0.117090    \ndecrsi      -1.0940    0.3349   0.3246 -3.370 0.000750 ***\ndeagsi       1.2075    3.3451   0.3240  3.726 0.000194 ***\nsexoM        0.2509    1.2852   0.3251  0.772 0.440233    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n            exp(coef) exp(-coef) lower .95 upper .95\nfasecb         1.4019     0.7133    0.4837    4.0631\nfasecrónica    0.5647     1.7709    0.2763    1.1540\ndecrsi         0.3349     2.9863    0.1772    0.6326\ndeagsi         3.3451     0.2989    1.7725    6.3128\nsexoM          1.2852     0.7781    0.6796    2.4306\n\nConcordance= 0.762  (se = 0.03 )\nLikelihood ratio test= 40.56  on 5 df,   p=1e-07\nWald test            = 38.19  on 5 df,   p=3e-07\nScore (logrank) test = 47.46  on 5 df,   p=5e-09\n\n# AIC\nAIC(m3)\n\n[1] 376.2529\n\n\n\n\nModelo 4: sexo + decr + deag + edad (sacando fase)\n\n\n# Ajustar modelo\nm4 &lt;- coxph(formula = Surv(tiempo, status) ~ decr + deag + sexo + edad, \n            data = datos)\n\n# Salida modelo \nsummary(m4)\n\nCall:\ncoxph(formula = Surv(tiempo, status) ~ decr + deag + sexo + edad, \n    data = datos)\n\n  n= 96, number of events= 49 \n\n             coef  exp(coef)   se(coef)      z Pr(&gt;|z|)    \ndecrsi -1.3160459  0.2681937  0.3150468 -4.177 2.95e-05 ***\ndeagsi  1.1988240  3.3162147  0.3084393  3.887 0.000102 ***\nsexoM   0.4531812  1.5733092  0.3241990  1.398 0.162158    \nedad    0.0003653  1.0003654  0.0142724  0.026 0.979578    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n       exp(coef) exp(-coef) lower .95 upper .95\ndecrsi    0.2682     3.7286    0.1446    0.4973\ndeagsi    3.3162     0.3015    1.8118    6.0700\nsexoM     1.5733     0.6356    0.8334    2.9701\nedad      1.0004     0.9996    0.9728    1.0287\n\nConcordance= 0.732  (se = 0.032 )\nLikelihood ratio test= 36.35  on 4 df,   p=2e-07\nWald test            = 32.6  on 4 df,   p=1e-06\nScore (logrank) test = 38.43  on 4 df,   p=9e-08\n\n# AIC\nAIC (m4)\n\n[1] 378.4578\n\n\nDe acuerdo al criterio AIC, considerando los modelos construidos, m3 sería un mejor modelo, dado que tiene un menor AIC, pero se observa que existen términos sin significación en este modelo, según el test de Wald.\nAjustemos ahora un modelo con tres variables, partiendo de m3, cuya fórmula era:\n\n\nSurv(tiempo, status) ~ fase + decr + deag + sexo\n&lt;environment: 0x000001c9ae20d7a8&gt;\n\n\nEn general, el criterio que se utiliza para “sacar” variables del modelo, es sacar aquellas cuyo \\(\\beta\\) haya resultado no significativo. En el caso de m3 serían los \\(\\beta\\) correspondientes a las variables sexo y fase.\n\n\nModelo 5: fase + decr + deag (sacando sexo)\n\n\n# Ajustar modelo\nm5 &lt;- coxph(formula = Surv(tiempo, status) ~ fase + decr + deag, data = datos)\n\n# Salida modelo \nsummary(m5)\n\nCall:\ncoxph(formula = Surv(tiempo, status) ~ fase + decr + deag, data = datos)\n\n  n= 96, number of events= 49 \n\n               coef exp(coef) se(coef)      z Pr(&gt;|z|)    \nfasecb       0.3449    1.4119   0.5402  0.639 0.523100    \nfasecrónica -0.6509    0.5216   0.3467 -1.878 0.060440 .  \ndecrsi      -1.0597    0.3466   0.3176 -3.336 0.000849 ***\ndeagsi       1.1633    3.2004   0.3160  3.681 0.000232 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n            exp(coef) exp(-coef) lower .95 upper .95\nfasecb         1.4119     0.7083    0.4898    4.0698\nfasecrónica    0.5216     1.9173    0.2644    1.0290\ndecrsi         0.3466     2.8856    0.1860    0.6458\ndeagsi         3.2004     0.3125    1.7227    5.9456\n\nConcordance= 0.753  (se = 0.03 )\nLikelihood ratio test= 39.95  on 4 df,   p=4e-08\nWald test            = 38.18  on 4 df,   p=1e-07\nScore (logrank) test = 47.11  on 4 df,   p=1e-09\n\n# AIC\nAIC(m5)\n\n[1] 374.8601\n\n\n\n\nModelo 6: decr + deag + sexo (sacando fase)\n\n\n# Ajustar modelo\nm6 &lt;- coxph(formula = Surv(tiempo, status) ~ decr + deag + sexo,  data = datos)\n\n# Salida modelo \nsummary(m6)\n\nCall:\ncoxph(formula = Surv(tiempo, status) ~ decr + deag + sexo, data = datos)\n\n  n= 96, number of events= 49 \n\n          coef exp(coef) se(coef)      z Pr(&gt;|z|)    \ndecrsi -1.3143    0.2687   0.3072 -4.279 1.88e-05 ***\ndeagsi  1.1977    3.3124   0.3051  3.925 8.66e-05 ***\nsexoM   0.4555    1.5769   0.3115  1.462    0.144    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n       exp(coef) exp(-coef) lower .95 upper .95\ndecrsi    0.2687     3.7220    0.1472    0.4906\ndeagsi    3.3124     0.3019    1.8215    6.0236\nsexoM     1.5769     0.6341    0.8564    2.9037\n\nConcordance= 0.74  (se = 0.031 )\nLikelihood ratio test= 36.35  on 3 df,   p=6e-08\nWald test            = 32.6  on 3 df,   p=4e-07\nScore (logrank) test = 38.43  on 3 df,   p=2e-08\n\n# AIC\nAIC(m6)\n\n[1] 376.4584\n\n\nEl m5 parece ser un mejor modelo, con un AIC menor (374.8600995).\nEnsayemos ahora un modelo con 2 variables, a partir entonces de m5:\n\n\nSurv(tiempo, status) ~ fase + decr + deag\n&lt;environment: 0x000001c9b0931738&gt;\n\n\nSaco fase, dado que su coeficiente resultó no significativo.\n\n\nModelo 7: deag + decr\n\n\n\n# Ajustar modelo\nm7 &lt;- coxph(formula = Surv(tiempo, status) ~ decr + deag, data = datos)\n\n# Salida modelo \nsummary(m7)\n\nCall:\ncoxph(formula = Surv(tiempo, status) ~ decr + deag, data = datos)\n\n  n= 96, number of events= 49 \n\n          coef exp(coef) se(coef)      z Pr(&gt;|z|)    \ndecrsi -1.2532    0.2856   0.3038 -4.125 3.71e-05 ***\ndeagsi  1.1001    3.0045   0.2959  3.717 0.000201 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n       exp(coef) exp(-coef) lower .95 upper .95\ndecrsi    0.2856     3.5015    0.1574     0.518\ndeagsi    3.0045     0.3328    1.6822     5.366\n\nConcordance= 0.739  (se = 0.03 )\nLikelihood ratio test= 34.13  on 2 df,   p=4e-08\nWald test            = 32.9  on 2 df,   p=7e-08\nScore (logrank) test = 37.62  on 2 df,   p=7e-09\n\n# AIC\nAIC (m7)\n\n[1] 376.6813\n\n\nAhora todas las variables tienen significación y, si bien no ha habido mejora en el AIC, según los valores del test de Wald, parece un mejor modelo. No obstante, podemos comparar los modelos “candidatos”.\nLa comparación entre los modelos se realiza con la función anova(), que realiza la prueba de Razón de Verosimilitud entre dos modelos. Los argumentos de la función son los dos modelos ajustados a comparar. La salida de la función es una tabla en donde muestra para los dos modelos el log-verosimilitud, el estadístico Chisq, los grados de libertad Df y el p-valor P(&gt;|Chi|) de la prueba.\nAhora, seleccionamos los dos modelos a comparar en nuestro caso: m5 y m7.\n\nanova(m5, m7)\n\nAnalysis of Deviance Table\n Cox model: response is  Surv(tiempo, status)\n Model 1: ~ fase + decr + deag\n Model 2: ~ decr + deag\n   loglik  Chisq Df Pr(&gt;|Chi|)  \n1 -183.43                       \n2 -186.34 5.8212  2    0.05444 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nRecordemos que los modelos deben ser anidados para poder hacer ANOVA. En este caso, lo que hace es comparar los log de Likelihood de ambos modelos. Esa diferencia se distribuye como un Chi cuadrado. El valor de p se encuentra en el límite de significación (considerando 0,05). Si somos estrictos, el valor de p indica que no hay diferencia entre ambos modelos, por lo tanto nos quedaríamos con el de menos variables, de acuerdo al principio de parsimonia.\nEn nuestro caso, podemos decidir quedarnos con el m5, dado que la fase de la enfermedad puede resultar un elemento predictivo importante. Como se indicó anteriormente, la regresión de Cox generalmente tiene fines predictivos, así que podemos ver en la bibliografía, modelos que no siguen todas las “normas” del modelado, porque se hace importante mantener variables como edad o sexo, aún cuando no resulten significativas, a fin de contar con intervalos de predicción por sexo o grupo etario.\nSupongamos entonces que decidimos quedarnos con el m5, aunque rompamos algunas reglas. Interpretemos ahora los parámetros del modelo:\n\nsummary(m5)\n\nCall:\ncoxph(formula = Surv(tiempo, status) ~ fase + decr + deag, data = datos)\n\n  n= 96, number of events= 49 \n\n               coef exp(coef) se(coef)      z Pr(&gt;|z|)    \nfasecb       0.3449    1.4119   0.5402  0.639 0.523100    \nfasecrónica -0.6509    0.5216   0.3467 -1.878 0.060440 .  \ndecrsi      -1.0597    0.3466   0.3176 -3.336 0.000849 ***\ndeagsi       1.1633    3.2004   0.3160  3.681 0.000232 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n            exp(coef) exp(-coef) lower .95 upper .95\nfasecb         1.4119     0.7083    0.4898    4.0698\nfasecrónica    0.5216     1.9173    0.2644    1.0290\ndecrsi         0.3466     2.8856    0.1860    0.6458\ndeagsi         3.2004     0.3125    1.7227    5.9456\n\nConcordance= 0.753  (se = 0.03 )\nLikelihood ratio test= 39.95  on 4 df,   p=4e-08\nWald test            = 38.18  on 4 df,   p=1e-07\nScore (logrank) test = 47.11  on 4 df,   p=1e-09\n\n\nPara la interpretación, nos centramos en la columna exp(coef), que corresponde a los HR. Partiendo de dichos valores, podemos concluir:\n\nEl riesgo de muerte de los pacientes que padecen enfermedad injerto contra huésped aguda es aproximadamente el triple de aquellos que no la han padecido (\\(HR = 3,2\\)).\nLos pacientes que hayan padecido enfermedad injerto contra huésped crónica tienen 0,34 veces el riesgo de morir post datos de los que no la tuvieron. Puede decirse también (y quizás quedaría mejor expresado) que tienen un 66% menos riesgo que los que no tuvieron enfermedad injerto contra huésped crónica (\\(HR = 0,34\\)).\nLos pacientes que reciben trasplante en la fase crónica de la enfermedad, tienen un riesgo 48% menor de morir post datos que aquellos que son trasplantados en la fase aguda (si consideramos un \\(p&lt; 0,1\\), este término tendría significación). Se observó un aumento de riesgo en los que reciben trasplante en la crisis blástica respecto de quienes lo reciben en la aguda, pero este término no resultó significativo.\nAlgunas alternativas\nComo hemos visto para modelados anteriores, podemos construir el modelo de Cox con funciones del paquete stats y del paquete performance (Lüdecke et al. 2021).\nLes recordamos que para la comparación de modelos con la función compare_performance() van a observar que se utiliza el indicador AIC weights en lugar del AIC. El AIC weights es un parámetro que cuantifica de manera relativa la calidad de un modelo. Refleja la probabilidad relativa de que un modelo sea el mejor entre un conjunto de modelos candidatos. Su cálculo se basa en la comparación de los valores de AIC de cada modelo considerado.\nPor esto, se interpreta de manera inversa al AIC: a mayor valor de AIC weights, mayor probabilidad de que el modelo sea un mejor candidato en comparación al resto.\nLes dejamos a continuación el proceso backwards utilizando funciones de estos paquetes.\n\n## Selección backwards a partir del modelo saturado\nm1 &lt;- coxph(Surv(tiempo, status) ~  fase + decr + deag + sexo + edad, \n            data = datos)\n\ndrop1(m1)\n\nSingle term deletions\n\nModel:\nSurv(tiempo, status) ~ fase + decr + deag + sexo + edad\n       Df    AIC\n&lt;none&gt;    378.14\nfase    2 378.46\ndecr    1 386.29\ndeag    1 389.22\nsexo    1 376.82\nedad    1 376.25\n\n# (-) sexo\nfit_m2 &lt;- update(m1, ~.-sexo)\n\n# (-) edad\nfit_m3 &lt;- update(m1, ~.-edad)\n\n# (-) fase\nfit_m4 &lt;- update(m1, ~.-fase)\n\n# Salida de los modelos\nsummary(fit_m2)\n\nCall:\ncoxph(formula = Surv(tiempo, status) ~ fase + decr + deag + edad, \n    data = datos)\n\n  n= 96, number of events= 49 \n\n                coef exp(coef) se(coef)      z Pr(&gt;|z|)    \nfasecb       0.34930   1.41808  0.54010  0.647 0.517801    \nfasecrónica -0.66770   0.51289  0.35813 -1.864 0.062262 .  \ndecrsi      -1.04100   0.35310  0.33283 -3.128 0.001762 ** \ndeagsi       1.15232   3.16554  0.32110  3.589 0.000332 ***\nedad        -0.00272   0.99728  0.01452 -0.187 0.851418    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n            exp(coef) exp(-coef) lower .95 upper .95\nfasecb         1.4181     0.7052    0.4920     4.087\nfasecrónica    0.5129     1.9498    0.2542     1.035\ndecrsi         0.3531     2.8321    0.1839     0.678\ndeagsi         3.1655     0.3159    1.6870     5.940\nedad           0.9973     1.0027    0.9693     1.026\n\nConcordance= 0.758  (se = 0.031 )\nLikelihood ratio test= 39.98  on 5 df,   p=2e-07\nWald test            = 38.28  on 5 df,   p=3e-07\nScore (logrank) test = 47.16  on 5 df,   p=5e-09\n\nsummary(fit_m3)\n\nCall:\ncoxph(formula = Surv(tiempo, status) ~ fase + decr + deag + sexo, \n    data = datos)\n\n  n= 96, number of events= 49 \n\n               coef exp(coef) se(coef)      z Pr(&gt;|z|)    \nfasecb       0.3378    1.4019   0.5429  0.622 0.533779    \nfasecrónica -0.5715    0.5647   0.3647 -1.567 0.117090    \ndecrsi      -1.0940    0.3349   0.3246 -3.370 0.000750 ***\ndeagsi       1.2075    3.3451   0.3240  3.726 0.000194 ***\nsexoM        0.2509    1.2852   0.3251  0.772 0.440233    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n            exp(coef) exp(-coef) lower .95 upper .95\nfasecb         1.4019     0.7133    0.4837    4.0631\nfasecrónica    0.5647     1.7709    0.2763    1.1540\ndecrsi         0.3349     2.9863    0.1772    0.6326\ndeagsi         3.3451     0.2989    1.7725    6.3128\nsexoM          1.2852     0.7781    0.6796    2.4306\n\nConcordance= 0.762  (se = 0.03 )\nLikelihood ratio test= 40.56  on 5 df,   p=1e-07\nWald test            = 38.19  on 5 df,   p=3e-07\nScore (logrank) test = 47.46  on 5 df,   p=5e-09\n\nsummary(fit_m4)\n\nCall:\ncoxph(formula = Surv(tiempo, status) ~ decr + deag + sexo + edad, \n    data = datos)\n\n  n= 96, number of events= 49 \n\n             coef  exp(coef)   se(coef)      z Pr(&gt;|z|)    \ndecrsi -1.3160459  0.2681937  0.3150468 -4.177 2.95e-05 ***\ndeagsi  1.1988240  3.3162147  0.3084393  3.887 0.000102 ***\nsexoM   0.4531812  1.5733092  0.3241990  1.398 0.162158    \nedad    0.0003653  1.0003654  0.0142724  0.026 0.979578    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n       exp(coef) exp(-coef) lower .95 upper .95\ndecrsi    0.2682     3.7286    0.1446    0.4973\ndeagsi    3.3162     0.3015    1.8118    6.0700\nsexoM     1.5733     0.6356    0.8334    2.9701\nedad      1.0004     0.9996    0.9728    1.0287\n\nConcordance= 0.732  (se = 0.032 )\nLikelihood ratio test= 36.35  on 4 df,   p=2e-07\nWald test            = 32.6  on 4 df,   p=1e-06\nScore (logrank) test = 38.43  on 4 df,   p=9e-08\n\n# Comparación de modelos\ncompare_performance(m1, fit_m2, fit_m3, fit_m4, \n                    metrics = \"AIC\", \n                    rank = T)\n\n# Comparison of Model Performance Indices\n\nName   | Model | AIC weights | Performance-Score\n------------------------------------------------\nfit_m3 | coxph |       0.404 |           100.00%\nfit_m2 | coxph |       0.304 |            62.75%\nm1     | coxph |       0.157 |             8.58%\nfit_m4 | coxph |       0.134 |             0.00%\n\n## Selección a partir de fit_m3\ndrop1(fit_m3)\n\nSingle term deletions\n\nModel:\nSurv(tiempo, status) ~ fase + decr + deag + sexo\n       Df    AIC\n&lt;none&gt;    376.25\nfase    2 376.46\ndecr    1 386.02\ndeag    1 387.91\nsexo    1 374.86\n\n# (-) sexo\nfit_m5 &lt;- update(fit_m3, ~.-sexo)\n\n# (-) fase\nfit_m6 &lt;- update(fit_m3, ~.-fase)\n\n# Salida de los modelos\nsummary(fit_m5)\n\nCall:\ncoxph(formula = Surv(tiempo, status) ~ fase + decr + deag, data = datos)\n\n  n= 96, number of events= 49 \n\n               coef exp(coef) se(coef)      z Pr(&gt;|z|)    \nfasecb       0.3449    1.4119   0.5402  0.639 0.523100    \nfasecrónica -0.6509    0.5216   0.3467 -1.878 0.060440 .  \ndecrsi      -1.0597    0.3466   0.3176 -3.336 0.000849 ***\ndeagsi       1.1633    3.2004   0.3160  3.681 0.000232 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n            exp(coef) exp(-coef) lower .95 upper .95\nfasecb         1.4119     0.7083    0.4898    4.0698\nfasecrónica    0.5216     1.9173    0.2644    1.0290\ndecrsi         0.3466     2.8856    0.1860    0.6458\ndeagsi         3.2004     0.3125    1.7227    5.9456\n\nConcordance= 0.753  (se = 0.03 )\nLikelihood ratio test= 39.95  on 4 df,   p=4e-08\nWald test            = 38.18  on 4 df,   p=1e-07\nScore (logrank) test = 47.11  on 4 df,   p=1e-09\n\nsummary(fit_m6)\n\nCall:\ncoxph(formula = Surv(tiempo, status) ~ decr + deag + sexo, data = datos)\n\n  n= 96, number of events= 49 \n\n          coef exp(coef) se(coef)      z Pr(&gt;|z|)    \ndecrsi -1.3143    0.2687   0.3072 -4.279 1.88e-05 ***\ndeagsi  1.1977    3.3124   0.3051  3.925 8.66e-05 ***\nsexoM   0.4555    1.5769   0.3115  1.462    0.144    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n       exp(coef) exp(-coef) lower .95 upper .95\ndecrsi    0.2687     3.7220    0.1472    0.4906\ndeagsi    3.3124     0.3019    1.8215    6.0236\nsexoM     1.5769     0.6341    0.8564    2.9037\n\nConcordance= 0.74  (se = 0.031 )\nLikelihood ratio test= 36.35  on 3 df,   p=6e-08\nWald test            = 32.6  on 3 df,   p=4e-07\nScore (logrank) test = 38.43  on 3 df,   p=2e-08\n\n# Comparación de modelos\ncompare_performance(fit_m3, fit_m5, fit_m6, \n                    metrics = \"AIC\", \n                    rank = T)\n\n# Comparison of Model Performance Indices\n\nName   | Model | AIC weights | Performance-Score\n------------------------------------------------\nfit_m5 | coxph |       0.513 |           100.00%\nfit_m3 | coxph |       0.256 |             8.85%\nfit_m6 | coxph |       0.231 |             0.00%\n\n## Selección a partir de fit_m5\ndrop1(fit_m5)\n\nSingle term deletions\n\nModel:\nSurv(tiempo, status) ~ fase + decr + deag\n       Df    AIC\n&lt;none&gt;    374.86\nfase    2 376.68\ndecr    1 384.27\ndeag    1 386.00\n\n# fit_m5 (-) fase\nfit_m7 &lt;- update(fit_m5, ~.-fase)\n\nsummary(fit_m7)\n\nCall:\ncoxph(formula = Surv(tiempo, status) ~ decr + deag, data = datos)\n\n  n= 96, number of events= 49 \n\n          coef exp(coef) se(coef)      z Pr(&gt;|z|)    \ndecrsi -1.2532    0.2856   0.3038 -4.125 3.71e-05 ***\ndeagsi  1.1001    3.0045   0.2959  3.717 0.000201 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n       exp(coef) exp(-coef) lower .95 upper .95\ndecrsi    0.2856     3.5015    0.1574     0.518\ndeagsi    3.0045     0.3328    1.6822     5.366\n\nConcordance= 0.739  (se = 0.03 )\nLikelihood ratio test= 34.13  on 2 df,   p=4e-08\nWald test            = 32.9  on 2 df,   p=7e-08\nScore (logrank) test = 37.62  on 2 df,   p=7e-09\n\n# Comparación de modelos\ncompare_performance(fit_m5, fit_m7, \n                    metrics = \"AIC\", \n                    rank = T)\n\n# Comparison of Model Performance Indices\n\nName   | Model | AIC weights | Performance-Score\n------------------------------------------------\nfit_m5 | coxph |       0.713 |           100.00%\nfit_m7 | coxph |       0.287 |             0.00%\n\n\nPor otro lado, recordemos que R permite también hacer una selección automática de los modelos, tanto para supervivencia como para cualquier tipo de modelado. Normalmente la bibliografía no recomienda los procedimientos automáticos, pero tengamos en claro lo que ocurre en cada caso:\n\n“hacia atrás / adelante” (el valor por defecto): la selección comienza con el modelo completo y elimina los predictores uno a la vez, en cada paso, considerando si el criterio será mejorado agregando de nuevo una variable eliminada en un paso anterior.\n“hacia adelante / hacia atrás”, la selección comienza con un modelo que incluye sólo una constante, y añade predictores uno a la vez, en cada paso considerando si el criterio se mejorará mediante la eliminación de una variable agregada anteriormente.\n“atrás” y “adelante” son similares sin la reconsideración en cada paso.\nPredicción de la curva de supervivencia\nAhora, estaríamos en condiciones de obtener la función de supervivencia ajustada mediante el modelo de Cox seleccionado. La función survfit() del paquete survival calcula la función de supervivencia pronosticada para un modelo de riesgos proporcionales de Cox.\nPara ello, la función de R es:\n\nsurvfit(m5) |&gt; \n  summary()\n\nPodríamos comparar esto con lo obtenido en la estimación de Kaplan-Meier (ver módulo de KM). Pero, siempre resulta más claro realizar la comparación en forma gráfica: graficaremos la función de supervivencia obtenida mediante el estimador de Kaplan-Meier y la obtenida mediante el modelo de Cox seleccionado. Para esto, primero tendremos que crear una lista con las funciones de supervivencia requeridas para luego graficarlas utilizando la función ggsurvplot_combine() del paquete survminer (Kassambara, Kosinski, y Biecek 2024).\n\n# Creamos un objeto con la estimación de KM\nglobal &lt;- Surv(datos$tiempo, datos$status) \n\n# Creamos una lista con la función de supervivencia del modelo final\n# seleccionado y la función del estimador de KM\nfit &lt;- list(modelo = survfit(m5), \n            global = survfit(global ~ 1)) \n\n# Generamos las curvas de supervivencia\nggsurvplot_combine(fit, \n                   data = datos,\n                   ggtheme = theme_bw(),\n                   title = \"Comparación del ajuste del modelo de Cox y el estimador de KM\",\n                   xlab = \"Días\",\n                   ylab = \"Supervivencia\",\n                   legend.title = \"\",\n                   legend.labs = c(\"Ajuste por Cox\", \"Estimador de KM\")\n                   )\n\n\n\n\n\n\n\nPodemos observar que el modelo seleccionado, ajusta bastante bien (sobreestima al principio, subestima al final). Sin embargo, en lo que a modelado respecta, para evaluar la calidad de ajuste contamos con parámetros adicionales.\nCalidad de ajuste de un modelo\nHay pocas medidas para evaluar el ajuste de un modelo de supervivencia. Una de las más simples es \\(R^2\\), cuya expresión es: \\[\nR^2_{LR}= 1- \\bigg(\\frac{Likelihood(modelo\\ nulo)}{Likelihood(modelo\\ ajustado)} \\bigg )^ \\frac{2}{n}\n\\]\nSu interpretación es similar a la del \\(R^2\\) de los otros modelos lineales:\n\\(R^2\\) puede ser interpretado como el poder explicativo de las covariables en el tiempo de ocurrencia del evento en estudio.\n\\(R^2\\) tiene valores bajos en los análisis de supervivencia (alrededor de 30% de poder explicativo), por lo que muchos autores no lo consideran apropiado. De hecho, en las salidas de R de los modelos, observamos que no lo muestra actualmente, siendo que antes sí lo ofrecía, atendiendo a estas y otras cuestiones teóricas. En su lugar, ofrece el valor de concordancia. Profundizaremos un poco sobre este concepto, ya que no lo hemos visto en otros modelos.\nEl uso de la estadística de concordancia para los modelos de Cox fue popularizado por Harrel (Harrell, Lee, y Mark 1996), y ahora es la medida más usada como bondad de ajuste en los modelos de supervivencia. Una ventaja de la estadística es que está bien definida, no solo para los modelos de supervivencia, sino también para la regresión lineal logística y ordinaria.\nEn general, si \\(yi\\) y \\(xi\\) son los valores observados y predichos por el modelo, respectivamente, la concordancia se define como \\(P(x_i &gt; x_j|y_i&gt; y_j)\\), la probabilidad de que la predicción \\(x\\) vaya en la misma dirección que los datos reales \\(y\\). Un par de las observaciones \\(i, j\\) se consideran concordantes si la predicción y los datos van en la misma dirección. La concordancia es la fracción de pares concordantes.\nEn general, se acepta la siguiente escala:\n\n\n\n\nConcordancia (c)\nPoder discriminatorio\n\n\n\n0.3 &lt; c &lt; 0.4\nBajo\n\n\nc = 0.5\nPor azar\n\n\n0.6 &lt; c &lt; 0.7\nComún\n\n\n0.7 &lt; c &lt; 0.8\nMuy bueno\n\n\n0.8 &lt; c &lt; 0.9\nExcelente\n\n\n\n\n\nSi observamos, el valor de concordancia para el modelo m5 (0.753), lo que indica muy buena concordancia entre lo observado y lo predicho por el modelo.\nAnálisis de residuos\nComo ustedes recordarán, se denomina residuo a la diferencia entre el valor observado y el valor estimado por la ecuación de regresión, es decir a lo que la ecuación de regresión deja sin explicar para cada observación.\nLa definición de residuo en el análisis de supervivencia no es tan simple y directa como esta definición, dado, que, por ejemplo, un residuo obtenido como respuesta observada menos esperada, no tiene en consideración el tiempo observado de un individuo censurado.\nEn el análisis de supervivencia, se describen distintos tipos de residuos que sirven para evaluar distintos aspectos del modelo:\n\n\n\n\nEvaluación\nTipo de residuo\n\n\n\nVerificar riesgos proporcionales\nResiduos de Schoenfeld\n\n\nEvaluar si es adecuado incluir las variables en forma lineal en el modelo\nResiduos Martingale\n\n\nDetectar residuos grandes (outliers)\nResiduos de Deviance\n\n\nImpacto que tiene cada observación en el modelo, tanto en forma global como por variable\nResiduos Score\n\n\n\n\n\nY ahora algo más… para covariables con pocas categorías, las curvas de KM estratificadas dan una idea si se cumple el presupuesto de riesgos proporcionales de Cox. Curvas que son razonablemente paralelas a lo largo del tiempo indican proporcionalidad de riesgo entre las categorías y, por el contrario, el entrecruzamiento o una variación en las distancias entre las categorías, podría indicar falta de proporcionalidad.\nSi tienen presentes las curvas de KM que construimos para este ejemplo, en todas las curvas que se muestran, parece razonable suponer que se cumple el supuesto de proporcionalidad, aún en el caso de la variable sexo, dado que ambas categorías están prácticamente superpuestas, separándose recién al final, cuando quedan pocas observaciones.\nA continuación, abordaremos el análisis de residuos, al menos de algunos de ellos. Para el desarrollo de gráficos seguiremos explorando el paquete survminer. Les aclaramos, además, que no utilizaremos la función check_model() del paquete performance para esta ocasión dado que no permite aún el análisis de residuales para modelos de Cox.\nResiduos de Schoenfeld\nPodríamos preguntarnos si el efecto de cada covariable será el mismo a medida que transcurra el tiempo, porque si no es así, se trataría de covariables tiempo dependientes. Para responder esta cuestión, se calculan los Residuos de Schoenfeld.\nSe genera un residuo para cada variable y para cada paciente, es decir que si tenemos un modelo de Cox con tres factores pronóstico se calcularán 3 residuos de Schoenfeld por paciente. Estos residuos valen cero para las observaciones incompletas. Es posible modificar estos residuos con el fin de que no valgan cero para las observaciones incompletas, obteniéndose entonces los denominados residuos Schoenfeld corregidos o escalados.\nEn R, la función cox.zph() realiza la prueba de hipótesis de correlación para los residuos de Schoenfeld de cada covariable con alguna transformación del tiempo, los argumentos de la función son:\n\nfit: Un objeto coxph resultado de un ajuste del modelo de regresión de Cox.\ntransform: Tipo de transformación del tiempo, posibles valores son \"km\", \"rank\", \"identity\" o una función.\nglobal: una prueba chi cuadrado global.\n\nLa salida de la función es una tabla en donde para cada covariable calcula el coeficiente \\(rho\\), el estadístico de prueba chisq y su respectivo p-valor, además incluye una prueba global. Por ejemplo, para el m5 o para m7, el supuesto de riesgos proporcionales puede ser verificado mediante el contraste de hipótesis generado mediante el comando:\n\ncox.zph(m5, \n        transform = \"km\", \n        global = TRUE)\n\n        chisq df       p\nfase    9.308  2 0.00952\ndecr    7.834  1 0.00513\ndeag    0.279  1 0.59742\nGLOBAL 18.546  4 0.00097\n\n\n¿Qué significa esta salida?\nLa \\(H_0\\) que se testea es que la correlación lineal entre el residuo de Schoenfeld y el tiempo de supervivencia es cero.\nEsto es equivalente a probar \\(H_0\\): pendiente igual a cero o $H_0: logHR$ es constante en el tiempo.\nDe lo expuesto se concluye que existe evidencia significativa al 5% de que se viole el supuesto de riesgos proporcionales, desde el punto de vista global, y para las covariables decr y fase.\nLa validación del supuesto de riesgos proporcionales puede realizarse también en forma gráfica con la función ggcoxdiagnostics():\n\nggcoxdiagnostics(m5, \n                 type = \"schoenfeld\", \n                 title = \"Residuos de Schonfeld\",\n                 ylab = \"Residuos\",\n                 xlab = \"Tiempo\")\n\n\n\n\n\n\n\nLa interpretación se facilita con el agregado de una línea de tendencia “lowess”, de esta forma, cuando esta línea se desvía de una línea constante, se sospecha una estructura tiempo dependiente en el efecto de la covariable.\nLos gráficos confirman lo que hallamos con el test: los residuos tienen un patrón o estructura con el tiempo. En el caso de la variable decr, parece visualizarse una relación lineal, lo mismo en fase:cb.\nFrente al problema de No proporcionalidad, se deberá evaluar:\n\nMagnitud\nPuntos influyentes\n\n¿Cuáles son las posibles soluciones? Estratificar por covariable dependiente del tiempo, dividir el eje del tiempo o utilizar otro tipo de modelo.\nContinuaremos analizando los residuos de este modelo y retomaremos el tema cuando lleguemos a modelos estratificados.\nResiduos Martingale\nEl residuo Martingale de un sujeto es simplemente la diferencia entre el número de acontecimientos observados y el número de acontecimientos esperados según el modelo.\nSe grafica: Mi vs índice de individuo, permitiendo revelar sujetos mal ajustados por el modelo. Nuevamente nos será de utilidad la función ggcoxdiagnostics():\n\nggcoxdiagnostics(m5, \n                 type = \"martingale\", \n                 title = \"Residuos de Martingale\",\n                 ylab = \"Residuos\",\n                 xlab = \"Índice\"\n                 )\n\n\n\n\n\n\n\nEste gráfico sirve para evidenciar posibles valores aberrantes: individuos que demoraron mucho o demasiado poco en tener el evento (como los que señalan las flechas). Se espera que se distribuyan en torno a la línea punteada del 0, sin ningún patrón remanente. Valores de Mi mayores que 0, indica que el numero de eventos observados es menor que los estimados por el modelo y viceversa."
  },
  {
    "objectID": "unidad_6/04_reg_cox.html#modelo-de-cox-estratificado",
    "href": "unidad_6/04_reg_cox.html#modelo-de-cox-estratificado",
    "title": "Regresión de Cox",
    "section": "Modelo de Cox estratificado",
    "text": "Modelo de Cox estratificado\nEn algunas situaciones no podemos asumir que el riesgo basal \\(h_o(t)\\) sea el mismo para todos los individuos del estudio. Esto podría ser por la característica propia de la variable (como la edad) o porque el diseño sea estratificado a priori (por ej. estadíos de una enfermedad). En estos casos, los distintos \\(h_o(t)\\) definen los distintos estratos.\nLos coeficientes \\(\\beta\\) también se estiman por el método de verosimilitud parcial, pero los individuos en riesgo que participan de la estimación son sólo los del estrato.\nEl modelo de Cox estratificado constituye una de las maneras de corregir el modelo de Cox cuando no se cumple el supuesto de riesgos proporcionales para alguna de las covariables. En este caso suele correrse el modelo estratificando por la covariable que no cumple con el supuesto de riesgo proporcional. Este procedimiento permite corregir el sesgo en la estimación del parámetro, que puede presentarse cuando se viola el supuesto de riesgo proporcional. Sin embargo, presenta una desventaja y es que no existe ningún \\(\\beta\\) que permita estimar el efecto de la covariable de estratificación.\nContinuando con nuestro ejemplo, vimos que la variable decr violaba el supuesto de proporcionalidad. Deberíamos entonces usar un modelo de Cox estratificado, donde decr sea la variable de estratificación.\n\nm5st &lt;- coxph(formula = Surv(tiempo, status) ~ fase + deag + strata(decr), data = datos)\n\n# Salida del modelo\nsummary(m5st)\n\nCall:\ncoxph(formula = Surv(tiempo, status) ~ fase + deag + strata(decr), \n    data = datos)\n\n  n= 96, number of events= 49 \n\n                coef exp(coef) se(coef)      z Pr(&gt;|z|)    \nfasecb       0.09209   1.09647  0.54271  0.170 0.865250    \nfasecrónica -0.72569   0.48399  0.35111 -2.067 0.038747 *  \ndeagsi       1.12109   3.06819  0.32007  3.503 0.000461 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n            exp(coef) exp(-coef) lower .95 upper .95\nfasecb          1.096     0.9120    0.3785    3.1765\nfasecrónica     0.484     2.0662    0.2432    0.9632\ndeagsi          3.068     0.3259    1.6385    5.7454\n\nConcordance= 0.646  (se = 0.041 )\nLikelihood ratio test= 17.02  on 3 df,   p=7e-04\nWald test            = 17.36  on 3 df,   p=6e-04\nScore (logrank) test = 18.52  on 3 df,   p=3e-04\n\n\nPodemos nuevamente chequear la proporcionalidad en este nuevo modelo:\n\ncox.zph(m5st, \n        transform=\"km\", \n        global=TRUE)\n\n        chisq df      p\nfase   10.848  2 0.0044\ndeag    0.448  1 0.5031\nGLOBAL 11.084  3 0.0113\n\n\nVemos que el modelo de Cox estratificado tampoco resuelve el problema."
  },
  {
    "objectID": "unidad_6/04_reg_cox.html#modelo-de-cox-extendido",
    "href": "unidad_6/04_reg_cox.html#modelo-de-cox-extendido",
    "title": "Regresión de Cox",
    "section": "Modelo de Cox extendido",
    "text": "Modelo de Cox extendido\nEl modelo de Cox que vimos inicialmente asume que las covariables no varían en el transcurso del tiempo. Pero sabemos que esto no siempre es así.\n¿Cómo analizar la supervivencia cuando las covariables cambian con el tiempo, como edad, tratamiento o hábitos alimentarios?\nEl modelo de Cox extendido aborda esta situación mediante un proceso de conteo. Tiene las mismas premisas del modelo de Cox\n\\[\nh(t|x(t))= h_0(t)exp(x(t)\\beta)\n\\] ¿Cuál es la diferencia? La primer diferencia está ya en la base de datos: tendremos más de una fila por individuo, dado que los valores de una variable para el mismo individuo cambian con el correr del tiempo."
  },
  {
    "objectID": "unidad_6/04_reg_cox.html#análisis-de-supervivencia-más-complejos",
    "href": "unidad_6/04_reg_cox.html#análisis-de-supervivencia-más-complejos",
    "title": "Regresión de Cox",
    "section": "Análisis de supervivencia más complejos",
    "text": "Análisis de supervivencia más complejos\nHasta ahora, vimos modelos de supervivencia adecuados para tratar problemas en los cuales el evento ocurre una única vez. Pero existen situaciones en lo que esto no es así y podríamos estar interesados en eventos que pueden acontecer más de una vez para un mismo individuo, como: infartos, gestaciones, fracturas, internaciones, etc. Así también nos podrían interesar situaciones en las que existen diferentes tipos de eventos derivados de un mismo factor de riesgo en el estudio: efectos adversos por medicamentos, enfermedades oportunistas en SIDA, etc. Este análisis se conoce como Supervivencia de eventos múltiples.\nEl análisis de eventos múltiples, pretende responder a dos preguntas básicas:\n\n¿Cuáles son los factores de riesgo asociados a los tiempos de diferentes eventos en un mismo individuo?\n¿Cómo analizar diferentes eventos como desenlace de una misma situación de riesgo?\n\nLas principales características de este tipo de análisis son:\n\nTener más de un tiempo de supervivencia por individuo.\nLos individuos pueden mantenerse en el grupo de riesgo aún después de la ocurrencia de algún evento.\n\nDado que los intervalos de tiempo no son independientes entre sí, la utilización directa del modelo de Cox no sirve. A su vez, no tener independencia entre las observaciones puede generar error en las estimaciones.\nLas estrategias para resolver estas situaciones son varias:\n\nAjustar un modelo de regresión en el que la variable respuesta es el número de eventos por paciente durante el período de estudio: Poisson\nUtilizar Cox considerando uno de los eventos: tiempo hasta el primer evento\nUtilizar Modelos multinivel\nUtilizar Modelos marginales\n\nA continuación presentaremos una clasificación de eventos múltiples, la cual no es rígida ni definitiva, dado que la evolución de métodos en esta área es grande y van surgiendo nuevos modelos, que se adecúan mejor a las situaciones analizadas.\n\nEventos Competitivos: sólo es posible observar tiempo hasta un evento primero que impide que otro tenga lugar. Ej; muerte x diferentes causas y un mismo factor de riesgo\nEventos Paralelos: la ocurrencia de un evento no excluye la aparición de otros eventos y no existe orden preferencial (Enfermedades oportunistas, los efectos secundarios, la pérdida de dientes)\nEventos Ordenados: la sucesión de tiempos sigue obligatoriamente un orden. Ej: IAM El riesgo subyacente cambia a medida que el individuo sufre nuevos eventos, es decir, los eventos, además de ordenarse en el tiempo, se ordenan según el riesgo basal.\n\nComo podrán intuir el análisis se complejiza aún más, por lo cual no los abordaremos en este curso. Solamente hemos presentado los lineamientos, para que ustedes sean capaces de reconocer situaciones donde se deben aplicar estas metodologías, con el fin de que no cometan errores metodológicos importantes en el futuro (al menos referidos a estos temas)."
  },
  {
    "objectID": "unidad_6/04_reg_cox.html#consideraciones-sobre-el-tamaño-de-la-muestra",
    "href": "unidad_6/04_reg_cox.html#consideraciones-sobre-el-tamaño-de-la-muestra",
    "title": "Regresión de Cox",
    "section": "Consideraciones sobre el tamaño de la muestra",
    "text": "Consideraciones sobre el tamaño de la muestra\nComo ya hemos visto en la primera unidad, para determinar el tamaño de muestra necesario para un estudio, tendremos que tener una idea de la magnitud del efecto que deseamos detectar. En el análisis de supervivencia, el efecto es el HR, por ejemplo, a los 5 años, a los 10 años o al intervalo de tiempo que se desee.\nSupongamos que deseamos saber si existen diferencias entre dos terapias diferentes A y B utilizadas habitualmente para tratar un determinado tipo de cáncer. Para ello se planea realizar un estudio de supervivencia. ¿Cuántos pacientes deberán estudiarse con cada tratamiento si se desea calcular el riesgo relativo con una precisión del 50% de su valor real y una seguridad del 95%? De experiencias previas, se estima que el valor real del riesgo relativo es aproximadamente igual a 3 y la probabilidad de fallecer entre los pacientes tratados con el tratamiento A de un 20%.\nEn este caso, para conocer si el tratamiento A tiene un efecto beneficioso sobre la evolución de los enfermos, podremos utilizar un modelo de regresión de Cox en el que se ajuste por la variable tratamiento, y a partir de cuyos coeficientes podremos estimar el HR asociado a la terapia recibida. Si, con este procedimiento, deseamos calcular el tamaño muestral mínimo necesario para detectar un determinado HR, deberemos conocer:\n\nUna idea del valor aproximado del riesgo relativo que se desea detectar (HR).\nLa proporción de expuestos al factor de estudio (p), es decir, en nuestro caso, la proporción de enfermos habitualmente tratados con la terapia A.\nEl porcentaje de observaciones censuradas que se espera en el total de la muestra (%C)\nEl nivel de confianza o seguridad con el que se desea trabajar \\(\\alpha\\)\nEl poder que se quiere para el estudio \\(1-\\beta\\)\n\nCon estos datos, el cálculo del tamaño muestral puede abordarse mediante la fórmula:\n\\[\nn= \\frac{(z_{1-\\alpha/2}+z_{1+\\beta})^2}{(log(HR))^2*(1-\\%C)*(1-p)*p}\n\\]\nEjemplo\nSupongamos que el tratamiento (A) suele aplicarse a un 70% de los pacientes que padecen ese tipo de cáncer, mientras que la otra terapia (B) es recibida sólo por un 30% de los enfermos. Si el efecto pronóstico del tratamiento recibido va a analizarse de modo univariado, la ecuación puede aplicarse para calcular el número necesario de pacientes a estudiar. Así, para detectar un riesgo relativo de 3, suponiendo un 20% de censura y trabajando con un \\(\\alpha= 95\\%\\) y un poder del 80% se tendría:\n\\[\nn= \\frac{(1.96+0.842)^2}{(log3)^2*(1-0.2)*(1-0.7)*0.7}= 47\n\\]\nNecesitaríamos 47 pacientes para detectar un HR de 3. A menor HR, mayor tamaño de muestra.\nSi ahora deseamos usar un modelo de Cox con más variables, el tratar de ajustar un modelo más complejo con el mismo número de pacientes llevará consigo una pérdida de precisión en la estimación de los coeficientes y, con ello, del HR asociado a cada una de las variables incluidas en el modelo multivariante. En esta situación, es obvio, que se necesita realizar alguna corrección en la ecuación que permita adaptar el tamaño muestral calculado a las variables que se incluirán a posteriori en el modelo.\n\\[\nn= \\frac{(z_{1-\\alpha/2}+z_{1+\\beta})^2}{(logHR)^2*(1-\\%C)*(1-p)*p}*\\frac {1}{1-\\rho^2}\n\\]\nEl término agregado se lo llama factor de inflación de la varianza, donde \\(\\rho\\) denota al coeficiente de correlación de Pearson entre el factor en estudio y aquella otra variable que incluiremos en el modelo. En el caso en el que se ajuste por más de otro factor en el modelo, lo más sencillo es considerar como el mayor coeficiente de correlación entre el factor de estudio y todas las variables incluidas. Este coeficiente de correlación, cuanto mayor sea, más incrementará el valor del factor de inflación de la varianza y, por tanto, se incrementará el tamaño de la muestra a estudiar.\nSiguiendo con el ejemplo, posiblemente el tratamiento aplicado a cada enfermo dependerá de las características clínicas particulares del mismo, y recibirá una u otra terapia en función, por ejemplo, del estadío del tumor. En términos estadísticos, podrá entonces decirse que el factor tratamiento se encuentra “correlacionado” con esta característica. Supongamos, por ejemplo, que la correlación existente entre el nuevo tratamiento y el estadío del tumor es de 0,25. Este dato lo podremos obtener a partir de un estudio piloto o de otros trabajos sobre el tema. En caso de desconocerse, deberemos asumir una correlación suficientemente alta para así asegurar un poder suficiente.\nEl cálculo del tamaño muestral, por tanto, permite al investigador precisar el número de pacientes a estudiar para detectar como significativos efectos de una magnitud determinada. El no hacerlo, o el no conocer cuántos pacientes necesitamos para detectar un efecto como significativo podría llevarnos a cometer un error de tipo II, es decir, no encontrar diferencias cuando sí las hay.\n\nHarrell, Lee, y Mark (1996)\nHernández-Ávila (2011)\nRoyo-Bordonada et al. (2009)\nWoodward (2005)\nTherneau y Grambsch (2000)\nKassambara, Kosinski, y Biecek (2024)"
  },
  {
    "objectID": "unidad_6/02_analisis_superv.html",
    "href": "unidad_6/02_analisis_superv.html",
    "title": "Análisis de supervivencia",
    "section": "",
    "text": "El estudio de procesos de supervivencia implica el seguimiento de individuos a lo largo del tiempo, pudiéndose presentar diversas situaciones que complican su caracterización. La característica más relevante de este tipo de datos —el tiempo hasta que ocurre un determinado suceso— es que, al finalizar el periodo de observación, probablemente no todos los pacientes habrán experimentado el evento de interés.\nAdemás puede ocurrir que algunos pacientes se hayan perdido durante el seguimiento por causas diversas, imposibilitando conocer su estado final. También es habitual que los pacientes vayan incorporándose durante todo el periodo de observación. En estos casos, quienes ingresan más tarde serán observados durante un tiempo más corto, por lo que su probabilidad de experimentar el suceso será menor.\nEn enfermedades crónicas, tales como el cáncer, la supervivencia se mide como la probabilidad de permanecer con vida durante un determinado intervalo de tiempo. Indicadores como la supervivencia al año o a los cinco años son comúnmente utilizados para caracterizar la gravedad de una enfermedad y establecer pronósticos. Típicamente, el pronóstico del cáncer se valora determinando el porcentaje de pacientes que sobrevive al menos cinco años después del diagnóstico.\nEl análisis de supervivencia se utiliza cuando el tiempo es el objeto de interés, ya sea como “tiempo hasta la ocurrencia de un evento” o como “riesgo de ocurrencia por unidad de tiempo”. Este enfoque incorpora el componente dinámico del tiempo, por lo que la variable respuesta es una combinación de dos elementos: el evento (respuesta) y el tiempo hasta que ocurre.\nEl evento o desenlace de interés es de tipo dicotómico, e indica si el suceso ha ocurrido o no (por ejemplo: muerte, recidiva, aparición de una complicación, etc.). La variable numérica, por su parte, representa el tiempo transcurrido hasta la aparición del evento.\nLos paradigmas de diseño de estudio que generan este tipo de información son principalmente los estudios de cohorte y los ensayos clínicos, ya que ambos implican el seguimiento de un grupo de individuos a lo largo del tiempo. Esto permite acceder directamente a la variable respuesta: el tiempo hasta la ocurrencia de un evento.\nRecordemos que en ambos diseños, pueden presentarse dos tipos de cohortes: cerradas y abiertas. Según el tipo, será posible estimar distintas medidas de ocurrencia: incidencia acumulada o densidad de incidencia. También es importante tener presente que, en ambos tipos de cohortes, puede haber pérdida de información por distintas razones, lo que debe ser considerado en el análisis.\n\n\n\n\n\n\n\n\nA la variable “tiempo hasta un evento”, en el contexto del análisis de supervivencia, se la llama habitualmente “tiempo de supervivencia”, aunque el evento de interés no sea la muerte. Algunos autores se refieren a los tiempos de supervivencia como “tiempos de fallo”. En general, en el lenguaje de supervivencia, el evento de interés, tiene una connotación negativa: muerte, recaída, rechazo a un trasplante, insuficiencia renal, etc.\nComenzaremos intentando comprender las diferencias que este tipo de análisis tiene con otras metodologías, en que se diferencian los datos del resto de los datos y por qué necesitamos mucho soporte estadístico para comprender esta metodología."
  },
  {
    "objectID": "unidad_6/02_analisis_superv.html#introducción",
    "href": "unidad_6/02_analisis_superv.html#introducción",
    "title": "Análisis de supervivencia",
    "section": "",
    "text": "El estudio de procesos de supervivencia implica el seguimiento de individuos a lo largo del tiempo, pudiéndose presentar diversas situaciones que complican su caracterización. La característica más relevante de este tipo de datos —el tiempo hasta que ocurre un determinado suceso— es que, al finalizar el periodo de observación, probablemente no todos los pacientes habrán experimentado el evento de interés.\nAdemás puede ocurrir que algunos pacientes se hayan perdido durante el seguimiento por causas diversas, imposibilitando conocer su estado final. También es habitual que los pacientes vayan incorporándose durante todo el periodo de observación. En estos casos, quienes ingresan más tarde serán observados durante un tiempo más corto, por lo que su probabilidad de experimentar el suceso será menor.\nEn enfermedades crónicas, tales como el cáncer, la supervivencia se mide como la probabilidad de permanecer con vida durante un determinado intervalo de tiempo. Indicadores como la supervivencia al año o a los cinco años son comúnmente utilizados para caracterizar la gravedad de una enfermedad y establecer pronósticos. Típicamente, el pronóstico del cáncer se valora determinando el porcentaje de pacientes que sobrevive al menos cinco años después del diagnóstico.\nEl análisis de supervivencia se utiliza cuando el tiempo es el objeto de interés, ya sea como “tiempo hasta la ocurrencia de un evento” o como “riesgo de ocurrencia por unidad de tiempo”. Este enfoque incorpora el componente dinámico del tiempo, por lo que la variable respuesta es una combinación de dos elementos: el evento (respuesta) y el tiempo hasta que ocurre.\nEl evento o desenlace de interés es de tipo dicotómico, e indica si el suceso ha ocurrido o no (por ejemplo: muerte, recidiva, aparición de una complicación, etc.). La variable numérica, por su parte, representa el tiempo transcurrido hasta la aparición del evento.\nLos paradigmas de diseño de estudio que generan este tipo de información son principalmente los estudios de cohorte y los ensayos clínicos, ya que ambos implican el seguimiento de un grupo de individuos a lo largo del tiempo. Esto permite acceder directamente a la variable respuesta: el tiempo hasta la ocurrencia de un evento.\nRecordemos que en ambos diseños, pueden presentarse dos tipos de cohortes: cerradas y abiertas. Según el tipo, será posible estimar distintas medidas de ocurrencia: incidencia acumulada o densidad de incidencia. También es importante tener presente que, en ambos tipos de cohortes, puede haber pérdida de información por distintas razones, lo que debe ser considerado en el análisis.\n\n\n\n\n\n\n\n\nA la variable “tiempo hasta un evento”, en el contexto del análisis de supervivencia, se la llama habitualmente “tiempo de supervivencia”, aunque el evento de interés no sea la muerte. Algunos autores se refieren a los tiempos de supervivencia como “tiempos de fallo”. En general, en el lenguaje de supervivencia, el evento de interés, tiene una connotación negativa: muerte, recaída, rechazo a un trasplante, insuficiencia renal, etc.\nComenzaremos intentando comprender las diferencias que este tipo de análisis tiene con otras metodologías, en que se diferencian los datos del resto de los datos y por qué necesitamos mucho soporte estadístico para comprender esta metodología."
  },
  {
    "objectID": "unidad_6/02_analisis_superv.html#el-lenguaje-propio-del-análisis-de-supervivencia",
    "href": "unidad_6/02_analisis_superv.html#el-lenguaje-propio-del-análisis-de-supervivencia",
    "title": "Análisis de supervivencia",
    "section": "El lenguaje propio del análisis de supervivencia",
    "text": "El lenguaje propio del análisis de supervivencia\nUna de las características distintivas de los datos de supervivencia es que la variable de interés —el tiempo hasta que ocurre un evento— solo toma valores positivos y suele presentar distribuciones sesgadas.\nPor ejemplo:\n\nEn un estudio sobre recaídas en pacientes de alto riesgo, la mayoría de los eventos pueden ocurrir temprano en el seguimiento.\nEn cambio, en un estudio poblacional sobre tiempo hasta la muerte, la mayoría de los eventos pueden ocurrir en etapas más tardías.\n\nEsto nos hace pensar que los procedimientos estadísticos estándar que asumen normalidad de los datos no son apropiados para estos casos. Aunque podríamos recurrir a procedimientos no paramétricos, existe un desafío adicional: la información de seguimiento suele estar incompleta para algunos participantes.\nCensura y truncamiento\nEn estudios con cohortes abiertas, los participantes que ingresan más tarde al estudio tienen un período de seguimiento más corto que quienes ingresan más temprano. Algunos participantes pueden abandonar el estudio antes de finalizar el seguimiento (por mudanza, pérdida de interés, etc.), o incluso fallecer, aunque el evento de interés no sea la muerte.\nEn cada uno de estos casos, tenemos información de seguimiento incompleta. Es posible que el tiempo hasta que se produce el evento del interés no se conozca porque el estudio termina o porque un participante se retira del estudio antes de experimentar el evento. Estos tiempos son llamados tiempos censurados.\nLa censura, se refiere a la pérdida de información e indica que no se ha observado el tiempo exacto en que ocurrió el evento de interés. Existen tres tipos:\n\n\nCensura a la derecha: ocurre cuando el tiempo entre el inicio del estudio y el evento de interés, es mayor que el tiempo de observación.\n\n\n\nCensura a la izquierda: ocurre cuando el individuo presenta el evento de interés antes de ingresar al estudio, por lo que se sabe que su tiempo de supervivencia no observado es menor que el tiempo de observación.\n\nUn ejemplo de esta situación puede darse en el caso en que se tiene el interés de determinar el momento (edad) en que el niño aprende a desarrollar cierta habilidad o tarea. Puede darse que algunos niños incluidos en el estudio ya hayan adquirido dicha habilidad previamente al ingreso al estudio.\n\nCensura intervalar: en algunos estudios, la observación de los sujetos no se realiza de manera continua y los períodos de observación pueden ser muy largos entre dos observaciones consecutivas. En esta situación, es común que ocurra el evento de interés entre dos puntos de observación y no se sepa cuando ocurrió exactamente.\n\nEl truncamiento ocurre cuando ciertos individuos no ingresan al estudio debido a que el evento ya ocurrió fuera del período de observación. Al igual que para la censura, existen truncamientos a la izquierda, a la derecha y por intervalos, siendo los dos primeros los más comunes.\n\nTruncamiento a la izquierda: en un estudio sobre supervivencia en mayores de 60 años, no se incluyen personas que murieron antes de esa edad.\nTruncamiento a la derecha: en un estudio sobre infección por VIH, solo se incluyen personas que se infectaron dentro de la ventana del estudio.\n\nEsta “desinformación” puede traer problemas técnicos importantes. La mayoría de los métodos de análisis de supervivencia suponen que los individuos censurados tienen la misma probabilidad de experimentar el evento que aquellos que permanecieron en el estudio. Si esto es así, decimos que la censura es “no informativa” y es lo deseable.\nPor ejemplo, si se pierde la información de un individuo por mudanza, no hay razón para pensar que eso está vinculado al evento de interés. Si en cambio, el individuo abandona el estudio por complicaciones médicas, es más probable que esté ligado a nuestro evento de interés. Este último caso, se trata de “censura informativa”, lo cual debe evitarse, porque afectará la validez de las estimaciones.\nEjemplo de cohorte abierta\nConsideremos un pequeño estudio prospectivo donde se siguen diez participantes para evaluar el desarrollo de infarto agudo de miocardio (IAM) durante un período de 10 años. Los participantes son reclutados para el estudio durante un período de dos años y son seguidos durante un máximo de 10 años.\nEl siguiente gráfico resume el seguimiento de cada participante, indicando eventos (IAM), abandonos, muertes y finalización del estudio sin evento:\n\n\n\n\n\n\n\n\nObservando el gráfico, podemos decir que, durante el seguimiento:\n\nTres pacientes sufrieron el evento de interés IAM (pacientes 2, 3 y 5)\nDos pacientes abandonaron el estudio por razones desconocidas (pacientes 4 y 7)\nEl paciente 10 falleció\nCuatro pacientes completaron los 10 años de seguimiento sin sufrir IAM (pacientes 1, 6, 8 y 9)\n\nBasados en estos datos: ¿Cuál es la probabilidad de un paciente de sufrir IAM en 10 años?\nTres de diez participantes sufren IAM en el transcurso del seguimiento, es decir que la probabilidad sería 0,3 (30%). Pero este valor probablemente sea una subestimación del verdadero porcentaje ya que dos participantes abandonaron el estudio y podrían haber sufrido un infarto, que habría sido observado si hubiésemos podido seguirlos los 10 años completos. Sus tiempos observados fueron censurados. Además, un participante muere después de tres años de seguimiento.\n¿Debemos incluir estos tres individuos en el análisis?\nSi excluimos a los tres participantes censurados, la estimación de la probabilidad de que un individuo sufra un IAM sería 3/7 = 43%, un valor sensiblemente superior a la estimación inicial de 30%. Esta diferencia destaca una de las particularidades del análisis de supervivencia: no siempre es posible observar a todos los participantes durante todo el período de seguimiento, lo que hace que estos datos requieran un tratamiento estadístico especial. En nuestro ejemplo:\n\nEl participante 4 se observó durante 4 años y durante ese período no tuvo un IAM\nEl participante 7 se observó durante 2 años y durante ese periodo no tuvo un IAM\n\nMientras no hayan experimentado el evento de interés, los individuos en seguimiento aportan información importante. Las técnicas de análisis de supervivencia aprovechan esa información parcial para estimar con mayor precisión la probabilidad de ocurrencia del evento. Muy pronto veremos cómo, pero antes, puntualizaremos un supuesto importante que se hace para hacer un uso adecuado de los datos censurados.\n\n\n\n\n\n\nEspecíficamente, se supone que la censura es independiente o no relacionada con la probabilidad de desarrollar el evento de interés. Dicho de otra forma, la censura debe ser no informativa y asume esencialmente que los participantes cuyos datos son censurados, tendrían la misma distribución de tiempos de supervivencia que si fueran realmente observados.\n\n\n\nEjemplo de cohorte cerrada\nAhora consideremos un estudio con los mismos participantes, pero todos ingresan al mismo tiempo. Aunque los eventos y censuras son los mismos, el momento en que ocurren cambia: los IAM ocurren más temprano, y los abandonos/muertes más tarde:\n\n\n\n\n\n\n\n\n¿Pueden estas diferencias en las experiencias de los participantes afectar a la estimación de la probabilidad de que un participante sufra un infarto de miocardio en un período de 10 años? En breve contestaremos también esta pregunta.\n\n\n\n\n\n\nEn el análisis de supervivencia se analiza no sólo el número de participantes que sufren el evento de interés (indicador dicotómico de estado del evento), sino también los momentos en que se producen los acontecimientos (tiempo de supervivencia).\n\n\n\nSi todos los pacientes bajo estudio experimentaran el evento durante el período de observación, sería sencillo calcular la proporción de aquellos que, transcurrido un determinado tiempo desde su inclusión, aún no han presentado el evento: es decir, los “sobrevivientes”. Podríamos entonces representar gráficamente esta proporción en función del tiempo desde la entrada al estudio, y obtendríamos la función de supervivencia, denotada como \\(S(t)\\).\nSin embargo, las cosas se complican cuando no todos los pacientes presentan el evento. Por ejemplo, un paciente podría haber ingresado a mitad del período de seguimiento, ser observado durante dos años y, al finalizar el estudio, no haber sufrido el evento.\nComo dijimos, en análisis de supervivencia nos interesa el tiempo hasta que ocurre un evento. Este tiempo se conoce como tiempo de supervivencia, incluso cuando el evento no sea la muerte. Se trata de una variable aleatoria continua, que no puede tomar valores negativos y que rara vez sigue una distribución normal. Ya hemos visto que toda variable aleatoria posee una distribución de probabilidad que describe su comportamiento. Así que, para estimar probabilidades de que el evento ocurra, necesitaremos conocer la distribución de estos tiempos de supervivencia.\n\n\n\n\n\n\nComo probablemente habrán advertido, estamos utilizando el lenguaje de la probabilidad, así que si no recuerdan los conceptos de función de densidad y función de probabilidad acumulada para una variable aleatoria continua, es un buen momento para revisarlos."
  },
  {
    "objectID": "unidad_6/02_analisis_superv.html#función-de-supervivencia-st",
    "href": "unidad_6/02_analisis_superv.html#función-de-supervivencia-st",
    "title": "Análisis de supervivencia",
    "section": "Función de Supervivencia \\(S(t)\\)\n",
    "text": "Función de Supervivencia \\(S(t)\\)\n\nComo mencionamos anteriormente, los tiempos de supervivencia se modelan mediante una función de densidad de probabilidad, denotada como \\(f(t)\\), que representa la probabilidad de que el evento ocurra exactamente en el tiempo \\(t\\). Sin embargo, como se trata de una variable continua, esta probabilidad puntual es técnicamente cero. Por eso, para obtener probabilidades debemos calcular el área bajo la curva de \\(f(t)\\) en un intervalo infinitesimal alrededor de \\(t\\).\nA partir de \\(f(t)\\), podemos definir la función de distribución acumulada \\(F(t)\\), que expresa la probabilidad de que el evento ocurra en un tiempo menor o igual a \\(t\\). Se obtiene calculando la integral de la \\(f(t)\\).\n\\[\nF(t)=P[T\\leq t]= \\int_0^t f(t)dt\n\\]\nPor ejemplo, si en un estudio estamos interesados en evaluar la supervivencia de pacientes pos trasplante hepático, necesitaríamos conocer la distribución de estos tiempos de supervivencia. El evento de interés sería “muerte pos trasplante” y si quisiéramos conocer la probabilidad de muerte a los dos, tres o cinco años, deberíamos realizar la integral señalada al tiempo deseado.\nSi existe una \\(F(t)\\), que nos da la probabilidad de morir a determinado tiempo \\(t\\), entonces, existirá la función de supervivencia \\(S(t)\\), que mide la probabilidad de sobrevivir más de un tiempo \\(t\\).\n\\(S(t)=Pr(T&gt;t)\\)\n\\(F(t)=Pr(T\\leq t)\\)\n\\(S(t)=1-F(t)\\)\nCumple con: \\(S(0)=1\\) (en el primer momento, todos los pacientes están vivos)\nPara \\(t \\rightarrow \\infty\\); \\(S(t) \\rightarrow 0\\) (en algún momento, lejano, todos moriremos!!!).\nSi considero un período de tiempo, entre \\(t_1\\) y \\(t_2\\):\nSi \\(t_1 &lt; t_2\\) ; entonces \\(S(t_1) &gt; S(t_2)\\)\nAl considerar la posibilidad de adoptar una distribución teórica para los tiempos de supervivencia, estamos haciendo un abordaje paramétrico. Las funciones densidad de probabilidad más utilizadas son: la exponencial, Weibull y lognormal. No profundizaremos en detalles sobre estas aproximaciones, porque no son tan aplicadas en el ámbito del análisis de supervivencia aplicado a salud.\nNos centraremos en los abordajes no paramétricos, que parten de la construcción de las llamadas Tablas de supervivencia, y tienen como objetivo central la estimación de dos medidas: la probabilidad de morir en un momento dado y la probabilidad de sobrevivir hasta un determinado tiempo. Existen dos métodos: el método actuarial y el método de Kaplan Meier, que describiremos en el siguiente capítulo.\n\nHernández-Ávila (2011)\nWoodward (2005)\nRoyo-Bordonada et al. (2009)\nEscuela Nacional de Sanidad (ENS). Instituto de Salud Carlos III. Ministerio de Ciencias e Innovación. Madrid (2009)"
  },
  {
    "objectID": "unidad_5/03_dispersion.html",
    "href": "unidad_5/03_dispersion.html",
    "title": "Dispersión de datos",
    "section": "",
    "text": "En modelos de regresión Poisson se asume equidispersión, es decir que la varianza de la variable dependiente es igual a su media: \\[\nvar(Y) =  \\mu\n\\]\nSin embargo, en la práctica, esta suposición frecuentemente no se cumple y la varianza puede ser mayor que la media (sobredispersión) o menor que la media (subdispersión). Además, en algunos casos, los datos pueden contener un exceso de ceros que el modelo Poisson no ajusta adecuadamente. Estos fenómenos pueden llevar a que las inferencias que hagamos sean incorrectas, por lo que deben controlarse al ajustar los modelos."
  },
  {
    "objectID": "unidad_5/03_dispersion.html#introducción",
    "href": "unidad_5/03_dispersion.html#introducción",
    "title": "Dispersión de datos",
    "section": "",
    "text": "En modelos de regresión Poisson se asume equidispersión, es decir que la varianza de la variable dependiente es igual a su media: \\[\nvar(Y) =  \\mu\n\\]\nSin embargo, en la práctica, esta suposición frecuentemente no se cumple y la varianza puede ser mayor que la media (sobredispersión) o menor que la media (subdispersión). Además, en algunos casos, los datos pueden contener un exceso de ceros que el modelo Poisson no ajusta adecuadamente. Estos fenómenos pueden llevar a que las inferencias que hagamos sean incorrectas, por lo que deben controlarse al ajustar los modelos."
  },
  {
    "objectID": "unidad_5/03_dispersion.html#sobredispersión",
    "href": "unidad_5/03_dispersion.html#sobredispersión",
    "title": "Dispersión de datos",
    "section": "Sobredispersión",
    "text": "Sobredispersión\nLa sobredispersión es común en datos de conteo y puede ser causada por varios factores, como la heterogeneidad no observada, correlación entre observaciones o una alta incidencia de valores extremos. La sobredispersión puede inflar los errores estándar, lo que lleva a una subestimación de la significancia estadística. Matemáticamente, se expresa como:\n\\[\nVar(Y) &gt; \\mu\n\\]\nPara detectar si un modelo tiene sobredispersión, se puede calcular el ratio de dispersión, que es el cociente entre la devianza del modelo y los grados de libertad. Un ratio significativamente mayor que 1 indica sobredispersión.\nEn R, la función check_overdispersion() del paquete performance evalúa el ratio de dispersión en un modelo. En el ejemplo de regresión Poisson el cociente del modelo final era cercano a 1, lo cual indicaba que no existía evidencia de sobredispersión y la distribución seleccionada ajustaba bien los datos.\nVeamos ahora un ejemplo de datos con sobredispersión a partir de los casos de dengue reportados en los departamentos de la Costa Atlántica de la provincia de Buenos Aires (PBA)1 entre las semanas epidemiológicas 1 y 20 de 20242. Los mismos se encuentran en el archivo “dengue_costa.txt”.\nComenzaremos por cargar los paquetes necesarios para el análisis:\n\n# Ajuste de modelos\nlibrary(glmmTMB)\n\n# Chequeo de supuestos\nlibrary(easystats)\n\n# Coeficientes\nlibrary(gtsummary)\n\n# Manejo de datos\nlibrary(janitor)\nlibrary(tidyverse)\n\nAhora cargamos los datos y exploramos su estructura:\n\n# Carga datos\ndatos &lt;- read_delim(\"datos/dengue_costa.txt\")\n\n# Explora datos\nglimpse(datos)\n\nRows: 1,047\nColumns: 6\n$ provincia      &lt;chr&gt; \"Buenos Aires\", \"Buenos Aires\", \"Buenos Aires\", \"Buenos…\n$ departamento   &lt;chr&gt; \"BAHIA BLANCA\", \"BAHIA BLANCA\", \"BAHIA BLANCA\", \"BAHIA …\n$ periodo        &lt;dbl&gt; 2024, 2024, 2024, 2024, 2024, 2024, 2024, 2024, 2024, 2…\n$ semana_epi     &lt;dbl&gt; 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 4, 4, 4, 4…\n$ grupo_edad_cat &lt;chr&gt; \"0-9 años\", \"10-19 años\", \"20-34 años\", \"35-65 años\", \"…\n$ casos          &lt;dbl&gt; 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0…\n\n\nLas variables de interés para el modelo son:\n\nsemana_epi: semana epidemiológica de reporte de los casos.\ngrupo_edad_cat: grupo etario.\ncasos: número de casos reportados por departamento, semana epidemiológica y grupo etario. Es la variable dependiente.\n\nAjustamos un modelo Poisson con todas las variables de interés:\n\nfit_pois &lt;- glm(casos ~ semana_epi + grupo_edad_cat,\n                data = datos,\n                family = poisson)\n\n# Salida del modelo\nsummary(fit_pois)\n\n\nCall:\nglm(formula = casos ~ semana_epi + grupo_edad_cat, family = poisson, \n    data = datos)\n\nCoefficients:\n                          Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)              -3.092154   0.279663 -11.057  &lt; 2e-16 ***\nsemana_epi                0.044249   0.008993   4.920 8.65e-07 ***\ngrupo_edad_cat10-19 años  1.172838   0.294392   3.984 6.78e-05 ***\ngrupo_edad_cat20-34 años  2.195273   0.271054   8.099 5.54e-16 ***\ngrupo_edad_cat35-65 años  2.312218   0.269545   8.578  &lt; 2e-16 ***\ngrupo_edad_cat65+ años    0.730874   0.314523   2.324   0.0201 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 1436.5  on 1046  degrees of freedom\nResidual deviance: 1187.9  on 1041  degrees of freedom\nAIC: 1735.1\n\nNumber of Fisher Scoring iterations: 6\n\n\nEl modelo ajustado no incluye un término offset, ya que este se utiliza cuando es necesario ajustar la tasa de eventos por una variable de tiempo o tamaño poblacional. En este ejemplo, podemos asumir que la población de cada departamento se mantiene constante en el periodo estudiado. Además, dado que una misma persona puede infectarse más de una vez con el virus del dengue, no es necesario incorporar un término de ajuste adicional.\nChequeamos si existe sobredispersión:\n\ncheck_overdispersion(fit_pois)\n\n# Overdispersion test\n\n       dispersion ratio =    1.955\n  Pearson's Chi-Squared = 2035.204\n                p-value =  &lt; 0.001\n\n\nEl cociente de dispersión es de 1.96, por lo que el modelo Poisson no es el más adecuado para representar los datos."
  },
  {
    "objectID": "unidad_5/03_dispersion.html#subdispersión",
    "href": "unidad_5/03_dispersion.html#subdispersión",
    "title": "Dispersión de datos",
    "section": "Subdispersión",
    "text": "Subdispersión\nLa subdispersión, aunque menos común, puede ocurrir en situaciones donde los conteos están más controlados o restringidos. Este fenómeno puede hacer que los errores estándar se subestimen, resultando en una sobreestimación de la significancia estadística. Matemáticamente se expresa como:\n\\[\nVar(Y) &lt; \\mu\n\\]\nPara detectar si un modelo tiene subdispersión también se usa el ratio de dispersión. Donde un ratio significativamente menor que 1 indica subdispersión."
  },
  {
    "objectID": "unidad_5/03_dispersion.html#control-de-la-sobredispersión-y-subdispersión",
    "href": "unidad_5/03_dispersion.html#control-de-la-sobredispersión-y-subdispersión",
    "title": "Dispersión de datos",
    "section": "Control de la sobredispersión y subdispersión",
    "text": "Control de la sobredispersión y subdispersión\nModelos quasi-Poisson\nUna forma común de manejar la sobredispersión en los datos es mediante el uso de modelos quasi-Poisson. Estos modelos ajustan los errores estándar sin modificar la media predicha, incorporando un parámetro adicional que permite que la varianza sea distinta a la media. Aunque los modelos quasi-Poisson son útiles para corregir la sobredispersión, no abordan las causas subyacentes de esta ni son adecuados para datos con subdispersión o un exceso de ceros. Además, la estimación de parámetros en estos modelos tiende a ser menos eficiente, ya que asumen que la sobredispersión es constante en todas las observaciones, lo que puede llevar a inferencias menos robustas. Debido a estas limitaciones, no profundizaremos en el uso de los modelos quasi-Poisson.\nDistribución binomial negativa\nDentro de los modelos lineales generalizados (GLM), la distribución binomial negativa es especialmente útil para manejar la sobredispersión en datos discretos. Esta distribución aporta flexibilidad, mejora las inferencias estadísticas con estimaciones más precisas, permite procesar grandes volúmenes de datos y se puede combinar con componentes de inflación de ceros.\nLa familia binomial negativa no está integrada en la función glm() del paquete stats. No obstante, existen varios paquetes que permiten su implementación. En el contexto del curso, usaremos el paquete glmmTMB (Brooks et al. 2017) para explorar alternativas a los modelos Poisson, dado su alto grado de versatilidad. La función base glmmTMB() permite ajustar desde regresiones lineales y modelos lineales generalizados (GLMs) hasta modelos mucho más complejos, lo que lo convierte en una herramienta poderosa para manejar una amplia variedad de estructuras de datos.\nA modo de ejemplo, ajustaremos el modelo Poisson anterior con glmmTMB():\n\nfit_pois_tmb &lt;- glmmTMB(casos ~ semana_epi + grupo_edad_cat,\n                data = datos,\n                family = poisson)\n\n# Salida modelo\nsummary(fit_pois_tmb)\n\n Family: poisson  ( log )\nFormula:          casos ~ semana_epi + grupo_edad_cat\nData: datos\n\n      AIC       BIC    logLik -2*log(L)  df.resid \n   1735.1    1764.8    -861.5    1723.1      1041 \n\n\nConditional model:\n                          Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)              -3.092153   0.279663 -11.057  &lt; 2e-16 ***\nsemana_epi                0.044249   0.008993   4.920 8.64e-07 ***\ngrupo_edad_cat10-19 años  1.172837   0.294392   3.984 6.78e-05 ***\ngrupo_edad_cat20-34 años  2.195272   0.271054   8.099 5.54e-16 ***\ngrupo_edad_cat35-65 años  2.312217   0.269545   8.578  &lt; 2e-16 ***\ngrupo_edad_cat65+ años    0.730874   0.314523   2.324   0.0201 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nAl comparar los resultados del modelo ajustado con glmmTMB y el modelo ajustado con el paquete stats, podemos observar que, aunque no presentan exactamente la misma información, los coeficientes y el AIC son prácticamente idénticos.\nAhora, ajustaremos el modelo de regresión binomial negativa utilizando el argumento family = nbinom2, que tiene como función de enlace por defecto el logaritmo (link = \"log\"):\n\n# Modelo binomial negativo \nfit_nb &lt;- glmmTMB(casos ~ semana_epi + grupo_edad_cat,  \n                  data = datos,                 \n                  family = nbinom2)  \n\n# Salida del modelo\nsummary(fit_nb)\n\n Family: nbinom2  ( log )\nFormula:          casos ~ semana_epi + grupo_edad_cat\nData: datos\n\n      AIC       BIC    logLik -2*log(L)  df.resid \n   1538.4    1573.0    -762.2    1524.4      1040 \n\n\nDispersion parameter for nbinom2 family (): 0.467 \n\nConditional model:\n                         Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)              -3.27950    0.33522  -9.783  &lt; 2e-16 ***\nsemana_epi                0.06159    0.01570   3.923 8.73e-05 ***\ngrupo_edad_cat10-19 años  1.16315    0.33067   3.518 0.000435 ***\ngrupo_edad_cat20-34 años  2.18318    0.30879   7.070 1.55e-12 ***\ngrupo_edad_cat35-65 años  2.30095    0.30722   7.490 6.90e-14 ***\ngrupo_edad_cat65+ años    0.71124    0.34949   2.035 0.041842 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nSi comparamos los coeficientes de este modelo con el de Poisson, observamos que son similares, pero los intervalos de confianza varían ligeramente debido al ajuste binomial negativo:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\n\nPoisson\n\n\nBinomial negativa\n\n\n\nexp(Beta)\n95% CI\np-value\nexp(Beta)\n95% CI\np-value\n\n\n\n\nsemana_epi\n1.05\n1.03, 1.06\n&lt;0.001\n1.06\n1.03, 1.10\n&lt;0.001\n\n\ngrupo_edad_cat\n\n\n\n\n\n\n\n\n    grupo_edad_cat10-19 años\n3.23\n1.81, 5.75\n&lt;0.001\n3.20\n1.67, 6.12\n&lt;0.001\n\n\n    grupo_edad_cat20-34 años\n8.98\n5.28, 15.3\n&lt;0.001\n8.87\n4.85, 16.3\n&lt;0.001\n\n\n    grupo_edad_cat35-65 años\n10.1\n5.95, 17.1\n&lt;0.001\n9.98\n5.47, 18.2\n&lt;0.001\n\n\n    grupo_edad_cat65+ años\n2.08\n1.12, 3.85\n0.020\n2.04\n1.03, 4.04\n0.042\n\n\n\nAbbreviation: CI = Confidence Interval\n\n\n\n\n\nLa regresión binomial negativa tiene como limitación no ser adecuada para datos con subdispersión. Además, son modelos computacionalmente complejos y su ajuste depende de la estructura específica de los datos, lo que puede afectar la estabilidad de las estimaciones.\nDistribución Conway-Maxwell Poisson\nOtra alternativa a la regresión de Poisson, que permite controlar tanto sobredispersión como subdispersión y exceso de ceros es el modelo Conway-Maxwell Poisson (COM-Poisson). El mismo ofrece una generalización del modelo Poisson estándar, introduciendo un parámetro adicional que permite ajustar de forma independiente la media y la varianza.\nPodemos ajustar estos modelos con la función glmmTMB() especificando el argumento family = compois(), que tiene como función de enlace por defecto el logaritmo (link = \"log\"):\n\n# Modelo COMPOIS\nfit_compois &lt;- glmmTMB(casos ~ semana_epi + grupo_edad_cat,  \n                  data = datos,                 \n                  family = compois)  \n\n# Salida del modelo\nsummary(fit_compois)\n\nUna desventaja de estos modelos es que son computacionalmente complejos y están implementados en pocos software estadísticos y paquetes de R. Por otro lado, los parámetros son menos intuitivos de interpretar, y puede presentar dificultades en la estimación de parámetros, especialmente en casos de datos limitados o de mala calidad."
  },
  {
    "objectID": "unidad_5/03_dispersion.html#exceso-de-ceros-zero-inflation",
    "href": "unidad_5/03_dispersion.html#exceso-de-ceros-zero-inflation",
    "title": "Dispersión de datos",
    "section": "Exceso de ceros (zero-inflation)\n",
    "text": "Exceso de ceros (zero-inflation)\n\nEl exceso de ceros ocurre cuando se observan más ceros en los datos de lo que el modelo Poisson predice. Este fenómeno puede ser un indicativo de que el modelo Poisson no es adecuado para los datos.\nPara identificar un exceso de ceros, se puede utilizar la función check_zeroinflation() del paquete performance:\n\ncheck_zeroinflation(fit_pois_tmb)\n\n# Check for zero-inflation\n\n   Observed zeros: 821\n  Predicted zeros: 739\n            Ratio: 0.90\n\n\nLos resultados del test indican que los datos presentan un exceso de ceros. Para explorar el origen de este fenómeno, comenzamos tabulando los datos por semana epidemiológica:\n\ndatos |&gt; \n  group_by(semana_epi) |&gt; \n  summarise(casos = sum(casos))\n\n# A tibble: 20 × 2\n   semana_epi casos\n        &lt;dbl&gt; &lt;dbl&gt;\n 1          1     4\n 2          2     2\n 3          3     4\n 4          4     6\n 5          5     3\n 6          6     6\n 7          7     9\n 8          8    11\n 9          9    20\n10         10    48\n11         11    51\n12         12    59\n13         13    63\n14         14    49\n15         15    31\n16         16    11\n17         17    12\n18         18     9\n19         19    11\n20         20     1\n\n\nObservamos que se reportaron casos de dengue en todas las semanas epidemiológicas. Ahora, veamos la distribución de los casos por departamento:\n\ndatos |&gt; \n  group_by(departamento) |&gt; \n  summarise(casos = sum(casos))\n\n# A tibble: 10 × 2\n   departamento       casos\n   &lt;chr&gt;              &lt;dbl&gt;\n 1 BAHIA BLANCA         117\n 2 CORONEL DORREGO        2\n 3 GENERAL ALVARADO       6\n 4 GENERAL PUEYRREDON    75\n 5 LA COSTA             150\n 6 LOBERIA                2\n 7 MAR CHIQUITA           8\n 8 NECOCHEA              14\n 9 PINAMAR               28\n10 TRES ARROYOS           8\n\n\nEn este caso, todos los departamentos reportaron casos de dengue durante el período de estudio. Procedamos a cruzar los datos por semana epidemiológica y departamento:\n\ndatos |&gt; \n  group_by(semana_epi, departamento) |&gt; \n  summarise(casos = sum(casos)) |&gt; \n  print(n = 20) # Muestra las primeras 20 observaciones\n\n# A tibble: 200 × 3\n# Groups:   semana_epi [20]\n   semana_epi departamento       casos\n        &lt;dbl&gt; &lt;chr&gt;              &lt;dbl&gt;\n 1          1 BAHIA BLANCA           1\n 2          1 CORONEL DORREGO        0\n 3          1 GENERAL ALVARADO       0\n 4          1 GENERAL PUEYRREDON     2\n 5          1 LA COSTA               1\n 6          1 LOBERIA                0\n 7          1 MAR CHIQUITA           0\n 8          1 NECOCHEA               0\n 9          1 PINAMAR                0\n10          1 TRES ARROYOS           0\n11          2 BAHIA BLANCA           0\n12          2 CORONEL DORREGO        0\n13          2 GENERAL ALVARADO       0\n14          2 GENERAL PUEYRREDON     1\n15          2 LA COSTA               0\n16          2 LOBERIA                0\n17          2 MAR CHIQUITA           0\n18          2 NECOCHEA               0\n19          2 PINAMAR                1\n20          2 TRES ARROYOS           0\n# ℹ 180 more rows\n\n\nEn esta tabla, se observa que no todos los departamentos en La Costa reportaron casos de dengue cada semana epidemiológica. Además, dado que los datos están separados por grupo etario, es razonable suponer que tampoco se reportaron casos para cada grupo en cada departamento y semana epidemiológica.\nPara obtener una visión más detallada, generamos una tabla de frecuencia para los reportes de cero casos:\n\ntabyl(datos$casos) |&gt; \n  adorn_pct_formatting()\n\n datos$casos   n percent\n           0 821   78.4%\n           1 136   13.0%\n           2  44    4.2%\n           3  23    2.2%\n           4  13    1.2%\n           5   5    0.5%\n           6   2    0.2%\n           7   1    0.1%\n           9   1    0.1%\n          12   1    0.1%\n\n\nLa tabla muestra que el 78.4% de los datos corresponden a observaciones con cero reportes de casos de dengue. Esto es esperado en el contexto de datos de vigilancia epidemiológica, donde es común encontrar muchas observaciones sin eventos.\nEste exceso de ceros se puede controlar usando modelos zero-inflated, que tendrán distribución Poisson o binomial negativa según exista o no sobredispersión de los datos.\nAñadiendo el argumento zi = ~ 1, podemos controlar el exceso de ceros en el modelo de regresión binomial negativa ajustada anteriormente. Indicando que la inflación en ceros sigue un modelo de intercepto único (sin predictores adicionales):\n\n# Modelo zero-inflated\nfit_zi_nb &lt;- glmmTMB(casos ~ semana_epi + grupo_edad_cat,  \n                  zi = ~ 1,\n                  data = datos,                 \n                  family = nbinom2)  \n\n# Salida del modelo\nsummary(fit_zi_nb)\n\n Family: nbinom2  ( log )\nFormula:          casos ~ semana_epi + grupo_edad_cat\nZero inflation:         ~1\nData: datos\n\n      AIC       BIC    logLik -2*log(L)  df.resid \n   1540.4    1580.0    -762.2    1524.4      1039 \n\n\nDispersion parameter for nbinom2 family (): 0.467 \n\nConditional model:\n                         Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)              -3.27950    0.33522  -9.783  &lt; 2e-16 ***\nsemana_epi                0.06159    0.01570   3.923 8.73e-05 ***\ngrupo_edad_cat10-19 años  1.16315    0.33067   3.518 0.000435 ***\ngrupo_edad_cat20-34 años  2.18318    0.30879   7.070 1.55e-12 ***\ngrupo_edad_cat35-65 años  2.30095    0.30722   7.490 6.90e-14 ***\ngrupo_edad_cat65+ años    0.71124    0.34949   2.035 0.041842 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nZero-inflation model:\n            Estimate Std. Error z value Pr(&gt;|z|)\n(Intercept)   -16.69    3025.54  -0.006    0.996\n\n\nLa salida de estos modelos se divide en tres partes que incluyen:\n\nConditional model: coeficientes, error estándar, valor z y significancia para el modelo de conteo binomial negativo.\nZero-inflation model: coeficiente para la probabilidad de observar un cero adicional, con su respectivo error estándar, valor z y significancia. El coeficiente del intercepto no es significativo, lo que indica que la probabilidad de observar un cero adicional no está significativamente influenciada por las variables en el modelo y podría ser necesario incluir un predictor en zi para mejorar el ajuste del modelo.\nEstadísticas del modelo: AIC, BIC, logLik, devianza y grados de libertad residuales.\n\n\n\n\n\n\n\nExisten diversas soluciones para tratar la sobredispersión, subdispersión y exceso de ceros en modelos de conteo. En este documento, nos enfocamos en las estrategias más comunes que pueden ser abordadas mediante modelos lineales generalizados utilizando el paquete glmmTMB.\n\n\n\n\n\n\nProblema\nModelo\n\n\n\nSobredispersión\nglmmTMB(formula, family = nbinom2)\n\n\nSubdispersión\nglmmTMB(formula, family = compois)\n\n\nSobredispersión + Exceso de Ceros\nglmmTMB(formula, family = nbinom2, zi = ~1)\n\n\n\n\n\n\n\nWickham et al. (2019)\nLüdecke et al. (2022)\nFirke (2024)\nSjoberg et al. (2021)\nAgresti (2015)\nSalinas-Rodríguez, Manrique-Espinoza, y Sosa-Rubí (2009)"
  },
  {
    "objectID": "unidad_5/03_dispersion.html#footnotes",
    "href": "unidad_5/03_dispersion.html#footnotes",
    "title": "Dispersión de datos",
    "section": "Notas",
    "text": "Notas\n\nFuente: https://es.wikipedia.org/wiki/Localidades_balnearias_del_mar_Argentino↩︎\nFuente: Vigilancia de las enfermedades por virus del Dengue y Zika. Datos Abiertos del Ministerio de Salud. Disponible en: http://datos.salud.gob.ar/dataset/vigilancia-de-dengue-y-zika↩︎"
  },
  {
    "objectID": "unidad_5/01_est_cohortes.html",
    "href": "unidad_5/01_est_cohortes.html",
    "title": "Estudios de cohortes",
    "section": "",
    "text": "En su forma más básica, un diseño de cohorte implica la selección de un grupo de personas expuestas al factor de estudio y un grupo similar pero no expuesto, considerado como el grupo de referencia. Ambos grupos serán seguidos durante un período determinado y se evaluará si durante el mismo se produjo o no el evento de interés. Al final del seguimiento, se realiza la comparación de ambos grupos para analizar los resultados.\nLa esencia de los estudios de cohortes radica en el seguimiento de los participantes a lo largo del tiempo. Por tratarse de estudios longitudinales, los mismos podrán ser prospectivos, retrospectivos o mixtos. Además, las cohortes podrán ser fijas o dinámicas. En una cohorte fija, se establece un período de reclutamiento y, una vez finalizado, no se admiten más participantes. En una cohorte dinámica (también llamada cohorte abierta), se permite la entrada y salida de sujetos a lo largo de todo el estudio.\nComo recordarán, la selección de los participantes se basa en la exposición, por lo cual la misma debe estar claramente definida. Como dijimos anteriormente, la idea clásica de un estudio de cohorte es comparar dos grupos, expuestos y no expuestos. Sin embargo, existen otras comparaciones posibles:\n\nEstudios de comparación con la población general: El estudio identifica y sigue a una cohorte expuesta, comparando la frecuencia del evento de salud en esta cohorte con la observada en la población general. Para ello, es esencial disponer de registros poblacionales que proporcionen esta información.\nComparaciones internas: La cohorte estudiada incluye tanto personas expuestas como no expuestas, y las comparaciones se realizan dentro de la propia cohorte.\n\nPor otra parte, el evento a observar puede tomar distintas formas: puede tratarse de un evento fijo, eventos múltiples, cambio en una medida de función (la cual se evalúa mediante tasa de cambio) o marcadores intermedios del evento. Estas variaciones también influyen en el abordaje analítico.\nAl diseñar un estudio de cohortes, es fundamental definir claramente los siguientes aspectos:\n\nCriterios de inclusión\nFechas de entrada y de salida de los participantes\nEstrategia de seguimiento\nEventos finales de interés\nDefinición de la exposición\nIdentificación de factores de confusión\nConsideraciones éticas\nEstrategias de análisis\nPotencia estadística\n\nEn la siguiente tabla se remarcan ventajas y desventajas de estos estudios:\n\n\n\n\nVentajas\nDesventajas\n\n\n\nMás cercanos a un experimento\nAlto costo y duración del estudio\n\n\nClaridad temporal entre exposición y evento\nRiesgo de pérdida de participantes\n\n\nPermiten estudiar varios eventos asociados a la exposición\nNecesidad de cohortes grandes\n\n\nEstimación directa de la incidencia\nDificultad para controlar confusores\n\n\nReducción de sesgos de selección y de memoria\nCambios en la exposición durante el estudio\n\n\nEficientes para evaluar exposiciones poco frecuentes\nNo adecuados para enfermedades raras\n\n\nPermiten fijar criterios de calidad en la medición del evento\n\n\n\n\n\n\n\nComo recordarán de los cursos de epidemiología previos, una de las formas más simples de análisis en un estudio de cohortes es el cálculo del Riesgo Relativo (RR). Sin embargo, hay aspectos adicionales de este diseño que son importantes de considerar.\nEl análisis de los estudios de cohortes debe tener en cuenta la variable tiempo. La pérdida de participantes por diversas razones y la diferente duración del seguimiento entre los miembros de la cohorte requieren el uso de técnicas analíticas especiales que permitan aprovechar toda la información disponible. Existen dos estrategias de análisis principales para el estudio de cohortes: el análisis persona-tiempo, que desarrollaremos en esta unidad y el análisis de supervivencia, que retomaremos en la Unidad 6 cuando veamos estudios experimentales. Ambos métodos requieren información precisa sobre el inicio y final del seguimiento de cada participante, así como su estado al momento de finalizar o salir del estudio (desarrollo o no el evento de interés). Además, es necesario definir una escala temporal adecuada para el análisis.\nLas técnicas de análisis persona-tiempo utilizan como unidad de análisis estratos o grupos homogéneos en los que se asume una tasa de incidencia uniforme. Las técnicas clásicas de análisis incluyen la comparación de tasas, la estandarización directa y las razones estandarizadas de incidencia o mortalidad (denominadas RIE o SIR, para la incidencia y RME o SMR para la mortalidad). El RIE y RME comparan el número de casos observados con el número esperado, teniendo en cuenta la frecuencia del evento en la población de referencia. La técnica multivariada por excelencia para la modelización de las tasas es la regresión de Poisson, que utiliza la distribución de Poisson para modelizar la variabilidad aleatoria del numerador de las tasas.\nA modo de resumen, compartimos esta tabla, extraída del libro de Hernández-Ávila (2011):\n\n\n\n\nAspecto\nAnálisis de supervivencia\nAnálisis tiempo-persona\n\n\n\nTamaño de la muestra\nRelativamente pequeño\nRelativamente grande\n\n\nTipo de eventos\nFrecuentes\nRaros\n\n\nEscala temporal\nÚnica\nÚnica o múltiple\n\n\nTipos de medida\nProbabilidad (condicional y acumulada)\nTasa (densidad)\n\n\nAnálisis bivariado\nComparación de curvas de supervivencia\nComparación de tasas\n\n\n\nLog-rank test\nRazón de tasas\n\n\n\nRazón de riesgos\nREM\n\n\nAnálisis multivariado\nRegresión de Cox\nRegresión de Poisson\n\n\n\n\n\n\nNolasco (2016)\nPérez Hoyos (2001)\nSalinas-Rodríguez, Manrique-Espinoza, y Sosa-Rubí (2009)"
  },
  {
    "objectID": "unidad_5/01_est_cohortes.html#introducción",
    "href": "unidad_5/01_est_cohortes.html#introducción",
    "title": "Estudios de cohortes",
    "section": "",
    "text": "En su forma más básica, un diseño de cohorte implica la selección de un grupo de personas expuestas al factor de estudio y un grupo similar pero no expuesto, considerado como el grupo de referencia. Ambos grupos serán seguidos durante un período determinado y se evaluará si durante el mismo se produjo o no el evento de interés. Al final del seguimiento, se realiza la comparación de ambos grupos para analizar los resultados.\nLa esencia de los estudios de cohortes radica en el seguimiento de los participantes a lo largo del tiempo. Por tratarse de estudios longitudinales, los mismos podrán ser prospectivos, retrospectivos o mixtos. Además, las cohortes podrán ser fijas o dinámicas. En una cohorte fija, se establece un período de reclutamiento y, una vez finalizado, no se admiten más participantes. En una cohorte dinámica (también llamada cohorte abierta), se permite la entrada y salida de sujetos a lo largo de todo el estudio.\nComo recordarán, la selección de los participantes se basa en la exposición, por lo cual la misma debe estar claramente definida. Como dijimos anteriormente, la idea clásica de un estudio de cohorte es comparar dos grupos, expuestos y no expuestos. Sin embargo, existen otras comparaciones posibles:\n\nEstudios de comparación con la población general: El estudio identifica y sigue a una cohorte expuesta, comparando la frecuencia del evento de salud en esta cohorte con la observada en la población general. Para ello, es esencial disponer de registros poblacionales que proporcionen esta información.\nComparaciones internas: La cohorte estudiada incluye tanto personas expuestas como no expuestas, y las comparaciones se realizan dentro de la propia cohorte.\n\nPor otra parte, el evento a observar puede tomar distintas formas: puede tratarse de un evento fijo, eventos múltiples, cambio en una medida de función (la cual se evalúa mediante tasa de cambio) o marcadores intermedios del evento. Estas variaciones también influyen en el abordaje analítico.\nAl diseñar un estudio de cohortes, es fundamental definir claramente los siguientes aspectos:\n\nCriterios de inclusión\nFechas de entrada y de salida de los participantes\nEstrategia de seguimiento\nEventos finales de interés\nDefinición de la exposición\nIdentificación de factores de confusión\nConsideraciones éticas\nEstrategias de análisis\nPotencia estadística\n\nEn la siguiente tabla se remarcan ventajas y desventajas de estos estudios:\n\n\n\n\nVentajas\nDesventajas\n\n\n\nMás cercanos a un experimento\nAlto costo y duración del estudio\n\n\nClaridad temporal entre exposición y evento\nRiesgo de pérdida de participantes\n\n\nPermiten estudiar varios eventos asociados a la exposición\nNecesidad de cohortes grandes\n\n\nEstimación directa de la incidencia\nDificultad para controlar confusores\n\n\nReducción de sesgos de selección y de memoria\nCambios en la exposición durante el estudio\n\n\nEficientes para evaluar exposiciones poco frecuentes\nNo adecuados para enfermedades raras\n\n\nPermiten fijar criterios de calidad en la medición del evento\n\n\n\n\n\n\n\nComo recordarán de los cursos de epidemiología previos, una de las formas más simples de análisis en un estudio de cohortes es el cálculo del Riesgo Relativo (RR). Sin embargo, hay aspectos adicionales de este diseño que son importantes de considerar.\nEl análisis de los estudios de cohortes debe tener en cuenta la variable tiempo. La pérdida de participantes por diversas razones y la diferente duración del seguimiento entre los miembros de la cohorte requieren el uso de técnicas analíticas especiales que permitan aprovechar toda la información disponible. Existen dos estrategias de análisis principales para el estudio de cohortes: el análisis persona-tiempo, que desarrollaremos en esta unidad y el análisis de supervivencia, que retomaremos en la Unidad 6 cuando veamos estudios experimentales. Ambos métodos requieren información precisa sobre el inicio y final del seguimiento de cada participante, así como su estado al momento de finalizar o salir del estudio (desarrollo o no el evento de interés). Además, es necesario definir una escala temporal adecuada para el análisis.\nLas técnicas de análisis persona-tiempo utilizan como unidad de análisis estratos o grupos homogéneos en los que se asume una tasa de incidencia uniforme. Las técnicas clásicas de análisis incluyen la comparación de tasas, la estandarización directa y las razones estandarizadas de incidencia o mortalidad (denominadas RIE o SIR, para la incidencia y RME o SMR para la mortalidad). El RIE y RME comparan el número de casos observados con el número esperado, teniendo en cuenta la frecuencia del evento en la población de referencia. La técnica multivariada por excelencia para la modelización de las tasas es la regresión de Poisson, que utiliza la distribución de Poisson para modelizar la variabilidad aleatoria del numerador de las tasas.\nA modo de resumen, compartimos esta tabla, extraída del libro de Hernández-Ávila (2011):\n\n\n\n\nAspecto\nAnálisis de supervivencia\nAnálisis tiempo-persona\n\n\n\nTamaño de la muestra\nRelativamente pequeño\nRelativamente grande\n\n\nTipo de eventos\nFrecuentes\nRaros\n\n\nEscala temporal\nÚnica\nÚnica o múltiple\n\n\nTipos de medida\nProbabilidad (condicional y acumulada)\nTasa (densidad)\n\n\nAnálisis bivariado\nComparación de curvas de supervivencia\nComparación de tasas\n\n\n\nLog-rank test\nRazón de tasas\n\n\n\nRazón de riesgos\nREM\n\n\nAnálisis multivariado\nRegresión de Cox\nRegresión de Poisson\n\n\n\n\n\n\nNolasco (2016)\nPérez Hoyos (2001)\nSalinas-Rodríguez, Manrique-Espinoza, y Sosa-Rubí (2009)"
  },
  {
    "objectID": "unidad_4/04_reg_logistica_cond.html",
    "href": "unidad_4/04_reg_logistica_cond.html",
    "title": "Regresión logística condicional",
    "section": "",
    "text": "En algunos estudios de casos y controles, se utiliza un diseño apareado para controlar la influencia de factores de confusión como la edad, el sexo o el lugar de residencia. La regresión logística condicional (RLC) es una extensión de la regresión logística clásica que incorpora el apareamiento de los datos, estratificando el análisis por grupos definidos por las variables de emparejamiento (estratos).\nEste modelo permite estimar el efecto de las covariables sobre una variable dependiente binaria mientras controla de manera efectiva los efectos de las variables de emparejamiento o agrupamiento. Es especialmente útil en estudios de casos y controles apareados, investigaciones multicéntricas y estudios observacionales que abarcan múltiples ubicaciones geográficas, proporcionando un enfoque robusto para manejar la heterogeneidad dentro de los datos."
  },
  {
    "objectID": "unidad_4/04_reg_logistica_cond.html#introducción",
    "href": "unidad_4/04_reg_logistica_cond.html#introducción",
    "title": "Regresión logística condicional",
    "section": "",
    "text": "En algunos estudios de casos y controles, se utiliza un diseño apareado para controlar la influencia de factores de confusión como la edad, el sexo o el lugar de residencia. La regresión logística condicional (RLC) es una extensión de la regresión logística clásica que incorpora el apareamiento de los datos, estratificando el análisis por grupos definidos por las variables de emparejamiento (estratos).\nEste modelo permite estimar el efecto de las covariables sobre una variable dependiente binaria mientras controla de manera efectiva los efectos de las variables de emparejamiento o agrupamiento. Es especialmente útil en estudios de casos y controles apareados, investigaciones multicéntricas y estudios observacionales que abarcan múltiples ubicaciones geográficas, proporcionando un enfoque robusto para manejar la heterogeneidad dentro de los datos."
  },
  {
    "objectID": "unidad_4/04_reg_logistica_cond.html#modelo-logístico-condicional",
    "href": "unidad_4/04_reg_logistica_cond.html#modelo-logístico-condicional",
    "title": "Regresión logística condicional",
    "section": "Modelo logístico condicional",
    "text": "Modelo logístico condicional\nEl modelo logístico condicional utiliza una función de verosimilitud condicional que elimina la necesidad de estimar los efectos de los factores de emparejamiento como interceptos adicionales. Este enfoque resulta especialmente útil cuando hay un gran número de grupos o estratos, ya que permite concentrarse exclusivamente en los coeficientes de las covariables de interés.\nSe modela la probabilidad de que un evento ocurra en función de las covariables dentro de cada estrato, definido por las variables de emparejamiento. La función de verosimilitud condicional se puede expresar matemáticamente como:\n\\[\nlogit(p) = \\alpha_1 + \\alpha_2 z_2 + ... + \\alpha_s z_s + \\beta_1 x_1 + ... + \\beta_p x_p\n\\]\ndonde:\n\n\\(\\alpha_1 + \\alpha_2 z_2 + ... + \\alpha_s z_s\\): Coeficientes asociados a las variables de estratificación \\(z_s\\), que identifican los estratos.\n\\(z_1, z_2, ..., z_s\\): Variables de emparejamiento o estratos.\n\\(\\beta_1, ..., \\beta_p\\): Efectos de las variables independientes ajustadas por los estratos.\n\\(x_1, ... x_p\\): Variables independientes incluídas en el modelo.\n\nEl uso de la verosimilitud condicional simplifica el modelo al evitar la estimación de un gran número de interceptos y reduce el sesgo en situaciones con pocos datos por estrato.\nSupuestos de la regresión logística condicional\n\nIndependencia dentro de los estratos: Las observaciones son independientes entre los diferentes estratos o grupos definidos por el emparejamiento, pero pueden estar correlacionadas dentro de los mismos.\nRelación logit-lineal: Existe una relación lineal entre el logit de la probabilidad del evento y las covariables explicativas.\nNo existencia de confusión residual: Las variables de emparejamiento deben ser suficientes para controlar por los factores de confusión.\nTamaño adecuado del estrato: Cada estrato debe tener al menos un caso y un control."
  },
  {
    "objectID": "unidad_4/04_reg_logistica_cond.html#ejemplo-práctico-en-r",
    "href": "unidad_4/04_reg_logistica_cond.html#ejemplo-práctico-en-r",
    "title": "Regresión logística condicional",
    "section": "Ejemplo práctico en R",
    "text": "Ejemplo práctico en R\nEn R podemos ajustar modelos de regresión logística condicional usando la función clogit() del paquete survival (Therneau 2024).\nCargamos los paquetes necesarios:\n\n# Análisis estadístico\nlibrary(gtsummary)\nlibrary(survival)\n\n# Chequeo de supuestos\nlibrary(easystats)\n\n# Apareamiento de datos\nlibrary(MatchIt)\n\n# Manejo y exploración de datos\nlibrary(skimr)\nlibrary(janitor)\nlibrary(tidyverse)\n\nUsaremos la base “higado_graso.txt” que contiene datos de un estudio de casos y controles apareados por sexo y edad para muertes por hígado graso no alcohólico:\n\n# Carga datos\ndatos &lt;- read_delim(\"datos/higado_graso.txt\")|&gt; \n  \n  # Cambia nivel de referencia de fib4\n  mutate(fib4_cat = fct_rev(fib4_cat))\n\n# Explora datos\nglimpse(datos)\n\nRows: 2,467\nColumns: 11\n$ id           &lt;dbl&gt; 19, 25, 26, 32, 56, 57, 68, 77, 78, 99, 100, 120, 126, 13…\n$ tipo         &lt;dbl&gt; 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, …\n$ edad         &lt;dbl&gt; 35, 39, 55, 45, 51, 81, 51, 55, 32, 33, 50, 82, 56, 69, 7…\n$ sexo         &lt;dbl&gt; 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, …\n$ bmi          &lt;dbl&gt; 30.49939, 24.26164, 30.59225, 22.11900, 36.42498, 26.3815…\n$ hdl          &lt;dbl&gt; 50.09091, 52.50000, 49.13043, 70.55556, 45.10526, 67.7500…\n$ p_sistolica  &lt;dbl&gt; NA, NA, 134.2500, NA, 164.0000, 148.0833, 124.5000, NA, N…\n$ p_diastolica &lt;dbl&gt; NA, NA, 134.2500, NA, 164.0000, 148.0833, 124.5000, NA, N…\n$ fib4_cat     &lt;fct&gt; bajo riesgo, bajo riesgo, bajo riesgo, bajo riesgo, bajo …\n$ fuma         &lt;dbl&gt; 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, …\n$ status       &lt;dbl&gt; 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, …\n\n\nEl dataset tiene 2467 observaciones y 11 columnas. Las variables de interés son las siguientes:\n\nid: identificador único del paciente.\ntipo: Indica si el paciente tiene hígado graso no alcohólico (caso: 1) o no presenta la condición (control: 0).\nedad: Edad del paciente al momento de su ingreso al hospital.\nsexo: Sexo biológico del paciente (1: masculino, 0: femenino).\nbmi: Índice de masa corporal del paciente al momento de la hospitalización.\nhdl: Nivel de lipoproteínas de alta densidad (HDL) en sangre.\np_sistolica: Presión sistólica en milímetros de mercurio (mmHg).\np_diastolica: Presión diastólica en milímetros de mercurio (mmHg).\nfib4_cat: Índice de fibrosis hepática categorizado (bajo riesgo, alto riesgo).\nfuma: Tabaquismo (0: no, 1: sí).\nstatus: Variable dependiente que toma el valor de 0 si la persona sobrevivió y 1 si falleció.\n\nExploremos más en profundidad los datos usando la función tabyl() del paquete janitor y la función skim() del paquete skimr:\n\n# Número de casos y controles\ntabyl(datos$tipo) |&gt; \n  adorn_pct_formatting()\n\n datos$tipo    n percent\n          0 1685   68.3%\n          1  782   31.7%\n\n# Niveles fib4\ntabyl(datos$fib4_cat)\n\n datos$fib4_cat    n   percent\n    bajo riesgo 2109 0.8548845\n    alto riesgo  358 0.1451155\n\n# Explora valores ausentes\nskim(datos)\n\n\nData summary\n\n\nName\ndatos\n\n\nNumber of rows\n2467\n\n\nNumber of columns\n11\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n1\n\n\nnumeric\n10\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\nfib4_cat\n0\n1\nFALSE\n2\nbaj: 2109, alt: 358\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\nid\n0\n1.00\n8644.27\n4998.09\n19.00\n4486.00\n8507.00\n12864.00\n17563.0\n▇▇▇▇▇\n\n\ntipo\n0\n1.00\n0.32\n0.47\n0.00\n0.00\n0.00\n1.00\n1.0\n▇▁▁▁▃\n\n\nedad\n0\n1.00\n53.60\n14.73\n18.00\n44.00\n53.00\n64.00\n98.0\n▂▆▇▃▁\n\n\nsexo\n0\n1.00\n0.50\n0.50\n0.00\n0.00\n0.00\n1.00\n1.0\n▇▁▁▁▇\n\n\nbmi\n0\n1.00\n31.22\n7.51\n9.21\n26.02\n30.14\n35.04\n84.4\n▂▇▁▁▁\n\n\nhdl\n0\n1.00\n50.92\n14.94\n19.83\n40.00\n48.53\n58.50\n134.0\n▅▇▂▁▁\n\n\np_sistolica\n928\n0.62\n131.41\n17.11\n40.00\n121.53\n132.50\n142.67\n211.0\n▁▁▇▃▁\n\n\np_diastolica\n928\n0.62\n131.41\n17.11\n40.00\n121.53\n132.50\n142.67\n211.0\n▁▁▇▃▁\n\n\nfuma\n0\n1.00\n0.64\n0.48\n0.00\n0.00\n1.00\n1.00\n1.0\n▅▁▁▁▇\n\n\nstatus\n0\n1.00\n0.12\n0.32\n0.00\n0.00\n0.00\n0.00\n1.0\n▇▁▁▁▁\n\n\n\n\n\nLas variables p_sistolica y p_diastolica tienen gran cantidad de valores ausentes, por lo que no serán consideradas para el análisis.\nComo no tenemos definido a que caso corresponde cada control, vamos a aparear casos y controles usando la función matchit() del paquete MatchIt (Ho et al. 2011), especificando que queremos dos controles por caso con el argumento ratio = 2:\n\n# Aparea casos y controles por sexo y edad\nmatch &lt;- matchit(tipo ~ edad + sexo, \n                 data = datos, \n                 exact = \"sexo\",\n                 ratio = 2)\n\n# Resumen del apareamiento\nsummary(match)\n\n\nCall:\nmatchit(formula = tipo ~ edad + sexo, data = datos, exact = \"sexo\", \n    ratio = 2)\n\nSummary of Balance for All Data:\n         Means Treated Means Control Std. Mean Diff. Var. Ratio eCDF Mean\ndistance        0.3268        0.3124          0.2552     1.0589    0.0507\nedad           51.0243       54.7923         -0.2550     1.0302    0.0486\nsexo            0.4962        0.5003         -0.0083          .    0.0041\n         eCDF Max\ndistance   0.1284\nedad       0.1274\nsexo       0.0041\n\nSummary of Balance for Matched Data:\n         Means Treated Means Control Std. Mean Diff. Var. Ratio eCDF Mean\ndistance        0.3268        0.3181          0.1549     1.1495    0.0301\nedad           51.0243       53.2270         -0.1490     1.1373    0.0288\nsexo            0.4962        0.4962          0.0000          .    0.0000\n         eCDF Max Std. Pair Dist.\ndistance   0.0882          0.1578\nedad       0.0838          0.1528\nsexo       0.0000          0.0000\n\nSample Sizes:\n          Control Treated\nAll          1685     782\nMatched      1564     782\nUnmatched     121       0\nDiscarded       0       0\n\n\nObtenemos los datos apareados:\n\ndatos &lt;- match.data(match, drop.unmatched = T) \n\nnames(datos)\n\n [1] \"id\"           \"tipo\"         \"edad\"         \"sexo\"         \"bmi\"         \n [6] \"hdl\"          \"p_sistolica\"  \"p_diastolica\" \"fib4_cat\"     \"fuma\"        \n[11] \"status\"       \"distance\"     \"weights\"      \"subclass\"    \n\n\nAjustamos el modelo de regresión logística condicional, usando la variable subclass como variable de estratificación:\n\nmod_cond &lt;- clogit(status ~ bmi + hdl + fuma + fib4_cat +\n                     strata(subclass),\n                   data = datos)\n\nRevisemos la salida del modelo:\n\nsummary(mod_cond)\n\nCall:\ncoxph(formula = Surv(rep(1, 2346L), status) ~ bmi + hdl + fuma + \n    fib4_cat + strata(subclass), data = datos, method = \"exact\")\n\n  n= 2346, number of events= 251 \n\n                         coef exp(coef)  se(coef)      z Pr(&gt;|z|)    \nbmi                  0.005507  1.005522  0.012814  0.430    0.667    \nhdl                 -0.006502  0.993519  0.006980 -0.931    0.352    \nfuma                -0.237017  0.788978  0.186629 -1.270    0.204    \nfib4_catalto riesgo  1.072163  2.921692  0.224099  4.784 1.72e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n                    exp(coef) exp(-coef) lower .95 upper .95\nbmi                    1.0055     0.9945    0.9806     1.031\nhdl                    0.9935     1.0065    0.9800     1.007\nfuma                   0.7890     1.2675    0.5473     1.137\nfib4_catalto riesgo    2.9217     0.3423    1.8831     4.533\n\nConcordance= 0.625  (se = 0.039 )\nLikelihood ratio test= 27.94  on 4 df,   p=1e-05\nWald test            = 25.92  on 4 df,   p=3e-05\nScore (logrank) test = 28.23  on 4 df,   p=1e-05\n\n\nProbemos eliminar una de las variables explicativas no significativas a la vez:\n\n# (-) Tabaquismo\nmod_cond1 &lt;- clogit(status ~ bmi + hdl + fib4_cat +\n                     strata(subclass),\n                   data = datos)\n\n# (-) IMC\nmod_cond2 &lt;- clogit(status ~ hdl + fuma + fib4_cat +\n                     strata(sexo, edad),\n                   data = datos)\n\n# (-) HDL\nmod_cond3 &lt;- clogit(status ~ bmi + fuma +  fib4_cat +\n                     strata(sexo, edad),\n                   data = datos)\n\nComparamos los modelos\n\n# Compara modelos\ncompare_performance(mod_cond, mod_cond1, mod_cond2, mod_cond3,\n                    metrics = \"common\")\n\n# Comparison of Model Performance Indices\n\nName      |  Model | AIC (weights) | BIC (weights) | Nagelkerke's R2 |  RMSE\n----------------------------------------------------------------------------\nmod_cond  | clogit | 393.1 (0.452) | 416.2 (0.044) |           0.073 | 0.230\nmod_cond1 | clogit | 392.8 (0.548) | 410.0 (0.956) |           0.069 | 0.230\nmod_cond2 | clogit | 921.1 (&lt;.001) | 938.3 (&lt;.001) |           0.055 | 0.276\nmod_cond3 | clogit | 923.5 (&lt;.001) | 940.8 (&lt;.001) |           0.052 | 0.276\n\n# Compara modelos y ordena según performance\ncompare_performance(mod_cond, mod_cond1, mod_cond2, mod_cond3,\n                    metrics = \"common\", rank = T)\n\n# Comparison of Model Performance Indices\n\nName      |  Model | Nagelkerke's R2 |  RMSE | AIC weights | BIC weights | Performance-Score\n--------------------------------------------------------------------------------------------\nmod_cond1 | clogit |           0.069 | 0.230 |       0.548 |       0.956 |            95.02%\nmod_cond  | clogit |           0.073 | 0.230 |       0.452 |       0.044 |            71.65%\nmod_cond2 | clogit |           0.055 | 0.276 |   1.04e-115 |   1.81e-115 |             3.55%\nmod_cond3 | clogit |           0.052 | 0.276 |   3.12e-116 |   5.45e-116 |             0.40%\n\n\nEl modelo sin fuma tiene mejor performance que el modelo saturado:\n\n# Salida modelo condicional 1\nsummary(mod_cond1)\n\nCall:\ncoxph(formula = Surv(rep(1, 2346L), status) ~ bmi + hdl + fib4_cat + \n    strata(subclass), data = datos, method = \"exact\")\n\n  n= 2346, number of events= 251 \n\n                         coef exp(coef)  se(coef)      z Pr(&gt;|z|)    \nbmi                  0.004445  1.004455  0.012740  0.349    0.727    \nhdl                 -0.006922  0.993102  0.006941 -0.997    0.319    \nfib4_catalto riesgo  1.064462  2.899278  0.223595  4.761 1.93e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n                    exp(coef) exp(-coef) lower .95 upper .95\nbmi                    1.0045     0.9956    0.9797     1.030\nhdl                    0.9931     1.0069    0.9797     1.007\nfib4_catalto riesgo    2.8993     0.3449    1.8705     4.494\n\nConcordance= 0.641  (se = 0.038 )\nLikelihood ratio test= 26.33  on 3 df,   p=8e-06\nWald test            = 24.56  on 3 df,   p=2e-05\nScore (logrank) test = 26.63  on 3 df,   p=7e-06\n\n# (-) BMI\nmod_cond1a &lt;- clogit(status ~ hdl + fib4_cat +\n                     strata(subclass),\n                   data = datos)\n\n# (-) HDL\nmod_cond1b &lt;- clogit(status ~ bmi + fib4_cat +\n                     strata(subclass),\n                   data = datos)\n\n# Compara modelos y ordena según performance\ncompare_performance(mod_cond1, mod_cond1a, mod_cond1b, \n                    metrics = \"common\", rank = T)\n\n# Comparison of Model Performance Indices\n\nName       |  Model | Nagelkerke's R2 |  RMSE | AIC weights | BIC weights | Performance-Score\n---------------------------------------------------------------------------------------------\nmod_cond1a | clogit |           0.069 | 0.230 |       0.492 |       0.601 |            72.00%\nmod_cond1  | clogit |           0.069 | 0.230 |       0.192 |       0.013 |            50.00%\nmod_cond1b | clogit |           0.066 | 0.230 |       0.316 |       0.386 |            32.90%\n\n# Salida modelo condicional 1b\nsummary(mod_cond1b)\n\nCall:\ncoxph(formula = Surv(rep(1, 2346L), status) ~ bmi + fib4_cat + \n    strata(subclass), data = datos, method = \"exact\")\n\n  n= 2346, number of events= 251 \n\n                        coef exp(coef) se(coef)     z Pr(&gt;|z|)    \nbmi                 0.007979  1.008011 0.012127 0.658    0.511    \nfib4_catalto riesgo 1.067510  2.908129 0.223684 4.772 1.82e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n                    exp(coef) exp(-coef) lower .95 upper .95\nbmi                     1.008     0.9921    0.9843     1.032\nfib4_catalto riesgo     2.908     0.3439    1.8759     4.508\n\nConcordance= 0.601  (se = 0.04 )\nLikelihood ratio test= 25.32  on 2 df,   p=3e-06\nWald test            = 23.74  on 2 df,   p=7e-06\nScore (logrank) test = 25.68  on 2 df,   p=3e-06\n\n# (-) bmi\nmod_cond1c &lt;- clogit(status ~ fib4_cat +\n                     strata(subclass),\n                   data = datos)\n\n# Compara modelos\ncompare_performance(mod_cond1b, mod_cond1c,\n                    metrics = \"common\")\n\n# Comparison of Model Performance Indices\n\nName       |  Model | AIC (weights) | BIC (weights) | Nagelkerke's R2 |  RMSE\n-----------------------------------------------------------------------------\nmod_cond1b | clogit | 391.8 (0.313) | 403.3 (0.025) |           0.066 | 0.230\nmod_cond1c | clogit | 390.2 (0.687) | 396.0 (0.975) |           0.065 | 0.230\n\n\nNos quedamos con el modelo que tiene solo fib4_cat como variable explicativa:\n\ntbl_regression(mod_cond1c, exponentiate = T)\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\nOR\n95% CI\np-value\n\n\n\nfib4_cat\n\n\n\n\n\n    bajo riesgo\n—\n—\n\n\n\n    alto riesgo\n2.94\n1.90, 4.55\n&lt;0.001\n\n\n\nAbbreviations: CI = Confidence Interval, OR = Odds Ratio\n\n\n\n\n\nEn el modelo final, las personas con alto riesgo de fibrosis hepática presentan 3,7 veces más probabilidades de morir (\\(95% IC: 2,45-5,57\\)) que aquellas sin la condición.\n\nAgresti (2015)\nWickham et al. (2019)\nLüdecke et al. (2022)"
  },
  {
    "objectID": "unidad_4/02_reg_logistica.html",
    "href": "unidad_4/02_reg_logistica.html",
    "title": "Regresión logística",
    "section": "",
    "text": "Cuando la variable dependiente es dicotómica o binaria, es decir, tiene dos categorías mutuamente excluyentes (éxito/fracaso; sí/no; positivo/negativo, etc.), los modelos de regresión lineal no son el abordaje más adecuado para el análisis. Consideremos un evento de salud que puede ocurrir o no (variable dependiente). Por ejemplo:\n\nUn paciente hospitalizado muere/no muere antes del alta.\nUn niño nace con/sin una malformación congénita.\nUn sujeto operado se infecta/no se infecta en el postoperatorio.\nUn niño camina/no camina a los 11 meses.\nUna droga mejora/no mejora los síntomas depresivos.\n\nTeniendo en cuenta que nuestra variable respuesta es dicotómica:\n\\[\nY = 1 \\rightarrow Si~el~hecho~ocurre \\\\\nY  = 0 \\rightarrow Si~el~hecho~no~ocurre\n\\]\nSi representáramos los datos con una función lineal, obtendríamos el siguiente gráfico:\n\n\n\n\n\n\n\n\nMatemáticamente, la función exponencial representa mejor esta relación:\n\n\n\n\n\n\n\n\nLa regresión logística se utiliza en los casos en que la variable dependiente es binaria, mientras que las variables independientes pueden ser de cualquier tipo (categóricas, dicotómicas, numéricas discretas o continuas). En vez de la ecuación de la recta, ahora tenemos otra ecuación que expresa la variable respuesta (\\(Y\\)) en función de la/las variables independientes. Esta ecuación, en realidad, expresa la probabilidad de que ocurra un hecho en función de ciertas variables que se presumen relevantes.\nLa expresión analítica es:\n\\[\nP(Y=1)_x=\\frac{1}{1+e^{(-\\alpha-\\beta_1X_1-\\beta_2X_2-\\dots-\\beta_kX_k)}}\n\\]\nPara comprender lo que significan los coeficientes \\(\\beta\\) del modelo, vamos a hacer algunas operaciones matemáticas. Comenzaremos por realizar una transformación logística, es decir, dividir ambos miembros de la ecuación por \\(1-P_{(Y=1)}\\):\n\\[\n\\frac{P(Y=1)_x}{1-P(Y=1)_x}=\\frac{\\frac{1}{1+e^{(-\\alpha-\\beta_1X_1-\\beta_2X_2-\\dots-\\beta_kX_k)}}}{1-\\frac{1}{1+e^{(-\\alpha-\\beta_1X_1-\\beta_2X_2-\\dots-\\beta_kX_k)}} }\n\\]\nSi ahora aplicamos logaritmo natural (\\(ln\\)) a ambos miembros de la ecuación, y aplicamos propiedades de los logaritmos, nos queda:\n\\[\nln\\bigg[\\frac{P_x}{1-P_x}\\bigg] = \\alpha + \\sum\\beta_ix_i\n\\]\nSi observamos el término que está entre corchetes, recordaremos que el cociente entre la probabilidad que un suceso ocurra, y la probabilidad de que no ocurra, es lo que conocemos como Odds, entonces:\n\\[\nln(Odds) = \\alpha+\\sum\\beta_ix_i\n\\]\nSi despejamos Odds de la ecuación anterior, podemos concluir entonces que:\n\\[\nOdds = e^{(\\alpha+\\sum\\beta_ix_i)}\n\\]\nPara comprender mejor cómo se interpretarán los coeficientes en la regresión logística, supongamos que queremos modelar la probabilidad de que un evento ocurra, \\(P_{(Y=1)}\\), en función de una única variable independiente dicotómica, que toma el valor 0 cuando la condición está ausente (\\(x=0\\)) y el valor 1 cuando está presente (\\(x=1\\)). Entonces:\nPara \\(x=1\\)\n\\[\nOdds_{evento/expuestos}=e^{(\\alpha+\\beta)}\n\\]\nPara \\(x=0\\)\n\\[\nOdds_{evento/expuestos}=e^{(\\alpha)}\n\\]\nEntonces, si queremos calcular el odds-ratio (OR):\n\\[\nOR = \\frac{e^{(\\alpha+\\beta)}}{e^\\alpha}=e^\\beta\n\\]\nPor lo tanto:\n\\[\nln\\; OR = \\beta\n\\]\nDe esta forma, vemos que \\(\\beta\\) = incremento del logaritmo del OR por cada unidad de incremento de \\(x\\).\nExtendiendo el razonamiento para la regresión logística múltiple, es decir cuando modelamos en función de más de una variable independiente, volvemos a la ecuación:\n\\[\nln(Odds) = \\alpha + \\sum\\beta_ix_i\n\\]\nDonde cada \\(\\beta_i\\): Incremento en log-odds para una unidad de incremento en \\(x_i\\) con todas las otras \\(x_i\\) constantes.\nIncluimos esta deducción para facilitar la comprensión del tema, pero para quienes el lenguaje matemático les es adverso, pueden hacer un “acto de fe” y quedarse con las conclusiones."
  },
  {
    "objectID": "unidad_4/02_reg_logistica.html#introducción",
    "href": "unidad_4/02_reg_logistica.html#introducción",
    "title": "Regresión logística",
    "section": "",
    "text": "Cuando la variable dependiente es dicotómica o binaria, es decir, tiene dos categorías mutuamente excluyentes (éxito/fracaso; sí/no; positivo/negativo, etc.), los modelos de regresión lineal no son el abordaje más adecuado para el análisis. Consideremos un evento de salud que puede ocurrir o no (variable dependiente). Por ejemplo:\n\nUn paciente hospitalizado muere/no muere antes del alta.\nUn niño nace con/sin una malformación congénita.\nUn sujeto operado se infecta/no se infecta en el postoperatorio.\nUn niño camina/no camina a los 11 meses.\nUna droga mejora/no mejora los síntomas depresivos.\n\nTeniendo en cuenta que nuestra variable respuesta es dicotómica:\n\\[\nY = 1 \\rightarrow Si~el~hecho~ocurre \\\\\nY  = 0 \\rightarrow Si~el~hecho~no~ocurre\n\\]\nSi representáramos los datos con una función lineal, obtendríamos el siguiente gráfico:\n\n\n\n\n\n\n\n\nMatemáticamente, la función exponencial representa mejor esta relación:\n\n\n\n\n\n\n\n\nLa regresión logística se utiliza en los casos en que la variable dependiente es binaria, mientras que las variables independientes pueden ser de cualquier tipo (categóricas, dicotómicas, numéricas discretas o continuas). En vez de la ecuación de la recta, ahora tenemos otra ecuación que expresa la variable respuesta (\\(Y\\)) en función de la/las variables independientes. Esta ecuación, en realidad, expresa la probabilidad de que ocurra un hecho en función de ciertas variables que se presumen relevantes.\nLa expresión analítica es:\n\\[\nP(Y=1)_x=\\frac{1}{1+e^{(-\\alpha-\\beta_1X_1-\\beta_2X_2-\\dots-\\beta_kX_k)}}\n\\]\nPara comprender lo que significan los coeficientes \\(\\beta\\) del modelo, vamos a hacer algunas operaciones matemáticas. Comenzaremos por realizar una transformación logística, es decir, dividir ambos miembros de la ecuación por \\(1-P_{(Y=1)}\\):\n\\[\n\\frac{P(Y=1)_x}{1-P(Y=1)_x}=\\frac{\\frac{1}{1+e^{(-\\alpha-\\beta_1X_1-\\beta_2X_2-\\dots-\\beta_kX_k)}}}{1-\\frac{1}{1+e^{(-\\alpha-\\beta_1X_1-\\beta_2X_2-\\dots-\\beta_kX_k)}} }\n\\]\nSi ahora aplicamos logaritmo natural (\\(ln\\)) a ambos miembros de la ecuación, y aplicamos propiedades de los logaritmos, nos queda:\n\\[\nln\\bigg[\\frac{P_x}{1-P_x}\\bigg] = \\alpha + \\sum\\beta_ix_i\n\\]\nSi observamos el término que está entre corchetes, recordaremos que el cociente entre la probabilidad que un suceso ocurra, y la probabilidad de que no ocurra, es lo que conocemos como Odds, entonces:\n\\[\nln(Odds) = \\alpha+\\sum\\beta_ix_i\n\\]\nSi despejamos Odds de la ecuación anterior, podemos concluir entonces que:\n\\[\nOdds = e^{(\\alpha+\\sum\\beta_ix_i)}\n\\]\nPara comprender mejor cómo se interpretarán los coeficientes en la regresión logística, supongamos que queremos modelar la probabilidad de que un evento ocurra, \\(P_{(Y=1)}\\), en función de una única variable independiente dicotómica, que toma el valor 0 cuando la condición está ausente (\\(x=0\\)) y el valor 1 cuando está presente (\\(x=1\\)). Entonces:\nPara \\(x=1\\)\n\\[\nOdds_{evento/expuestos}=e^{(\\alpha+\\beta)}\n\\]\nPara \\(x=0\\)\n\\[\nOdds_{evento/expuestos}=e^{(\\alpha)}\n\\]\nEntonces, si queremos calcular el odds-ratio (OR):\n\\[\nOR = \\frac{e^{(\\alpha+\\beta)}}{e^\\alpha}=e^\\beta\n\\]\nPor lo tanto:\n\\[\nln\\; OR = \\beta\n\\]\nDe esta forma, vemos que \\(\\beta\\) = incremento del logaritmo del OR por cada unidad de incremento de \\(x\\).\nExtendiendo el razonamiento para la regresión logística múltiple, es decir cuando modelamos en función de más de una variable independiente, volvemos a la ecuación:\n\\[\nln(Odds) = \\alpha + \\sum\\beta_ix_i\n\\]\nDonde cada \\(\\beta_i\\): Incremento en log-odds para una unidad de incremento en \\(x_i\\) con todas las otras \\(x_i\\) constantes.\nIncluimos esta deducción para facilitar la comprensión del tema, pero para quienes el lenguaje matemático les es adverso, pueden hacer un “acto de fe” y quedarse con las conclusiones."
  },
  {
    "objectID": "unidad_4/02_reg_logistica.html#modelos-lineales-generalizados",
    "href": "unidad_4/02_reg_logistica.html#modelos-lineales-generalizados",
    "title": "Regresión logística",
    "section": "Modelos lineales generalizados",
    "text": "Modelos lineales generalizados\nLa regresión logística forma parte de la familia de modelos lineales generalizados (GLM por su nombre en inglés, Generalized Linear Models), utilizados para predecir la probabilidad de que ocurra un evento binario (como infectado/no infectado, enfermo/no enfermo, sobreviviente/fallecido, etc.) en función de una o más variables independientes. Los GLM extienden los modelos de regresión lineales al permitir el uso de distribuciones no normales de errores (como binomiales, Poisson, gamma, entre otras) y varianzas no constantes. Estos modelos se caracterizan por una estructura de errores específica y una función de enlace que conecta la variable respuesta con la(s) variable(s) independiente(s).\nEn el caso de la regresión logística, donde la variable respuesta es binaria (0, 1), la estructura de errores pertenece a la familia de distribución binomial. La función de enlace típica para linealizar la relación entre la variable respuesta y la(s) variable(s) independiente(s) es la función logit, que es el logaritmo natural del odds-ratio (OR). Esta función de enlace transforma la escala de probabilidades (de 0 a 1) a una escala lineal (de \\(-\\infty\\) a \\(+\\infty\\)), lo que facilita la modelización de la relación entre las variables independientes y la variable respuesta binaria."
  },
  {
    "objectID": "unidad_4/02_reg_logistica.html#componentes-del-modelo-logístico",
    "href": "unidad_4/02_reg_logistica.html#componentes-del-modelo-logístico",
    "title": "Regresión logística",
    "section": "Componentes del modelo logístico",
    "text": "Componentes del modelo logístico\nAl igual que en la regresión lineal múltiple (RLM), antes de adentrarnos en el modelado, exploraremos cómo interpretar la salida de R para un modelo de regresión logística obtenida a partir de la función summary(modelo):\n\n\n\nCall:\nglm(formula = y ~ x1 + x2, family = binomial, data = datos)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  -0.3362     0.2816  -1.194   0.2325    \nx1B           0.4267     0.4012   1.064   0.2875    \nx1C           0.7016     0.3996   1.756   0.0791 .  \nx2            1.2669     0.2103   6.023 1.71e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 277.08  on 199  degrees of freedom\nResidual deviance: 221.48  on 196  degrees of freedom\nAIC: 229.48\n\nNumber of Fisher Scoring iterations: 4\n\n\nLos elementos a tener en cuenta son:\n\nCall: Muestra la fórmula del modelo utilizado.\nEstimate: Contiene los coeficientes \\(\\beta\\) estimados, incluyendo el intercepto (\\(\\beta_0\\)) y las variables explicativas (\\(\\beta_i\\)). Al aplicar a estos coeficientes la función exp(), que representa la función inversa del logaritmo natural, se obtiene el Odds Ratio (OR). Esto implica que los coeficientes de las variables independientes se interpretan como el OR de que ocurra el evento para cada incremento de la variable independiente, manteniendo constantes las demás variables independientes.\nStd. Error: Representa el error estándar asociado a cada coeficiente.\nz value: Corresponde al estadístico del test de Wald para evaluar la significancia de cada coeficiente.\nPr(&gt;|z|): Proporciona los p-valores asociados al test de Wald, que indican si los coeficientes son significativamente diferentes de cero.\n\nAdemás, en la parte final de la salida se presentan:\n\nNull deviance: La devianza del modelo nulo, que asume que ninguna de las variables explicativas tiene efecto.\nResidual deviance: La devianza del modelo ajustado, que indica cuánto se reduce la devianza al incluir las variables explicativas.\nAIC: Una medida de la calidad del modelo, penalizando por la complejidad para favorecer modelos más parsimoniosos.\nNumber of Fisher Scoring iterations: Número de iteraciones realizadas durante el proceso de estimación de parámetros del modelo.\n\nA continuación, profundizaremos en el significado de algunos de estos conceptos.\nTest de Wald\nEl test de Wald se utiliza para evaluar la significación de una variable dentro del modelo. De manera similar al test \\(F\\) parcial en la regresión lineal múltiple, este test contrasta la hipótesis nula:\n\\[\nH_0 : \\beta_i = 0\n\\]\nExpresamos la prueba en términos del coeficiente \\(\\beta_i\\) porque esta formulación es análoga a la utilizada en regresión lineal. Sin embargo, en regresión logística, los resultados suelen presentarse en términos de odds-ratio (OR).\nDado que una pendiente \\(\\beta_i = 0\\) implica un OR de 1, la hipótesis nula también puede expresarse como “la razón de probabilidades es 1”. Esto significa que la variable explicativa no tiene capacidad predictiva, ya que las probabilidades de los grupos comparados son iguales.\nLa interpretación sigue criterios comunes en inferencia estadística: valores de \\(p&lt;0,05\\) sugieren evidencia suficiente para rechazar la hipótesis nula, indicando que la variable contribuye significativamente al modelo (\\(p &lt; 0,1\\) podría considerarse en contextos más flexibles). Es fundamental recordar que, en regresión logística, los coeficientes \\(\\beta_i\\) no se interpretan directamente. Para obtener su efecto sobre la razón de probabilidades, se debe calcular su exponencial \\(\\exp(\\beta_i)\\).\nMáxima verosimilitud\nMientras que en la regresión lineal múltiple (RLM) los coeficientes \\(\\beta\\) se obtenían por el método de los mínimos cuadrados, en la regresión logística (RLOG) se obtienen mediante el método de máxima verosimilitud (ML, por su nombre en inglés, Maximum Likelihood). El fundamento de esta técnica radica en utilizar la información disponible de los datos de la muestra para seleccionar el valor del parámetro que maximiza la probabilidad de observar los resultados muestrales. La ML, entonces, se calcula mediante un proceso iterativo.\nPor lo tanto, una medida adecuada para evaluar la concordancia del modelo con los datos sería el producto de todas las probabilidades (predichas por el modelo), que los \\(n\\) sujetos de la muestra realmente tengan la condición observada. Es decir, un buen modelo sería el que asigne una probabilidad de 1 (\\(p = 1\\)) a cada sujeto que realmente tenga la condición y de 0 (\\(p = 0\\)) a cada sujeto libre de ella, correspondiendo a una ML de 1. Por el contrario, un modelo deficiente tendría una verosimilitud cercana a 0. En consecuencia, la proximidad de la verosimilitud a 1 expresa cuán eficiente ha sido el ajuste realizado para modelar la realidad.\nDeviance\nLa deviance (\\(D\\)), también conocida como devianza o distancia, se define como:\n\\[\nD = -2lnV\n\\]\nDonde \\(V\\) es la verosimilitud del modelo.\nDado que, como explicamos anteriormente, \\(V &lt; 1\\), su logaritmo siempre será negativo, haciendo que la devianza sea siempre un número positivo. El grado de ajuste de un modelo será mejor cuanto más próxima a 1 es la verosimilitud y, en consecuencia, cuanto más cercana a cero sea la devianza.\nAl ajustar el modelo se calculan dos devianzas: la correspondiente al “modelo nulo” (\\(D_0\\)), que es aquel en que no se ha incorporado ninguna variable independiente, y la \\(D_f\\) del modelo. La diferencia (o cociente) entre estas dos devianzas mide la “contribución” que hacen las variables incorporadas al modelo:\n\\[\n-2lnV_0 - (-2lnV_f)\n\\]\nLa \\(D_0\\) es siempre mayor que la de cualquier modelo ampliado. Esto es razonable, ya que el modelo nulo es mucho menos complejo (no incorpora información de variables “explicativas”) y, por lo tanto, tendrá una capacidad predictiva inferior.\nLikelihood Ratio Test\nEl Likelihood Ratio Test (LRT) compara la probabilidad de los datos observados bajo dos modelos: uno que incluye los predictores y otro que no. Para ello, evalúa la diferencia en los residuos entre ambos modelos, lo que equivale a comparar sus respectivas devianzas (\\(D\\)).\n\\[\nD_0 - D = -2lnV_0 + 2lnV = -2ln(V_0-V) = -2ln\\bigg(\\frac{V_0}{V}\\bigg)  \\]\nDonde \\(\\frac{V_0}{V}\\) es el Likelihood Ratio o razón de verosimilitud.\nEl LRT permite evaluar la significancia de la incorporación de predictores al modelo, comparándolo con el modelo nulo (sin predictores).El estadístico de prueba sigue una distribución \\(\\chi^2\\) con grados de libertad equivalentes al número de predictores incluidos en el modelo.\nEn términos prácticos, la razón de verosimilitud se obtiene al comparar las devianzas de dos modelos, uno con más y otro con menos predictores. Esto permite determinar si la inclusión de ciertas variables mejora significativamente el ajuste del modelo. En la salida de un modelo, los valores de devianza aparecen bajo los títulos Null deviance (para el modelo nulo) y Residual deviance (para el modelo con predictores).\nCriterio de Información de Akaike\nEl Criterio de Información de Akaike (AIC) es una medida de la calidad relativa de un modelo estadístico, para un conjunto dado de datos. Se define como:\n\\[\nAIC= 2k – 2ln(D)\n\\]\nDonde,\n\n\\(k\\): número de parámetros del modelo.\n\\(D\\): devianza del modelo.\n\nEl AIC proporciona un método para la selección de modelos, donde valores menores de AIC indican un mejor ajuste del modelo a los datos.\n\n\n\n\n\n\nPara comparar dos o más modelos de regresión logística, se pueden utilizar las siguientes técnicas:\n\n\nAIC: El primer término de la ecuación del AIC penaliza por la inclusión de variables en el modelo (\\(2k\\)), mientras que el segundo compensa por la bondad de ajuste (\\(2ln(D)\\)). Por lo tanto, dado un conjunto de modelos para los datos, el modelo preferido es aquel con el valor mínimo de AIC.\nEn R, realizamos la comparación con la función AIC():\n\n# Genero modelo más sencillo\nmod2 &lt;- update(modelo, ~.-x1)\n\n# Comparo AIC\nAIC(modelo, mod2)\n\n       df      AIC\nmodelo  4 229.4788\nmod2    2 228.6598\n\n\n\n\nLikelihood Ratio Test: Este test permite comparar dos modelos evaluando la significancia de la diferencia de devianzas. El estadístico tiene una distribución \\(\\chi^2\\) con grados de libertad iguales a la diferencia en el número de parámetros entre los dos modelos comparados.\nLa función anova() del paquete stats permite comparar modelos por si diferencia de varianzas:\n\nanova(modelo, mod2, test = \"Chisq\")\n\nAnalysis of Deviance Table\n\nModel 1: y ~ x1 + x2\nModel 2: y ~ x2\n  Resid. Df Resid. Dev Df Deviance Pr(&gt;Chi)\n1       196     221.48                     \n2       198     224.66 -2  -3.1811   0.2038\n\n\nTambién podemos usar la función test_lrt() del paquete performance:\n\ntest_lrt(modelo, mod2)\n\n# Likelihood-Ratio-Test (LRT) for Model Comparison (ML-estimator)\n\nName   | Model | df | df_diff | Chi2 |     p\n--------------------------------------------\nmodelo |   glm |  4 |         |      |      \nmod2   |   glm |  2 |      -2 | 3.18 | 0.204\n\n\n\n\n\n\n\nBondad de ajuste\nDe forma análoga al Coeficiente de Determinación (\\(R^2\\)) utilizado en regresión lineal, se han desarrollado diversos coeficientes para estimar la proporción de variabilidad explicada por las variables independientes en modelos de regresión logística.\nEn R, podemos calcular estos coeficientes utilizando el paquete performance:\n\n\nCoeficiente de McFadden: Es una de las medidas más utilizadas y se interpreta de manera similar al \\(R^2\\) en regresión lineal:\n\nr2_mcfadden(modelo)\n\n# R2 for Generalized Linear Regression\n       R2: 0.201\n  adj. R2: 0.193\n\n\n\n\nCoeficiente de Cox y Snell: Se basa en la razón de verosimilitud y es una generalización del \\(R^2\\) en modelos lineales:\n\nr2_coxsnell(modelo)\n\nCox & Snell's R2 \n       0.2427035 \n\n\n\n\nCoeficiente de Nagelkerke: El coeficiente de Nagelkerke es una versión corregida del coeficiente de Cox y Snell, cuyo valor máximo es menor a 1 incluso para un modelo perfecto, corrigiendo así la tendencia de subestimación del coeficiente \\(R^2\\).\n\nr2_nagelkerke(modelo)\n\nNagelkerke's R2 \n      0.3237019 \n\n\n\n\nCoeficiente de Tjur: Calcula el Coeficiente de Discriminación (\\(D\\)) para modelos lineales generalizados con variable respuesta binaria. Es el proporcionado por defecto para modelos de regresión logística en performance:\n\nr2(modelo)\n\n# R2 for Logistic Regression\n  Tjur's R2: 0.251\n\n\n\n\nTest de Hosmer-Lemeshow: Ninguno de los coeficientes anteriores mide directamente la calidad del ajuste del modelo. Para evaluar esto, utilizamos el test de Hosmer-Lemeshow, que compara las probabilidades predichas con las observadas.\nEste test calcula un estadístico de distribución \\(\\chi^2\\) con \\(n-2\\) grados de libertad, utilizando varios grupos basados en los deciles de las probabilidades predichas. En R:\n\nperformance_hosmer(modelo)\n\n# Hosmer-Lemeshow Goodness-of-Fit Test\n\n  Chi-squared: 12.091\n           df:  8    \n      p-value:  0.147\n\n\nEn un test de bondad de ajuste, la hipótesis nula siempre afirma que el modelo propuesto se ajusta bien a los datos observados. Por lo tanto, un \\(p&gt;0,05\\) implica que lo observado se ajusta suficientemente bien a lo esperado bajo el modelo.\n\nCapacidad predictiva del modelo\nOtro aspecto a evaluar en un modelo de RLOG es su capacidad de discriminación, es decir, la habilidad del modelo para distinguir entre individuos en los que ocurre el evento y aquellos en los que no. Una medida común de esta discriminación es el área bajo la curva ROC (Receiver Operating Characteristic), que se construye utilizando las probabilidades predichas por el modelo.\nPara evaluar la efectividad del modelo en la clasificación de observaciones, se puede construir una tabla de clasificación que cruza el verdadero valor de la observación (1 o 0) con la predicción del modelo. Como el modelo de RLOG estimará probabilidades en el rango de 0 a 1, tendremos que elegir un punto de corte en forma arbitraria. Por ejemplo, podríamos decidir que probabilidades estimadas mayores a 0.5 sean indicativas de que el evento ha ocurrido, mientras que probabilidades menores o iguales a 0.5 indiquen que el evento no ha ocurrido.\nLa capacidad predictiva de un modelo de regresión logística se resume utilizando los conceptos de sensibilidad y especificidad. Quienes trabajen en áreas relacionadas al diagnóstico estarán más familiarizados con estos conceptos.\n\n\nSensibilidad: Probabilidad de que el modelo prediga correctamente que el evento ha ocurrido cuando realmente ha ocurrido.\n\\[\nP(\\hat{y} = 1|y = 1)\n\\]\n\nEspecificidad: Probabilidad de que el modelo prediga correctamente que el evento no ha ocurrido cuando realmente no ha ocurrido.\n\n\\[\nP(\\hat{y}=0|y=0)\n\\]\nLa curva ROC es un gráfico que representa la sensibilidad en función de 1 menos la especificidad. Si vamos modificando los valores del valor de corte y representamos la sensibilidad (en el eje Y) frente a 1 – especificidad (en el eje X) tenemos la curva ROC. Cuanto mayor sea el área bajo esta curva, mejores serán las predicciones del modelo.\n\n\n\n\n\n\n\n\nEsta curva representa, para todos los pares posibles de individuos formados por uno en el que ocurrió el evento y otro en el que no, la proporción de aquellos para los cuales el modelo predice una mayor probabilidad de haber experimentado el evento.\n\n\n\n\n\n\nPara evaluar un modelo de RLOG deberíamos observar:\n\n\nCoeficientes de determinación : Variabilidad explicada por el modelo.\n\nTest de Hosmer‐Lemeshow: Bondad de ajuste, diferencia entre los valores predichos por el modelo y los valores observados en la muestra.\n\nAIC, LRT o ANOVA: Comparación de modelos.\n\nCurva ROC: Capacidad predictiva (especialmente cuando el propósito es predictivo).\n\nIntervalos de confianza: Exactitud del coeficiente."
  },
  {
    "objectID": "unidad_4/02_reg_logistica.html#construcción-del-modelo-en-r",
    "href": "unidad_4/02_reg_logistica.html#construcción-del-modelo-en-r",
    "title": "Regresión logística",
    "section": "Construcción del modelo en R",
    "text": "Construcción del modelo en R\nEl ajuste de un modelo de regresión logística en R se realiza utilizando la función glm(), del paquete stats:\n\nglm(formula, family = binomial(link = \"logit\"), data)\n\n\n\nformula: Especifica la relación entre la variable dependiente y las variables independientes en el modelo. Sigue la estructura estándar:\n\n\\[\nvariable\\_dependiente \\sim variable\\_indep_1 + variable\\_indep_2 +\\dots+ variable\\_indep_n\n\\]\n\n\nfamily: Se refiere a la familia de distribuciones y la función de enlace utilizada para ajustar el modelo. Las opciones comunes incluyen:\n\ngaussian(): Utilizada para variables dependientes continuas con distribución normal. El enlace predeterminado es identity, lo cual es análogo a ajustar un modelo de RLM. Puede emplear también enlaces log, e inverse.\nbinomial(): Utilizada para variables dependientes binarias (0, 1). El enlace predeterminado es logit, que es el más común en la regresión logística. También admite enlaces como probit, cauchit, log, y cloglog.\npoisson(): Usada para variables dependientes numéricas discretas. El enlace predeterminado es el logaritmo (log), y también admite identity y sqrt.\nOtras familias como Gamma(), inverse.gaussian(), quasi(), quasipoisson(), y quasibinomial() para diferentes distribuciones de errores que no abordaremos en el curso.\n\nSi la función de enlace no se especifica, se utiliza el enlace canónico (predeterminado) para cada familia. Por ejemplo, si omitimos el argumento link = \"logit\", de todas maneras quedaría definido ese mismo enlace para la familia binomial.\n\ndata: Nombre del dataframe que contiene las variables utilizadas en el modelo.\n\nGestión de variables dicotómicas\nDecíamos que el modelo con enlace logit es un modelo de regresión típico:\n\\[\nY = f(X + E)\n\\]\ndonde la variable respuesta (variable aleatoria \\(Y\\)) es dicotómica o binaria (toma dos valores: 0 y 1), habitualmente sobre si nuestra unidad de análisis tiene una característica (1) o no la tiene (0).\nNuestras variables dicotómicas pueden tener originalmente formatos variados y sus categorías también pueden definirse con etiquetas diferentes. Por ejemplo, podemos tener variables dicotómicas con formato lógico (+/-, TRUE/FALSE), con formato caracter (Si/No, Vivo/Muerto, etc.) o con formato numérico codificado (0-1, 1-2 o cualquier combinación personalizada de códigos).\nEn R las variables categóricas que utilizamos como dependientes en estos modelos corresponden convenientemente al tipo de datos factor. Recordemos que un factor es, interna y técnicamente, una variable numérica compuesta de enteros sucesivos a partir de 1. Cada entero es un nivel o categoría de la variable y está acompañado de una etiqueta que nos facilita recordar a qué categoría corresponde.\nLos modelos binomiales asumen las variables dicotómicas codificándolas como 0 y 1, lo cuál podría ser problemático si, como numéricos, los factores comienzan con 1. Como la estructura de los factores es conocida por las funciones que estiman modelos logit, estas funciones convierten internamente la variable categórica/factor en una variable codificada como 0/1, sin que el usuario tenga que hacer nada.\nPor lo tanto, no es necesario recodificar la variable o convertirla en dummy, esto se procesa de manera transparente para el usuario. Lo único que debemos asegurarnos es que la variable sea factor y que el nivel de referencia sea la ausencia de la característica (por ejemplo, enfermedad = No).\nDebemos usar la función levels() de R base para consultar los niveles o categorías de un factor e identificar el primer nivel del factor, que será el nivel de referencia en el modelo, es decir, \\(Y=0\\).\nPodemos modificar estos niveles de referencia mediante la función relevel() de R base o con fct_rev() de tidyverse, cuando estos se encuentren invertidos.\n\n# Factor con dos categorías\nfactor &lt;- factor(c(\"1\", \"0\", \"1\", \"1\", \"0\"))\n\n# Cambio nivel de referencia en R base\nrelevel(factor, ref = \"1\")\n\n[1] 1 0 1 1 0\nLevels: 1 0\n\n# Cambio nivel de referencia en tidyverse\nfct_rev(factor)\n\n[1] 1 0 1 1 0\nLevels: 1 0\n\n\nEste procesamiento es extensivo a las variables categóricas (dicotómica o politómicas) que se incluyen en los modelos de regresión como explicativas. En tidyverse, podemos cambiar los niveles de referencia de un factor con más de dos categorías con la función fct_relevel().\n\n# Factor con más de dos categorías\nfactor &lt;- factor(c(\"CONF\", \"DESC\", \"SOSP\", \"CONF\", \"DESC\", \"PROB\"))\n\n# Niveles por defecto\nlevels(factor)\n\n[1] \"CONF\" \"DESC\" \"PROB\" \"SOSP\"\n\n# Pongo DESC como nivel de referencia\nfct_relevel(factor, \"DESC\", after = 0)\n\n[1] CONF DESC SOSP CONF DESC PROB\nLevels: DESC CONF PROB SOSP\n\n\nAjuste del modelo\nLa razón de verosimilitud en un modelo de regresión puede calcularse restando la devianza del modelo con predictores de la del modelo nulo:\n\nmodelo$null.deviance - modelo$deviance\n\n[1] 55.60009\n\n\nEn este cálculo, estamos comparando la null deviance (modelo sin predictores) con la residual deviance (modelo con predictores). Alternativamente, la función anova() proporciona una tabla detallada que desglosa la contribución de cada variable al modelo junto con sus respectivas devianzas.\n\nanova(modelo)\n\nAnalysis of Deviance Table\n\nModel: binomial, link: logit\n\nResponse: y\n\nTerms added sequentially (first to last)\n\n     Df Deviance Resid. Df Resid. Dev  Pr(&gt;Chi)    \nNULL                   199     277.08              \nx1    2    4.080       197     273.00    0.1301    \nx2    1   51.521       196     221.48 7.085e-13 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nEn un modelo con múltiples predictores, no todas las variables necesariamente aportan información relevante. Identificar y excluir aquellas con menor impacto puede simplificar el modelo sin comprometer su capacidad predictiva. Es fundamental definir qué se entiende por “variable relevante”, ya que la importancia de una variable no debe evaluarse únicamente desde el punto de vista estadístico. El marco conceptual es clave en la selección de predictores, especialmente cuando el objetivo del modelo es analítico. La decisión debe basarse tanto en el conocimiento del área de estudio como en la evidencia estadística.\nUn error común es suponer que los métodos estadísticos pueden sustituir el conocimiento sustantivo del problema. El abuso de modelos de regresión sin una adecuada justificación teórica puede llevar a asociaciones espurias o a resultados estadísticamente significativos pero clínicamente, biológicamente o socialmente irrelevantes.\nExisten diversos métodos estadísticos para la selección de variables en regresión logística. Estos buscan lograr un modelo parsimonioso que se ajuste bien a los datos sin incluir predictores innecesarios. Si bien estos procedimientos son similares a los utilizados en regresión lineal múltiple, algunos criterios de selección específicos varían."
  },
  {
    "objectID": "unidad_4/02_reg_logistica.html#ejemplo-práctico-en-r",
    "href": "unidad_4/02_reg_logistica.html#ejemplo-práctico-en-r",
    "title": "Regresión logística",
    "section": "Ejemplo práctico en R",
    "text": "Ejemplo práctico en R\nUtilizaremos una tabla de datos denominada “caminar.txt”, que contiene observaciones de un estudio de casos y controles donde se evaluó a 800 niños, con el objetivo de identificar qué factores se asociaban al desarrollo de la marcha.\nComencemos cargando en R los paquetes que utilizaremos:\n\n# Tablas de coeficientes y regresiones simples\nlibrary(gtsummary) \n\n# Chequeo de supuestos\nlibrary(easystats) \n\n# Manejo de datos\nlibrary(skimr)\nlibrary(flextable)\nlibrary(tidyverse)\n\nCargamos los datos y exploramos su estructura. Vemos dos tipos de variable: Enteros &lt;int&gt; para las numéricas y caracter &lt;chr&gt; para las categóricas.\n\n# Carga datos\ndatos &lt;- read.csv2(\"datos/caminar.txt\")\n\n# Explora datos\nglimpse(datos)\n\nRows: 800\nColumns: 6\n$ id         &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, …\n$ camina     &lt;int&gt; 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1,…\n$ edad_dias  &lt;int&gt; 265, 318, 554, 389, 529, 574, 578, 582, 398, 289, 336, 476,…\n$ edad_meses &lt;int&gt; 8, 10, 18, 12, 17, 19, 19, 19, 13, 9, 11, 15, 16, 9, 9, 18,…\n$ pareja_est &lt;chr&gt; \"No\", \"Si\", \"No\", \"Si\", \"Si\", \"No\", \"Si\", \"Si\", \"No\", \"No\",…\n$ lacto_180  &lt;chr&gt; \"Si\", \"Si\", \"No\", \"Si\", \"Si\", \"No\", \"Si\", \"Si\", \"Si\", \"Si\",…\n\n\nLas variables a considerar son:\n\ncamina: si al momento de la entrevista caminaban (1, 0)\nedad_meses: a qué edad habían comenzado a caminar (medida en meses)\nedad_dias: a qué edad habían comenzado a caminar (medida en días)\npareja_est: si la madre tenía pareja estable (Si, No)\nlacto_180: si había tomado pecho al menos 6 meses (Si, No)\n\nCoercionamos los tipos character a factor.\n\ndatos &lt;- datos |&gt;    \n  mutate(camina = factor(camina),          \n         pareja_est = factor(pareja_est),           \n         lacto_180 = factor(lacto_180))\n\nComo las categorías de las tres variables convertidas son “0” y “1” o “Si” y “No”, y el orden automático del lenguaje R es alfabético, el nivel de referencia será “0” y “No”. Confirmémoslo mediante la visualización de los niveles:\n\nlevels(datos$camina)  \n\n[1] \"0\" \"1\"\n\nlevels(datos$pareja_est)  \n\n[1] \"No\" \"Si\"\n\nlevels(datos$lacto_180)\n\n[1] \"No\" \"Si\"\n\n\nEfectuamos una breve descripción univariada usando el paquete skimr:\n\ndatos |&gt; \n  # Omitimos identificador de paciente\n  select(-id) |&gt; \n  skim() \n\n\nData summary\n\n\nName\nselect(datos, -id)\n\n\nNumber of rows\n800\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n3\n\n\nnumeric\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\ncamina\n0\n1\nFALSE\n2\n1: 419, 0: 381\n\n\npareja_est\n0\n1\nFALSE\n2\nNo: 467, Si: 333\n\n\nlacto_180\n0\n1\nFALSE\n2\nSi: 454, No: 346\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\nedad_dias\n0\n1\n424.20\n104.30\n240\n334\n424\n516.25\n600\n▇▆▇▆▇\n\n\nedad_meses\n0\n1\n13.67\n3.47\n8\n11\n14\n17.00\n20\n▇▅▇▅▆\n\n\n\n\n\nSelección de modelos\nA continuación iniciamos un proceso iterativo manual tipo forward basado en la selección de modelos propuesto en el capítulo 8 del libro de Silva Ayçaguer (1995).\nPaso 1: Se ajustan tantos modelos de regresión logística simple como posibles variables explicativas tengamos. Dos de las variables (edad_dias y edad_meses) son la misma expresada en unidades diferentes, por lo que intentaremos quedarnos con la que explique más y a su vez sea estadísticamente significativa..\n\n# edad en días \nmod1 &lt;- glm(camina ~ edad_dias, \n            data = datos,              \n            family = binomial)  \n\n# edad en meses \nmod2 &lt;- glm(camina ~ edad_meses, \n            data = datos,              \n            family = binomial)  \n\n# pareja estable \nmod3 &lt;- glm(camina ~ pareja_est, \n            data = datos,              \n            family = binomial)  \n\n# lactancia \nmod4 &lt;- glm(camina ~ lacto_180, \n            data = datos,              \n            family = binomial)\n\nSe calcula el Likelihood Ratio test de cada uno de ellos.\n\nmod1$null.deviance - mod1$deviance  \n\n[1] 258.118\n\nmod2$null.deviance - mod2$deviance  \n\n[1] 258.7028\n\nmod3$null.deviance - mod3$deviance  \n\n[1] 246.6692\n\nmod4$null.deviance - mod4$deviance\n\n[1] 352.0365\n\n\nSe identifica el mayor de estas \\(G^2\\) (mod4) y se evalúa si es significativo\n\ntest_lrt(mod4)\n\n# Likelihood-Ratio-Test (LRT) for Model Comparison (ML-estimator)\n\nName       | Model | df | df_diff |   Chi2 |      p\n---------------------------------------------------\nNull model |   glm |  1 |         |        |       \nFull model |   glm |  2 |       1 | 352.04 | &lt; .001\n\n\nPaso 2: Como la variable lacto_180 es significativa, se incorpora como primer variable al modelo y a continuación se generan modelos de dos variables independientes combinando esta variable con cada una de las otras.\n\n# mod 4 + edad en días \nmod5_1 &lt;- glm(camina ~ lacto_180 + edad_dias, \n              data = datos,              \n              family = binomial)  \n\n# mod4 + edad en meses \nmod5_2 &lt;- glm(camina ~ lacto_180 + edad_meses, \n              data = datos,              \n              family = binomial)  \n\n# mod4 + pareja estable \nmod5_3 &lt;- glm(camina ~ lacto_180 + pareja_est, \n             data = datos,              \n             family = binomial)\n\nSe identifica la pareja para la cual la deviance (\\(-2lnV_f\\)) es menor.\n\nmod5_1$deviance  \n\n[1] 592.4274\n\nmod5_2$deviance  \n\n[1] 593.5655\n\nmod5_3$deviance\n\n[1] 614.9587\n\n\nEn este caso, el mod5_1 con las variables lacto_180 y edad_dias tiene una deviance de 592.43, menor a los otros dos modelos.\nSe evalúa si el agregado de la variable edad_dias es significativa respecto al modelo con la variable lacto_180 sola.\n\ntest_lrt(mod4, mod5_1)\n\n# Likelihood-Ratio-Test (LRT) for Model Comparison (ML-estimator)\n\nName   | Model | df | df_diff |   Chi2 |      p\n-----------------------------------------------\nmod4   |   glm |  2 |         |        |       \nmod5_1 |   glm |  3 |       1 | 162.77 | &lt; .001\n\n\nPaso 3: Como es significativa, se incorpora al modelo y a continuación se agrega la tercer y última variable posible en el modelo (dado que la variable edad_meses es la misma que edad_dias que ya está incorporada).\n\nmod6 &lt;- glm(camina ~ lacto_180 + edad_dias + pareja_est, \n            data = datos,                  \n            family = binomial)\n\nSe evalúa la significación del agregado. Si es significativo el modelo quedaría con las tres variables, de lo contrario quedan las dos iniciales.\n\ntest_lrt(mod5_1, mod6)\n\n# Likelihood-Ratio-Test (LRT) for Model Comparison (ML-estimator)\n\nName   | Model | df | df_diff |  Chi2 |      p\n----------------------------------------------\nmod5_1 |   glm |  3 |         |       |       \nmod6   |   glm |  4 |       1 | 94.71 | &lt; .001\n\n\nEl proceso iterativo continúa de haber más variables para agregar.\nTambién se pueden utilizar métodos automáticos, al igual que vimos en la regresión lineal múltiple, partiendo por ejemplo de un modelo saturado y aplicando la función step(). Recuerden que estos modelos automáticos los utilizamos para comparar los procesos manuales y que se basan en únicamente en iteraciones con procesos matemático-estadísticos sin tener en cuenta la “relevancia” conceptual de las variables y las relaciones entre ellas.\n\nmodelo_saturado &lt;- glm(camina ~ .-id, \n                       data = datos,                         \n                       family = binomial)  \n\nmodelo_step &lt;- step(modelo_saturado, direction = \"both\")\n\nStart:  AIC=507.72\ncamina ~ (id + edad_dias + edad_meses + pareja_est + lacto_180) - \n    id\n\n             Df Deviance    AIC\n- edad_meses  1   497.72 505.72\n- edad_dias   1   498.63 506.63\n&lt;none&gt;            497.72 507.72\n- pareja_est  1   592.43 600.43\n- lacto_180   1   691.11 699.11\n\nStep:  AIC=505.72\ncamina ~ edad_dias + pareja_est + lacto_180\n\n             Df Deviance    AIC\n&lt;none&gt;            497.72 505.72\n+ edad_meses  1   497.72 507.72\n- pareja_est  1   592.43 598.43\n- edad_dias   1   614.96 620.96\n- lacto_180   1   691.58 697.58\n\n\nEn este caso la selección automática utiliza el AIC que funciona de manera similar a la regresión lineal y el seleccionado termina siendo el que tiene el AIC más bajo.\n\nsummary(modelo_step)\n\n\nCall:\nglm(formula = camina ~ edad_dias + pareja_est + lacto_180, family = binomial, \n    data = datos)\n\nCoefficients:\n              Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  -7.596812   0.619037 -12.272   &lt;2e-16 ***\nedad_dias     0.012168   0.001272   9.566   &lt;2e-16 ***\npareja_estSi  2.242393   0.249994   8.970   &lt;2e-16 ***\nlacto_180Si   3.007014   0.249097  12.072   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1107.23  on 799  degrees of freedom\nResidual deviance:  497.72  on 796  degrees of freedom\nAIC: 505.72\n\nNumber of Fisher Scoring iterations: 6\n\n\nEn este ejemplo el modelo automático es coincidente con el forward manual que realizamos.\nAplicando AIC() a la serie de modelos que fuimos realizando de forma manual y automática podemos comparar:\n\nAIC(mod4)  \n\n[1] 759.1933\n\n# modelo solo con lacto_180 \nAIC(mod5_1) \n\n[1] 598.4274\n\n# modelo con lacto_180 y edad_dias \nAIC(mod6) \n\n[1] 505.7191\n\n# modelo con lacto_180, edad_dias y pareja_est \nAIC(modelo_step) # modelo automático idem mod6 \n\n[1] 505.7191\n\n\nEl modelo final tiene el AIC más bajo (505.7).\nAl igual que en RLM el paquete performance muestra en una tabla la comparación de índices AIC entre otros, para los modelos definidos.\n\ncompare_performance(mod4, mod5_1, mod6, \n                    metrics = \"AIC\")\n\n# Comparison of Model Performance Indices\n\nName   | Model | AIC (weights)\n------------------------------\nmod4   |   glm | 759.2 (&lt;.001)\nmod5_1 |   glm | 598.4 (&lt;.001)\nmod6   |   glm | 505.7 (&gt;.999)\n\n\nSi añadimos el argumento rank = TRUE nos muestra los modelos ordenados por AIC:\n\ncompare_performance(mod4, mod5_1, mod6, \n                    metrics = \"AIC\", \n                    rank = TRUE)\n\n# Comparison of Model Performance Indices\n\nName   | Model | AIC weights | Performance-Score\n------------------------------------------------\nmod6   |   glm |        1.00 |           100.00%\nmod5_1 |   glm |    7.39e-21 |         7.39e-19%\nmod4   |   glm |    9.09e-56 |             0.00%\n\n\nCalidad de ajuste\nExisten diversas formas de medir la calidad de ajuste de un modelo de regresión logística. De manera global, esta calidad puede ser evaluada a través de coeficientes de determinación \\(R^2\\), probabilidades estimadas o test estadísticos.\nLos coeficientes de determinación calculan el grado de “explicación de la variabilidad de la variable de respuesta” conseguido con el modelo a partir de las variables independientes. Como vimos anteriormente, pueden estimarse con funciones del paquete performance:\n\nr2_mcfadden(mod6)  \n\n# R2 for Generalized Linear Regression\n       R2: 0.550\n  adj. R2: 0.549\n\nr2_coxsnell(mod6)  \n\nCox & Snell's R2 \n       0.5332159 \n\nr2_nagelkerke(mod6)  \n\nNagelkerke's R2 \n      0.7114904 \n\nr2_tjur(mod6)\n\nTjur's R2 \n0.6177795 \n\n\nEstos coeficientes alcanzan el valor 1 cuando el vaticinio es perfecto (esto significa que el modelo atribuye probabilidad 1 a aquellos sujetos de la muestra que efectivamente tuvieron el evento y valores iguales a 0 a quienes no lo tuvieron) y se aproximan a 0 en la medida que las probabilidades atribuidas por el modelo disten más de 1 y 0, respectivamente.\nRespecto a test estadísticos, la prueba de razón de verosimilitud (LRT), recientemente utilizada en el proceso de selección de modelos, es una de las que podemos utilizar basada en la devianza.\n\n# R base\nanova(mod6, test = \"Chisq\")\n\nAnalysis of Deviance Table\n\nModel: binomial, link: logit\n\nResponse: camina\n\nTerms added sequentially (first to last)\n\n           Df Deviance Resid. Df Resid. Dev  Pr(&gt;Chi)    \nNULL                         799    1107.23              \nlacto_180   1   352.04       798     755.19 &lt; 2.2e-16 ***\nedad_dias   1   162.77       797     592.43 &lt; 2.2e-16 ***\npareja_est  1    94.71       796     497.72 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# performance\ntest_lrt(mod6)  \n\n# Likelihood-Ratio-Test (LRT) for Model Comparison (ML-estimator)\n\nName       | Model | df | df_diff |   Chi2 |      p\n---------------------------------------------------\nNull model |   glm |  1 |         |        |       \nFull model |   glm |  4 |       3 | 609.51 | &lt; .001\n\n\nObservamos que las dos funciones son complementarias. Mientras test_lrt() nos muestra la razón de verosimilitud entre el modelo completo y el nulo en forma global, la función anova() nos detalla en la tabla de análisis de varianzas la deviance entre cada paso del modelo, es decir, a partir de incorporar una a una las variables con su respectiva significación.\nEntre las pruebas de bondad de ajuste basadas en la agrupación de las probabilidades estimadas bajo el modelo de regresión, usaremos el test de Hosmer y Lemeshow. Si bien existen variados paquetes que proveen esta, nosotros utilizaremos la función performance_hosmer() del paquete performance.\n\nperformance_hosmer(mod6)\n\n# Hosmer-Lemeshow Goodness-of-Fit Test\n\n  Chi-squared: 10.660\n           df:  8    \n      p-value:  0.222\n\n\nEl test de bondad de ajuste de Hosmer y Lemeshow nos da un p-valor para el mod6 de 0.222, por lo que concluimos que el modelo ajusta adecuadamente los datos (no hay diferencia significativa entre los datos observados en relación a los esperados).\nCurva ROC\nEn R podemos graficar la curva ROC, que es una herramienta más específica de modelos predictivos, para comparar diferentes modelos. La función performance_roc() del paquete performance se aplica de la siguiente manera:\n\nperformance_roc(mod6) |&gt;    \n  \n# Genera gráfico   \n  plot() + \n  \n  # Cambia color de fondo\n  theme_minimal()\n\n\n\n\n\n\n\nCuanto mayor es el área bajo la curva, más eficiente es el modelo. Si el modelo tiene capacidad predictiva nula la curva coincide con la diagonal principal del cuadrado, y el área bajo la curva toma su valor mínimo de 0,5. Por el contrario, un modelo perfecto tiene una curva ROC con área 1.\nAnalíticamente podemos visualizar el valor del área bajo la curva (AUC de Area under the curve) e incorporar los intervalos de confianza al 95% con la función performance_accuracy():\n\nperformance_accuracy(mod6)\n\n# Accuracy of Model Predictions\n\nAccuracy (95% CI): 94.00% [93.11%, 95.09%]\nMethod: Area under Curve\n\n\nLos valores de AUC se pueden interpretar siguiendo el criterio:\n\n\n\n\nValor\nCapacidad predictiva\n\n\n\n0.5-0.6\nMala\n\n\n0.6-0.75\nRegular\n\n\n0.75-0.9\nBuena\n\n\n0.9-0.97\nMuy buena\n\n\n0.97-1\nExcelente\n\n\n\n\n\nNuestro modelo logra un AUC cercano a 0.94, que está dentro de la categoría muy buena.\nCálculo del OR\nLos valores de odds-ratio se obtienen como el resultado de calcular el valor del número \\(e\\) elevado a los coeficientes estimados del modelo (exponencial o inversa del logaritmo natural).\nEl código de R para aplicar esta función al mod6 (con redondeo de 2 decimales) es:\n\ncoef(mod6) |&gt;    \n  exp() |&gt;     \n  round(2)\n\n (Intercept)  lacto_180Si    edad_dias pareja_estSi \n        0.00        20.23         1.01         9.42 \n\n\nCuando estamos frente a una regresión logística múltiple, como en este caso, nos permite obtener las odds-ratio ajustadas por otras variables.\nLa interpretación sería:\n\nLa probabilidad de caminar de los bebés que fueron amamantados por al menos 6 meses fue 20 veces mayor (OR = 20.23) que la de los bebés que no fueron amamantados ese tiempo, ajustada por las variables edad (en días) y pareja estable.\nLa probabilidad de caminar de los bebés cuyas madres tenían pareja estable fue aproximadamente 9 veces mayor (OR = 9.42) que la de los bebés cuyas madres no tenían pareja estable, ajustada por las variables edad (en días) y lactancia a los 6 meses.\nLa probabilidad de caminar de los bebés se incrementa un 1% por cada día más de vida (OR = 1.01), ajustada por las variables lactancia a los 6 meses y pareja estable.\n\nPodemos también, incorporar intervalos de confianza de los OR agregando la función confint().\n\nconfint(mod6) |&gt;    \n  exp() |&gt;     \n  round(2)\n\n             2.5 % 97.5 %\n(Intercept)   0.00   0.00\nlacto_180Si  12.60  33.52\nedad_dias     1.01   1.01\npareja_estSi  5.84  15.59\n\n\nLa función tbl_regression() del paquete gtsummary nos permite generar tablas de los coeficientes exponenciados, su intervalo de confianza y p-valor para cada variable del modelo:\n\ntbl_regression(mod6, exponentiate = T)\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\nOR\n95% CI\np-value\n\n\n\nlacto_180\n\n\n\n\n\n    No\n—\n—\n\n\n\n    Si\n20.2\n12.6, 33.5\n&lt;0.001\n\n\nedad_dias\n1.01\n1.01, 1.01\n&lt;0.001\n\n\npareja_est\n\n\n\n\n\n    No\n—\n—\n\n\n\n    Si\n9.42\n5.84, 15.6\n&lt;0.001\n\n\n\nAbbreviations: CI = Confidence Interval, OR = Odds Ratio\n\n\n\n\n\nSi quisiéramos representar gráficamente el modelo de regresión, podemos usar la función model_parameters() de easystats:\n\nmodel_parameters(mod6, exponentiate = T) |&gt; \n  plot()\n\n\n\n\n\n\n\nResiduales\nComo en toda regresión, podemos examinar los residuos para asegurarnos que el modelo se ajusta bien a los datos observados.\nEl principal propósito de examinar los residuales es:\n\nDetectar los puntos en los que el modelo se ajusta mal.\nDetectar los puntos que ejercen una influencia excesiva sobre el modelo.\n\nPara buscar los casos conflictivos podemos fijarnos en lo siguiente:\n\nMirar los residuales estandarizados y asegurarnos que no hay valores atípicos.\nObservar el gráfico Residuals vs Leverage incluído en la salida de plot(modelo) donde se presentan distancias de Cook.\n\nLos residuales se almacenan dentro del objeto de regresión del modelo bajo el nombre residuals. Se puede extraer los residuales estandarizados con la función rstandar() y luego clasificar por su valor absoluto y cuantificar. Además, la función flextable() del paquete flextable (Gohel y Skintzos 2024) nos muestra la salida de manera visualmente atractiva:\n\ndatos$rstandar &lt;- rstandard(mod6)  \n\ndatos |&gt; \n  mutate(clasif = case_when(\n    between(abs(rstandar),2, 2.5) ~ \"&gt; 2\",   \n    between(abs(rstandar),2.5, 3)  ~ \"&gt; 2.5\",   \n    between(abs(rstandar),3, +Inf) ~ \"&gt; 3\")) |&gt;    \n  count(clasif) |&gt;    \n  mutate(prop = n/sum(n)) |&gt; \n  \n  flextable()\n\n\n\n\n\nclasif\nn\nprop\n\n\n\n&gt; 2\n16\n0.02000\n\n\n&gt; 2.5\n6\n0.00750\n\n\n&gt; 3\n1\n0.00125\n\n\n\n777\n0.97125\n\n\n\n\n\n\nObservamos que las proporciones de valores residuales estandarizados por encima de 2.5 y 3 son muy pequeños. Solo hay un caso mayor de 3 que podríamos revisar.\nPodemos obtener el gráfico de las distancias de Cook usando el comando check_outliers() de performance:\n\ncheck_outliers(mod6) |&gt; \n  plot()\n\n\n\n\n\n\n\nDe forma similar a la evaluación de residuales de la regresión lineal, estaremos atentos a valores periféricos en la esquina superior e inferior. Esos lugares, fuera de las líneas punteadas, son los lugares donde los puntos influyentes aparecen.\nEn este ejemplo no se observa ninguno, por lo que en definitiva, no pareciese existir observaciones influyentes que tengan efecto sobre el modelo.\nColinealidad\nSe dice que existe colinealidad o multicolinealidad cuando dos o más de las covariables del modelo mantienen una relación lineal.\nNormalmente, se tolera una multicolinealidad moderada, es decir, una mínima correlación entre covariables. Si esta correlación fuese significativa, su efecto sería el incremento exagerado de los errores estándar y, en ocasiones, del valor estimado para los coeficientes de regresión, lo que hace las estimaciones poco creíbles. Podemos verificar este supuesto con el estadístico VIF (Variance Inflation Factor) utilizando el paquete performance:\n\n# Test de colinealidad\ncheck_collinearity(mod6)\n\n# Check for Multicollinearity\n\nLow Correlation\n\n       Term  VIF   VIF 95% CI Increased SE Tolerance Tolerance 95% CI\n  lacto_180 1.12 [1.06, 1.25]         1.06      0.89     [0.80, 0.94]\n  edad_dias 1.08 [1.03, 1.22]         1.04      0.92     [0.82, 0.97]\n pareja_est 1.05 [1.01, 1.23]         1.02      0.95     [0.81, 0.99]\n\n# Gráfico de colinealidad\ncheck_collinearity(mod6) |&gt; \n  plot()\n\n\n\n\n\n\n\nLos resultados para cada una de las variables independientes es cercano a 1, por lo que descartamos problemas de multicolinealidad.\nInteracción\nEn este punto aplica el mismo concepto y explicación que la expresada en el material desarrollado en la Unidad 3.\nAnte la pregunta de la existencia de una variable modificadora de efecto dentro de la regresión múltiple, podemos realizar un análisis comparativo separando por estratos o subgrupos. El objetivo es identificar si la relación de la variable respuesta y una variable independiente cambia de acuerdo al nivel de otra variable independiente.\nTomemos, por ejemplo, las variables lacto_180 y pareja_est. Ambas son estadísticamente significativas dentro del modelo mod6. Construyamos dos modelos que contengan lacto_180 y edad_dias, aplicados en dos subgrupos de la tabla datos: uno con niños de madres con pareja estable y otro con madres sin pareja estable.\n\n# Crear subgrupos\ncon_pareja &lt;- datos %&gt;% filter(pareja_est == \"Si\")\n\nsin_pareja &lt;- datos %&gt;% filter(pareja_est == \"No\")\n\n# Ajustar modelos para cada subgrupo\nmod_con_pareja &lt;- glm(camina ~ lacto_180 + edad_dias, \n                      family = binomial(link = \"logit\"), \n                      data = con_pareja)\n\nmod_sin_pareja &lt;- glm(camina ~ lacto_180 + edad_dias, \n                      family = binomial(link = \"logit\"), \n                      data = sin_pareja)\n\nAhora generemos las tablas de coeficientes para cada modelo con la función tbl_regression() de gtsummary\n\n# Comparar coeficientes entre los dos modelos\ntab_con_pareja &lt;- tbl_regression(mod_con_pareja, exponentiate = T)\n\ntab_sin_pareja &lt;- tbl_regression(mod_sin_pareja, exponentiate = T)\n\nPodemos colocar las tablas una al lado de la otra con la función tbl_merge() del mismo paquete:\n\ntbl_merge(\n  tbls = list(tab_con_pareja, tab_sin_pareja),\n          tab_spanner = c(\"Pareja estable\", \"Sin pareja estable\")\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\n\nPareja estable\n\n\nSin pareja estable\n\n\n\nOR\n95% CI\np-value\nOR\n95% CI\np-value\n\n\n\n\nlacto_180\n\n\n\n\n\n\n\n\n    No\n—\n—\n\n—\n—\n\n\n\n    Si\n55.1\n21.8, 163\n&lt;0.001\n13.2\n7.65, 23.5\n&lt;0.001\n\n\nedad_dias\n1.01\n1.01, 1.02\n&lt;0.001\n1.01\n1.01, 1.01\n&lt;0.001\n\n\n\nAbbreviations: CI = Confidence Interval, OR = Odds Ratio\n\n\n\n\n\nAl analizar los coeficientes de los modelos ajustados para cada subgrupo, podemos identificar si la relación entre lacto_180 y camina, y entre edad_dias y camina, varía significativamente según el estado de la variable pareja_est. Observamos que los coeficientes de edad_dias se mantienen constantes en ambos estratos, mientras que los OR de lacto_180 son diferentes, esto indica que pareja_est actúa como modificadora de efecto.\nEn R, la forma de incluir y probar un término de interacción dentro de un modelo es mediante el uso del símbolo de multiplicación (*) en lugar del +. Esto provoca que se tengan en cuenta cada una de las variables individualmente y la interacción entre ellas.\n\nmod_interaccion &lt;- glm(camina ~  edad_dias + lacto_180 * pareja_est,\n            data = datos,\n            family = binomial)\n\nComparo los modelos con y sin interacción usando performance:\n\ncompare_performance(mod6, mod_interaccion, metrics = \"common\")\n\n# Comparison of Model Performance Indices\n\nName            | Model | AIC (weights) | BIC (weights) | Tjur's R2 |  RMSE\n---------------------------------------------------------------------------\nmod6            |   glm | 505.7 (0.104) | 524.5 (0.548) |     0.618 | 0.309\nmod_interaccion |   glm | 501.4 (0.896) | 524.8 (0.452) |     0.622 | 0.307\n\n\nObservamos que el modelo con interacción tiene mejor AIC y \\(R^2\\) que el mod6. Visualicemos ahora sus coeficientes:\n\ntbl_regression(mod_interaccion, exponentiate = T)\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\nOR\n95% CI\np-value\n\n\n\nedad_dias\n1.01\n1.01, 1.02\n&lt;0.001\n\n\nlacto_180\n\n\n\n\n\n    No\n—\n—\n\n\n\n    Si\n13.4\n7.79, 24.1\n&lt;0.001\n\n\npareja_est\n\n\n\n\n\n    No\n—\n—\n\n\n\n    Si\n4.89\n2.43, 9.97\n&lt;0.001\n\n\nlacto_180 * pareja_est\n\n\n\n\n\n    Si * Si\n3.67\n1.32, 10.8\n0.015\n\n\n\nAbbreviations: CI = Confidence Interval, OR = Odds Ratio\n\n\n\n\n\nEl término de interacción es significativo e implica el exceso de la variabilidad de los datos que no puede ser explicada por la suma de las variables consideradas. En este caso la interacción es positiva (existe “sinergia” entre las variables) con un OR de 3.67.\nTamaño de la muestra y número de variables independientes\nEn el ámbito epidemiológico, es fundamental asegurarse de que el tamaño de la muestra sea adecuado para obtener estimaciones confiables en un modelo de regresión logística. Una regla general para determinar el tamaño de muestra es la fórmula propuesta por Freeman (1987), que establece que el tamaño de muestra debería ser 10 veces el número de variables independientes a estimar, más uno.\n\\[\nn = 10*(k+1)\n\\]\ndonde:\n\n\\(n\\) es el tamaño de la muestra.\n\\(k\\) es el número de variables independientes.\n\nAdemás, se recomienda que para que la variable dicotómica de respuesta sea fiable, debe haber al menos 10 casos para cada uno de los valores posibles (0 y 1). Esto asegura que las estimaciones sean confiables y que el modelo no esté sesgado.\nPara nuestro ejemplo:\n\nNúmero de observaciones: 800\nNúmero de variables independientes en mod_interaccion: 4\n\nAplicando la fórmula de Freeman:\n\nn_min = 10 * (4 + 1)\n\nn_min\n\n[1] 50\n\n\nDado que el modelo incluye 4 variables independientes, la fórmula sugiere que se necesita un mínimo de 50 observaciones. En nuestro caso, el tamaño de la muestra es 800, lo cual supera ampliamente el requisito mínimo de 50 observaciones, confirmando que tenemos un tamaño de muestra adecuado para el modelo propuesto.\nVerifiquemos ahora si existen al menos 10 casos para cada valor de la variable respuesta:\n\ndatos |&gt; \n  count(camina)\n\n  camina   n\n1      0 381\n2      1 419\n\n\nLa base de datos tiene 419 casos donde la variable de respuesta es 1 y 381 casos donde es 0. Esto cumple con la recomendación de tener al menos 10 casos para cada valor de la variable dicotómica de respuesta, garantizando que las estimaciones sean confiables.\n\n«EPIDAT 4.2 - Consellería de Sanidade - Servizo Galego de Saúde» (s. f.)\nEscuela Nacional de Sanidad (ENS). Instituto de Salud Carlos III. Ministerio de Ciencias e Innovación. Madrid (2009)\nField, Miles, y Field (2014)\nHernández-Ávila (2011)\nOrtega Calvo y Cayuela Domínguez (2002)\nRothman (2012)\nSilva Ayçaguer (1995)\nThompson (1994)\n(Wickham et al. 2019; Lüdecke et al. 2022; Sjoberg et al. 2021; Waring et al. 2022)"
  },
  {
    "objectID": "unidad_3/07_reg_lineal_multiple.html",
    "href": "unidad_3/07_reg_lineal_multiple.html",
    "title": "Regresión lineal múltiple",
    "section": "",
    "text": "Los modelos de Regresión Lineal Múltiple (RLM) son herramientas estadísticas ampliamente utilizadas cuando la variable dependiente es continua y existen dos o más variables independientes, que pueden ser tanto continuas como categóricas. En el caso de las covariables categóricas, estas pueden ser dicotómicas, ordinales o tener múltiples niveles.\nAl igual que la Regresión Lineal Simple (RLS), que permite estimar el efecto bruto de una variable independiente sobre la variable dependiente, la RLM nos permite conocer el efecto conjunto de dos o más variables independientes (\\(X_1\\), \\(X_2\\),…\\(X_k\\)) sobre la variable dependientea (\\(Y\\)). De esta manera, la RLM nos permite:\n\nAnalizar la dirección y fuerza de la asociación entre la variable dependiente y las variables independientes.\nIdentificar las variables independientes importantes en la predicción o explicación de la variable dependiente.\nDescribir la relación entre una o más variables independientes, controlando la confusión.\nDetectar interacciones, es decir, cómo cambia la relación entre la variable dependiente y una variable independiente según el nivel de otra variable independiente.\n\nEl modelo estadístico de la RLS que expresa la relación entre \\(X\\) e \\(Y\\) es:\n\\[\nY = \\beta_0 + \\beta_1X_1\n\\]\nEste modelo se representa gráficamente como una recta de ajuste en un plano bidimensional (dos dimensiones).\nPor otro lado, el modelo estadístico de la RLM es:\n\\[\nY = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + ...+\\beta_kX_k  \n\\]\nDonde \\(\\beta_0\\), \\(\\beta_1\\), \\(\\beta_2\\),…,\\(\\beta_k\\) son los parámetros de la regresión. Para cada combinación de valores de \\(X_1\\), \\(X_2\\),…\\(X_k\\) existe una distribución \\(Y\\) cuya media es una función lineal de \\(X_1\\), \\(X_2\\),…, \\(X_k\\).\n\n\n\n\n\n\n\n\nLa representación gráfica de la recta de ajuste en la regresión lineal múltiple se realiza en un espacio de dimensión \\(K + 1\\), donde \\(K\\) es el número de variables independientes. En el caso de la regresión lineal simple (RLS), la relación se puede representar fácilmente en un plano bidimensional (2D). Sin embargo, a medida que aumentamos el número de variables independientes, la representación gráfica se vuelve más compleja, ya que requerimos más dimensiones, lo que dificulta visualizar el modelo en el espacio.\nEn el caso puntual que el modelo tuviera 2 variables independientes, la ecuación sería:\n\\[\nY = \\beta_0 + \\beta_1 X_1 + \\beta_2X_2\n\\]\nEn este caso, la relación podría representarse en un plano tridimensional (3D), donde los ejes corresponden a \\(X_1\\), \\(X_2\\) y la variable dependiente \\(Y\\). En este plano, la superficie de ajuste es un plano que describe la relación entre las tres variables, y la orientación de dicho plano estará determinada por los coeficientes \\(\\beta_1\\) y \\(\\beta_2\\).\n\n\n\n\n\n\nEn forma similar a la RLS, la interpretación de cada parámetro \\(\\beta\\) de la regresión es:\n\n\\(\\beta_0\\): es el valor esperado de \\(Y\\) cuando todas las otras variables son iguales a cero.\n\\(\\beta_1\\) es la pendiente a lo largo del eje \\(X_1\\) y representa el cambio esperado en la respuesta por unidad de cambio en \\(X_1\\) a valores constantes de \\(X_2\\).\n\\(\\beta_2\\) es la pendiente a lo largo del eje \\(X_2\\) y representa el cambio esperado en la respuesta por unidad de cambio en \\(X_2\\) a valores constantes de \\(X_1\\)."
  },
  {
    "objectID": "unidad_3/07_reg_lineal_multiple.html#introducción",
    "href": "unidad_3/07_reg_lineal_multiple.html#introducción",
    "title": "Regresión lineal múltiple",
    "section": "",
    "text": "Los modelos de Regresión Lineal Múltiple (RLM) son herramientas estadísticas ampliamente utilizadas cuando la variable dependiente es continua y existen dos o más variables independientes, que pueden ser tanto continuas como categóricas. En el caso de las covariables categóricas, estas pueden ser dicotómicas, ordinales o tener múltiples niveles.\nAl igual que la Regresión Lineal Simple (RLS), que permite estimar el efecto bruto de una variable independiente sobre la variable dependiente, la RLM nos permite conocer el efecto conjunto de dos o más variables independientes (\\(X_1\\), \\(X_2\\),…\\(X_k\\)) sobre la variable dependientea (\\(Y\\)). De esta manera, la RLM nos permite:\n\nAnalizar la dirección y fuerza de la asociación entre la variable dependiente y las variables independientes.\nIdentificar las variables independientes importantes en la predicción o explicación de la variable dependiente.\nDescribir la relación entre una o más variables independientes, controlando la confusión.\nDetectar interacciones, es decir, cómo cambia la relación entre la variable dependiente y una variable independiente según el nivel de otra variable independiente.\n\nEl modelo estadístico de la RLS que expresa la relación entre \\(X\\) e \\(Y\\) es:\n\\[\nY = \\beta_0 + \\beta_1X_1\n\\]\nEste modelo se representa gráficamente como una recta de ajuste en un plano bidimensional (dos dimensiones).\nPor otro lado, el modelo estadístico de la RLM es:\n\\[\nY = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + ...+\\beta_kX_k  \n\\]\nDonde \\(\\beta_0\\), \\(\\beta_1\\), \\(\\beta_2\\),…,\\(\\beta_k\\) son los parámetros de la regresión. Para cada combinación de valores de \\(X_1\\), \\(X_2\\),…\\(X_k\\) existe una distribución \\(Y\\) cuya media es una función lineal de \\(X_1\\), \\(X_2\\),…, \\(X_k\\).\n\n\n\n\n\n\n\n\nLa representación gráfica de la recta de ajuste en la regresión lineal múltiple se realiza en un espacio de dimensión \\(K + 1\\), donde \\(K\\) es el número de variables independientes. En el caso de la regresión lineal simple (RLS), la relación se puede representar fácilmente en un plano bidimensional (2D). Sin embargo, a medida que aumentamos el número de variables independientes, la representación gráfica se vuelve más compleja, ya que requerimos más dimensiones, lo que dificulta visualizar el modelo en el espacio.\nEn el caso puntual que el modelo tuviera 2 variables independientes, la ecuación sería:\n\\[\nY = \\beta_0 + \\beta_1 X_1 + \\beta_2X_2\n\\]\nEn este caso, la relación podría representarse en un plano tridimensional (3D), donde los ejes corresponden a \\(X_1\\), \\(X_2\\) y la variable dependiente \\(Y\\). En este plano, la superficie de ajuste es un plano que describe la relación entre las tres variables, y la orientación de dicho plano estará determinada por los coeficientes \\(\\beta_1\\) y \\(\\beta_2\\).\n\n\n\n\n\n\nEn forma similar a la RLS, la interpretación de cada parámetro \\(\\beta\\) de la regresión es:\n\n\\(\\beta_0\\): es el valor esperado de \\(Y\\) cuando todas las otras variables son iguales a cero.\n\\(\\beta_1\\) es la pendiente a lo largo del eje \\(X_1\\) y representa el cambio esperado en la respuesta por unidad de cambio en \\(X_1\\) a valores constantes de \\(X_2\\).\n\\(\\beta_2\\) es la pendiente a lo largo del eje \\(X_2\\) y representa el cambio esperado en la respuesta por unidad de cambio en \\(X_2\\) a valores constantes de \\(X_1\\)."
  },
  {
    "objectID": "unidad_3/07_reg_lineal_multiple.html#presupuestos-del-modelo-de-rlm",
    "href": "unidad_3/07_reg_lineal_multiple.html#presupuestos-del-modelo-de-rlm",
    "title": "Regresión lineal múltiple",
    "section": "Presupuestos del modelo de RLM",
    "text": "Presupuestos del modelo de RLM\nIndependencia\nLas observaciones \\(Y_i\\) son independientes unas de otras: el efecto de \\(X_1\\) sobre la respuesta media no depende de \\(X_2\\) y viceversa, siempre y cuando no exista interacción. Cuando existe interacción entre \\(X_1\\) e \\(X_2\\) , el efecto de \\(X_1\\) sobre la respuesta media de \\(Y\\) depende \\(X_2\\) y viceversa (\\(X_1\\) e \\(X_2\\) no son independientes cuando existe interacción).\nLinealidad\nPara cada combinación de valores de las variables independientes (\\(X_1\\), \\(X_2\\),…, \\(X_k\\)) el valor medio de \\(Y\\) es función lineal de \\(X_1\\), \\(X_2\\),…,\\(X_k\\).. La linealidad se define en relación a los coeficientes de la regresión, por lo tanto el modelo puede incluir términos cuadráticos e interacciones\n\nModelo con interacción\n\n\\[\nY = \\beta_0X_1 + \\beta_2X_2 + \\beta_3X_1X_2\n\\]\n\n\nModelo con términos cuadráticos\n\\[\nY = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + \\beta_3X_1^2 + \\beta_4X_2^2\n\\]\n\nHomocedasticidad\nla varianza de \\(Y\\) para los distintos valores de \\(X_1\\), \\(X_2\\),…,\\(X_k\\) se mantiene constante.\nNormalidad\nLos valores de \\(Y\\) tienen una distribución normal según los valores de \\(X_1\\), \\(X_2\\), \\(X_k\\) , ésto nos permite realizar inferencias en relación a los parámetros del modelo.\nAl igual que en la RLS la estimación de los parámetros de la regresión (coeficientes) se realiza mediante el Método de los Mínimos Cuadrados (MMC). El mismo consiste en adoptar como estimativas de los parámetros de la regresión los valores que minimicen la suma de los cuadrados de los residuos.\n\\[\n\\sum_{i=1}^{i=n}e_1^2=\\sum_{i=1}^{i=n}(Y_i-\\hat{Y_i})^2=\\sum_{i=n}^{i=n}(Y_i-(\\hat{\\beta}_0+\\hat{\\beta}_1X_1+\\dots+\\hat{\\beta}_kX_k))^2\n\\]"
  },
  {
    "objectID": "unidad_3/07_reg_lineal_multiple.html#interpretación-del-modelo",
    "href": "unidad_3/07_reg_lineal_multiple.html#interpretación-del-modelo",
    "title": "Regresión lineal múltiple",
    "section": "Interpretación del modelo",
    "text": "Interpretación del modelo\nComenzaremos aprendiendo cómo interpretar un modelo de regresión lineal múltiple (RLM) antes de abordar su construcción. Para ilustrarlo, observemos en detalle la salida de R para un modelo de RLM en el que modelamos la variable V23, en función de las variables V10, V11, V12, V14, V15, V17, V18 y V24.\n\n\n\nCall:\nlm(formula = V23 ~ ., data = data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.45958 -0.57136  0.05354  0.57868  2.25036 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)  \n(Intercept)  0.09504    0.09502   1.000   0.3199  \nV10         -0.05189    0.09725  -0.534   0.5950  \nV11         -0.11508    0.09711  -1.185   0.2391  \nV12         -0.08137    0.09151  -0.889   0.3763  \nV14         -0.16411    0.09738  -1.685   0.0954 .\nV15         -0.03240    0.10283  -0.315   0.7534  \nV17         -0.04053    0.09040  -0.448   0.6550  \nV18          0.14425    0.09456   1.525   0.1306  \nV24         -0.04953    0.08940  -0.554   0.5809  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9081 on 91 degrees of freedom\nMultiple R-squared:  0.09027,   Adjusted R-squared:  0.0103 \nF-statistic: 1.129 on 8 and 91 DF,  p-value: 0.3518\n\n\nEn la salida obtenida, encontramos varios términos clave para interpretar:\n\nEstimate: muestra los coeficientes estimados (\\(\\beta\\)) para el intercepto (\\(\\beta_0\\)) y para cada una de las variables explicativas (\\(\\beta_i\\)).\nStd. Error: el error estándar de cada coeficiente estimado \\(\\beta\\).\nt-value: los valores del test \\(t\\) para cada coeficiente, que evalúan si los coeficientes son significativamente diferentes de cero.\nPr(&gt;|t|): los valores \\(p\\) asociados a los test \\(t\\) para cada coeficiente.\nResidual standard error: el error estándar de los residuales.\nMultiple R-squared: el coeficiente de determinación \\(R^2\\) múltiple, que indica qué proporción de la variabilidad de la variable dependiente es explicada por el modelo.\nAdjusted R-squared: el coeficiente de determinación ajustado, que penaliza por la inclusión de variables que no mejoran el modelo.\nF-statistic: el resultado del test \\(F\\) global, que evalúa la significancia del modelo en su conjunto, junto con su valor \\(p\\).\n\nTest F parcial\nEn un modelo de regresión, la estimación de los coeficientes se realiza partiendo de una muestra, lo que implica que diferentes muestras pueden generar distintos valores de los parámetros. El test F parcial evalúa la siguiente hipótesis:\n\\[\nH_0 = \\beta_1 = \\beta_2 = ... \\beta_n = 0  \\\\H_1 = \\exists\\beta_i \\neq 0\n\\]\nDonde \\(H_0\\) indica que todos los coeficientes de las variables independientes son cero, y \\(H_1\\) implica que al menos uno de los coeficientes es diferente de cero. Este test evalúa la contribución de cada variable al modelo, es decir, si la inclusión de esa variable mejora significativamente la explicación de la variabilidad de \\(Y\\). En modelos de regresión lineales generalizados (GLM), el test de Wald cumple esta misma función, mientras que en los modelos de regresión lineales múltiplos, el test \\(F\\) parcial cumple el mismo rol.\nVarianza residual\nAl igual que en la regresión lineal simpley el análisis de la varianza (ANOVA), podemos descomponer la variabilidad de la variable dependiente \\(Y\\) en dos componentes:\n\nVariabilidad explicada por el modelo.\nVariabilidad no explicada por el modelo, atribuida a factores aleatorios.\n\nMatemáticamente, esto se expresa como:\n\\[\nVariabilidad \\ total = Variabilidad \\ regresión \\ + \\ Variabilidad \\ residual\n\\]\nDe manera equivalente, la suma de cuadrados totales (SCT) se descompone en la suma de cuadrados de la regresión (SCR) y la suma de cuadrados residuales (SCE):\n\\[\n\\sum (y_i-\\bar{y})^2 = \\sum (\\hat{y}-\\bar{y})^2 + \\sum (y_i -\\hat{y}_i)^2\n\\\\\nSCT = SCR + SCE\n\\]\nA continuación, mostramos cómo estos términos se distribuyen entre los grados de libertad y se obtienen los cuadrados medios correspondientes:\n\n\n\n\n\nGrados de libertad\nCM\n\n\n\nSCT\nn-1\nCMT = SCT/n-1\n\n\nSCR\nk-1\nCMR = SCR/k-1\n\n\nSCE\nn-k-1\nCME = SCE/n-k-1\n\n\n\n\n\nTest F global\nEl test \\(F\\) global compara el modelo de regresión ajustado con el modelo nulo (sin variables independientes) y evalúa el efecto conjunto de todas las variables independientes. Su fórmula es:\n\\[\nF = CMR/CME\n\\\\\ngl = (k -1, n - k - 1)\n\\]\nCoeficiente de determinación\nAl igual de lo que aprendimos en la RLS la bondad de ajuste del modelo de RLM se valora con el coeficiente de determinación (\\(R^2\\)), que nos dice qué proporción de la variabilidad de \\(Y\\) es explicada por los coeficientes de la regresión del modelo en estudio. Su valor se calcula como:\n\\[ R^2 = \\frac{SCT-SCE}{SCT}=\\frac{SCR}{SCT} \\]\nSin embargo, al agregar más variables al modelo, \\(R^2\\) siempre mejora, incluso si la nueva variable no mejora sustancialmente el modelo. Este fenómeno se debe a que \\(R^2\\) solo refleja la proporción de variabilidad explicada, pero no penaliza la inclusión de variables que no aportan al modelo.\nPor esta razón, se utiliza el \\(R^2\\) ajustado, que corrige el \\(R^2\\) penalizando la inclusión de variables que no aportan significativamente:\n\\[ R^2_a = 1 - \\bigg [ \\frac{n-1}{n-(k+1)}\\bigg]\\frac{SCE}{SCT}=1-\\bigg[\\frac{n-1}{n-(k+1)}\\bigg](1-R^2)  \\]\nComo el término \\(1 - R^2\\) es constante y \\(n\\) es mayor que \\(k\\), a medida que agregamos variables, el cociente entre paréntesis crece, lo que hace que la penalización por agregar variables no significativas también aumente. Esto hace que el \\(R^2\\) ajustado sea una medida más fiable de la calidad del modelo.\n\n\n\n\n\n\nCuando tenemos que elegir el mejor modelo será necesario utilizar distintos criterios para compararlos y basar nuestra decisión en elegir el modelo que mejor explique la variación de \\(Y\\) con el menor número de variables independientes, el modelo más simple y efectivo, también llamado el modelo más parsimonioso."
  },
  {
    "objectID": "unidad_3/07_reg_lineal_multiple.html#variables-dummy",
    "href": "unidad_3/07_reg_lineal_multiple.html#variables-dummy",
    "title": "Regresión lineal múltiple",
    "section": "Variables dummy\n",
    "text": "Variables dummy\n\nEn los modelos de regresión, podemos incluir tanto variables cuantitativas como variables categóricas. Las variables categóricas pueden ser dicotómicas (por ejemplo, sexo: femenino/masculino; hábito de fumar: sí/no) o tener más de dos categorías (por ejemplo, grupo sanguíneo, religión, color de ojos).\nPara modelar las variables categóricas, cada categoría se transforma en una variable dicotómica (binaria), con un número de categorías menos uno. Es decir, para una variable con \\(n\\) categorías, se crearán \\(n-1\\) variables dummy, donde \\(1\\) indica la presencia de esa categoría y \\(0\\) indica su ausencia. En estos casos, se selecciona como grupo basal (o grupo de referencia) la categoría con el valor más bajo.\nEjemplo para variable independiente dicotómica\nSupongamos que tenemos una variable cualitativa dicotómica, como “Hábito de Fumar”, que toma los valores 1 (si fuma) y 0 (si no fuma). Si ajustamos un modelo con esta variable, la ecuación sería:\n\\[\nY = \\beta_0+\\beta_1F_1  \n\\]\nDonde:\n\n\\(Y\\) es la variable dependiente.\n\\(\\beta_0\\) es el intercepto (valor basal de \\(Y\\) cuando el individuo no fuma).\n\\(\\beta_1\\) es el coeficiente asociado al hábito de fumar, que indica el cambio en \\(Y\\) cuando el individuo fuma (es decir, la diferencia entre el grupo de fumadores y no fumadores).\nEjemplo para variable independiente con múltiples categorías\nAhora, consideremos una variable “Región”, que tiene tres categorías: Noreste (NE), Norte (N) y Centro Oeste (CO). Para modelarla como variables dummy, la transformamos en dos variables: \\(Re_1\\) y \\(Re_2\\), ya que hay \\(n-1 = 3-1 = 2\\) categorías dummy.\nLa transformación de las categorías sería:\n\n\n\n\nRegión\nRE1\nRE2\n\n\n\nNE (basal)\n0\n0\n\n\nN\n1\n0\n\n\nCO\n0\n1\n\n\n\n\n\nEn este caso:\n\n\\(Re_1 = 1\\) si vive en el Norte, \\(Re_1 = 0\\) si vive en otra región.\n\\(Re_2 = 1\\) si vive en el Centro Oeste, \\(Re_2 = 0\\) si vive en otra región.\n\nAhora imaginemos el modelo de regresión entre \\(Y\\) y la variable “región” (supongamos que \\(Y\\) es la Tasa de Mortalidad Infantil):\n\\[\nY = \\beta_0+\\beta_1Re_1 + \\beta_2Re_2\n\\]\nPara interpretar el modelo:\n\nSi el individuo vive en la región Norte (\\(Re_1 = 1\\) y \\(Re_2 = 0\\)), la ecuación se reduce a:\n\n\\[\nY=\\beta_0+\\beta_1Re_1  \n\\]\n\nSi el individuo vive en la región Centro Oeste (\\(Re_1 = 0\\) y \\(Re_2 = 1\\)), la ecuación sería:\n\n\\[ Y=\\beta_0+\\beta_2Re_2  \\]\n\nSi el individuo vive en la región Noreste (grupo basal, \\(Re_1 = 0\\) y \\(Re_2 = 0\\)), la ecuación se reduce a:\n\n\\[ Y=\\beta_0 \\]\nEn este contexto, \\(\\beta_1\\) y \\(\\beta_2\\) nos estarán indicando cuánto se modifica la TMI según consideremos la región Norte o Centro Oeste. El coeficiente \\(\\beta_0\\) es el valor basal medio de la TMI considerada en la región Noreste\nLas variables dummy o indicadoras no tienen un significado por sí solas, ya que su interpretación depende de cómo se comparan con el grupo basal. Es importante que todas las categorías de la variable cualitativa estén representadas en el modelo, y la inclusión de estas variables debe ser contrastada en bloque, incluso si algunos de los tests \\(F\\) parciales para una categoría no son significativos.\nAdemás, al agregar una variable dummy al modelo, esta incrementa los grados de libertad de la regresión en función de la cantidad de categorías de la variable. Por ejemplo, si una variable tiene \\(n\\) categorías, se agregarán \\(n-1\\) variables dummy, cada una con su respectivo grado de libertad.\nCuando trabajamos en R, el lenguaje se encarga de construir las variables dummy automáticamente siempre que su estructura sea factor. Los tipos de datos factor tienen una estructura de niveles donde el primero de ellos es el de referencia. Para modificar el nivel de referencia podemos utilizar las funciones relevel() de R base o fct_relevel() de tidyverse."
  },
  {
    "objectID": "unidad_3/07_reg_lineal_multiple.html#estimación-y-predicción-en-modelos-de-regresión",
    "href": "unidad_3/07_reg_lineal_multiple.html#estimación-y-predicción-en-modelos-de-regresión",
    "title": "Regresión lineal múltiple",
    "section": "Estimación y predicción en modelos de regresión",
    "text": "Estimación y predicción en modelos de regresión\nLa regresión es una herramienta clave en los análisis tanto explicativos como predictivos, y se utiliza de las siguientes maneras:\n\nCon fines explicativos: Este uso tiene como objetivo obtener estimaciones precisas sobre variables de interés para realizar inferencias o cuantificar relaciones entre variables, controlando por otras. Aquí se busca entender cómo los cambios en las variables independientes afectan a la variable dependiente.\nCon fines predictivos: La regresión también se utiliza para predecir el comportamiento de la variable dependiente (\\(Y\\)) basándose en los valores de las variables independientes (\\(X_k\\)) observados en la muestra. Sin embargo, la predicción solo debe hacerse si existe una correlación lineal adecuada y si el modelo ajustado es adecuado para los datos.\n\nConsideraciones para la predicción:\n\nNo utilizar el modelo si no hay correlación lineal: Si no existe una correlación lineal significativa entre \\(Y\\) y las \\(X_k\\), la predicción no será válida. En estos casos, la mejor predicción es la media muestral de \\(Y\\).\nNo extrapolar más allá de los valores muestrales conocidos: Las predicciones fuera del rango de los datos observados pueden ser inexactas.\nDatos actualizados: La predicción debe basarse en datos recientes y relevantes.\nNo predecir para poblaciones distintas: El modelo debe ser usado solo para hacer predicciones dentro de la población de la cual se obtuvieron los datos muestrales.\nRelación no necesariamente causal para modelos predictivos: Mientras que para modelos explicativos la relación entre \\(X\\) y \\(Y\\) debe ser causal (en términos temporales y lógicos), para la predicción basta con que exista una correlación."
  },
  {
    "objectID": "unidad_3/07_reg_lineal_multiple.html#lineamientos-la-selección-del-modelo-de-regresión",
    "href": "unidad_3/07_reg_lineal_multiple.html#lineamientos-la-selección-del-modelo-de-regresión",
    "title": "Regresión lineal múltiple",
    "section": "Lineamientos la selección del modelo de regresión",
    "text": "Lineamientos la selección del modelo de regresión\nDada una variable dependiente \\(Y\\) y un conjunto de \\(k\\) variables independientes \\(X_1\\), \\(X_2\\), \\(X_3\\)….,\\(X_k\\) , nuestro interés es definir el mejor conjunto de \\(p\\) predictores (\\(p \\leq k\\)) y el correspondiente modelo de regresión para describir la relación entre \\(Y\\) y las variables \\(X\\).\nPara ello, debemos seguir los siguientes pasos:\n\nEspecificar el conjunto de variables potencialmente predictivas/explicativas\nEvaluar colinealidad\nEspecificar un criterio estadístico para la elección de las variables\nEspecificar una estrategia para seleccionar modelos\nConducir el análisis específico\nEvaluar los presupuestos del modelo\nEvaluar la confiabilidad del modelo escogido\n\nIdentificación de variables potencialmente predictivas/explicativas\nEl primer paso para construir un modelo de regresión es seleccionar un conjunto adecuado de variables predictivas. Esto implica identificar aquellas variables que mejor expliquen la variable dependiente sin que ninguna de ellas sea combinación lineal de las restantes.\nEl primer paso a seguir es identificar las relaciones entre la variable dependiente y las variables independientes de forma bivariada utilizando tests de correlación y gráficos de dispersión para las variables explicativas continuas y tests de asociación para las variables explicativas categóricas.\nColinealidad\nUn problema frecuente en los modelos de RLM es el de la multicolinealidad, que ocurre cuando las variables independientes están relacionadas entre sí en forma lineal. Si bien no implica una violación de las hipótesis o presupuestos del modelo, puede ocasionar problemas en la inferencia, ya que:\n\nAumenta las varianzas y covarianzas de los estimadores.\nLos errores de las estimaciones serán grandes.\nTiende a producir estimadores con valores absolutos grandes.\nLos coeficientes de cada variable independiente difieren notablemente de los que se obtendrían por RLS.\nNo se puede identificar de forma precisa el efecto individual de cada variable colineal sobre la variable respuesta.\n\nA la hora de plantear modelos de RLM conviene estudiar previamente la existencia de casi-colinealidad (la colinealidad exacta no es necesario estudiarla previamente, ya que todos los algoritmos la detectan, de hecho no pueden acabar la estimación). Como medida de la misma hay varios estadísticos propuestos:\n\nPodemos examinar la matriz de correlación\nRealizar gráficos de dispersión entre las variables explicativas/predictivas\nCálculo del factor de inflación de la varianza (VIF por sus siglas en inglés):\n\n\\[\nVIF = \\frac{1}{1-R^2_i}\n\\]\nUna regla empírica, citada por Kleinbaum et al. (1988), consiste en considerar que existen problemas de colinealidad si algún VIF es superior a 10. Por otro lado Kim (2019) considera que un VIF entre 5 y 10 indican la presencia de colinealidad. En caso de detectar colinealidad entre dos predictores, existen dos posibles soluciones:\n\nExcluir uno de los predictores problemáticos intentando conservar el que, a juicio del investigador, está influyendo realmente en la variable respuesta\nCombinar las variables colineales en un único predictor, aunque con el riesgo de perder su interpretación\nSelección de variables\nGeneralmente incluiremos en el modelo aquellas variables que resultaron significativas en el análisis bivariado y otras que, aun cuando no resultaran significativas, decidamos mantener por cuestiones teóricas, porque se necesitan establecer predicciones para distintas categorías de dicha variable, etc.\nEl proceso de selección de variables puede realizarse en forma manual o automática, siendo este último desaconsejado por la mayoría de los autores, ya que en el proceso de ajuste de un modelo no sólo se involucran criterios estadísticos sino también conceptuales. Existen tres estrategias para realizar el proceso , basadas en el valor del test \\(F\\) parcial:\n\nMétodo jerárquico o forward: se basa en el criterio del investigador que introduce predictores determinados en un orden específico en relación al marco teórico. Comienza con un modelo nulo que solo contiene el intercepto (\\(\\beta_0\\)) y agrega secuencialmente una variable a la vez, eligiendo la que proporciona el mayor beneficio en términos de ajuste del modelo. Este proceso continúa hasta que agregar más variables no mejore significativamente el ajuste del modelo.\nMétodo de entrada forzada o backward: es el método inverso al anterior. Se introducen todos los predictores simultáneamente y, en cada paso, elimina la variable que tenga el menor impacto en el ajuste del modelo. Este proceso continúa hasta que eliminar más variables no mejore significativamente el ajuste del modelo. Permite evaluar cada variable en presencia de las otras.\nMétodo paso a paso o stepwise: emplea criterios matemáticos para decidir qué predictores contribuyen significativamente al modelo y en qué orden se introducen. Se trata de una combinación de la selección forward y backward. Comienza con un modelo nulo, pero tras cada nueva incorporación se realiza un test de extracción de predictores no útiles como en el backward. Presenta la ventaja de que si a medida que se añaden predictores, alguno de los ya presentes deja de contribuir al modelo, se elimina.\nSelección de modelos\nHasta ahora hemos desarrollado algunos criterios que se pueden utilizar para comparar modelos como \\(R^2\\), \\(R^2\\) ajustado y \\(F\\) global. Como hemos mencionado anteriormente el uso de \\(R^2\\) como único criterio de selección tiene varias desventajas: tiende a sobreestimar, al adicionar variables siempre aumenta, por lo que si fuera el único criterio elegiría modelos con el mayor número de variables, no tiene en consideración la relación entre parámetros y tamaño muestral.\nPara modelos anidados, podemos realizar una comparación entre ambos modelos mediante un ANOVA. Existen otros criterios como el Criterio de Información de Akaike (AIC), el Criterio de Información Bayesiano (BIC), etc. Tanto el BIC como el AIC, son funciones del logaritmo de la verosimilitud y un término de penalidad basado en el número de parámetros del modelo.\nRecuerden que frente a \\(p\\) variables independientes existen \\(2^p\\) posibles modelos. No necesariamente el modelo con mayor número de variables es el mejor. Debemos priorizar siempre el principio de parsimonia (el modelo más simple que mejor explique). El tamaño de la muestra también es importante, algunos autores recomiendan que el número de observaciones sea como mínimo entre 10 y 20 veces el número de predictores del modelo."
  },
  {
    "objectID": "unidad_3/07_reg_lineal_multiple.html#validación-y-diagnóstico-del-modelo",
    "href": "unidad_3/07_reg_lineal_multiple.html#validación-y-diagnóstico-del-modelo",
    "title": "Regresión lineal múltiple",
    "section": "Validación y diagnóstico del modelo",
    "text": "Validación y diagnóstico del modelo\nUtilizaremos el análisis de los residuales para realizar los contrastes a posteriori de las hipótesis del modelo. Recordemos que los residuos o residuales se definen como la diferencia entre el valor observado y el valor predicho por el modelo.\n\\[\ny-\\hat{y}=e~(residuo~ o ~error~ residual)\n\\]\nEl planteamiento habitual es considerar que, como dijimos inicialmente, los valores de \\(Y\\) tienen una distribución normal según los valores de \\(X_1\\), \\(X_2\\),… \\(X_k\\)., entonces, los residuos también tendrán una distribución normal. Los residuos tienen unidades de medida y, por tanto no se puede determinar si es grande o pequeño a simple vista. Para solucionar este problema se define el residuo estandarizado como el cociente entre el residuo y su desvío estandar. Se considera que un residuo tiene un valor alto, y por lo tanto puede influir negativamente en el análisis, si su residuo estandarizado es mayor a 3 en valor absoluto. También se trabaja con los residuos tipificados o con los residuos estudentizados.\nNormalidad\nEl análisis de normalidad de los residuos lo realizaremos gráficamente (Histograma y gráfico de probabilidad normal) y analíticamente (Contraste de Kolmogorov-Smirnov) o similar.\nHomocedasticidad y linealidad\nLa hipótesis de homocedasticidad establece que la variabilidad de los residuos es independiente de las variables explicativas. En general, la variabilidad de los residuos estará en función de las variables explicativas, pero como las variables explicativas están fuertemente correlacionadas con la variable dependiente, bastara con examinar el gráfico de valores pronosticados versus residuos (a veces residuos al cuadrado).\nComprobamos la hipótesis de homogeneidad de las varianzas gráficamente representando los residuos tipificados frente a los valores predichos por el modelo. El análisis de este gráfico puede revelar una posible violación de la hipótesis de homocedasticidad, por ejemplo si detectamos que el tamaño de los residuos aumenta o disminuye de forma sistemática para algunos valores ajustados de la variable \\(Y\\), si observamos que el gráfico muestra forma de embudo. Si por el contrario dicho gráfico no muestra patrón alguno, entonces no podemos rechazar la hipótesis de igualdad de varianzas.\nValores de influencia (leverage)\nSe considera que una observación es influyente a priori si su inclusión en el análisis modifica sustancialmente el sentido del mismo. Una observación puede ser influyente si es un outlier respecto a alguna de las variables explicativas. Para detectar estos problemas se utiliza la medida de Leverage:\n\\[\nl(i)=\\frac{1}{n}\\bigg(1+\\frac{(x_i-\\bar{x})^2}{S^2_x}\\bigg)  \n\\]\nEste estadístico mide la distancia de un punto a la media de la distribución. Valores cercanos a 2/\\(n\\) indican casos que pueden influir negativamente en la estimación del modelo introduciendo un fuerte sesgo en el valor de los estimadores.\nDistancia de Cook\nEs una medida de cómo influye la observación \\(i\\)-ésima sobre la estimación de \\(\\beta\\) al ser retirada del conjunto de datos. Una distancia de Cook grande significa que una observación tiene un peso grande en la estimación de \\(\\beta\\). Son puntos influyentes las observaciones que presenten\n\\[ D_i=\\frac{4}{n-p-2}  \\]\nIndependencia de residuos\nLa hipótesis de independencia de los residuos la realizaremos mediante el contraste de Durbin-Watson."
  },
  {
    "objectID": "unidad_3/07_reg_lineal_multiple.html#ejemplo-práctico-en-r",
    "href": "unidad_3/07_reg_lineal_multiple.html#ejemplo-práctico-en-r",
    "title": "Regresión lineal múltiple",
    "section": "Ejemplo práctico en R",
    "text": "Ejemplo práctico en R\nEn el documento Introducción al modelado estadístico vimos como generar en R una fórmula para un modelo lineal simple:\n\\[ variable \\ dependiente \\; \\sim variable  \\ independiente \\]\nPara generar fórmulas que contengan más de una variable independiente o predictora (necesario para que sea una regresión lineal múltiple) debemos agregarlas mediante el símbolo \\(+\\)\n\\[ variable \\ dependiente \\ \\sim var\\_indepen\\_1 \\ + \\ var\\_indepen\\_2 \\ + \\; \\dots \\ + \\ var\\_indepen\\_n \\]\nSi luego de la ~ incluímos un punto como notación de “todas”, estamos creando un modelo saturado con todas las variables incluidas dentro del dataframe:\n\nlm(y ~., data = datos)\n\nEsto es útil cuando tenemos muchas posibles variables explicativas y queremos conocer cuáles tienen una correlación significativa.\nTambién se puede descartar alguna o algunas variables explicativas en particular basado en la misma estructura, mediante el símbolo \\(-\\)\n\nlm(y ~ .-x2, data = datos)\n\nEn la línea anterior, incluimos dentro del modelo a todas las variables de data menos x2.\nUsaremos para el ejemplo el dataset “cancer_USA.txt”, que contiene información sobre la tasa de mortalidad por cáncer cada 100.000 habitantes de distintos condados de Estados Unidos.\nLectura de datos y visualización de estructura\nPara comenzar, cargamos los paquetes de R necesarios para el análisis:\n\n# correlogramaas \nlibrary(GGally) \n\n# chequeo de supuestos y análisis de residuales  \nlibrary(easystats) \nlibrary(gvlma)  \nlibrary(lmtest)   \nlibrary(nortest)\n\n# manejo de datos  \nlibrary(tidyverse)      \n\nA continuación leemos los datos y realizamos una exploración rápida\n\n# Carga datos\ndatos &lt;- read_csv2(\"datos/cancer_USA.txt\")\n\n# Explora datos\nglimpse(datos)\n\nRows: 213\nColumns: 10\n$ condado            &lt;chr&gt; \"Belknap County\", \"Carroll County\", \"Cheshire Count…\n$ estado             &lt;chr&gt; \"New Hampshire\", \"New Hampshire\", \"New Hampshire\", …\n$ tasa_mortalidad    &lt;dbl&gt; 182.6, 168.8, 162.8, 181.6, 155.0, 163.2, 173.1, 17…\n$ mediana_edad       &lt;dbl&gt; 46.1, 50.3, 42.0, 48.1, 41.9, 40.1, 42.6, 43.5, 37.…\n$ mediana_edad_cat   &lt;chr&gt; \"45+ años\", \"45+ años\", \"36-39 años\", \"45+ años\", \"…\n$ mediana_ingresos   &lt;dbl&gt; 59831, 57556, 56008, 42491, 56353, 71233, 62429, 79…\n$ pct_pobreza        &lt;dbl&gt; 10.0, 10.7, 11.8, 14.9, 11.6, 8.7, 9.5, 6.1, 11.8, …\n$ pct_salud_publica  &lt;dbl&gt; 16.8, 13.4, 12.7, 22.5, 14.0, 12.7, 13.1, 9.0, 12.4…\n$ pct_sec_incompleta &lt;dbl&gt; 15.4, 14.8, 6.1, 13.2, 6.6, 12.9, 10.9, 13.5, 6.1, …\n$ pct_desempleo      &lt;dbl&gt; 5.3, 5.8, 6.1, 6.9, 5.0, 5.9, 5.2, 5.6, 6.5, 5.8, 1…\n\n\nLa base de datos tiene 100 observaciones y 9 variables. La variable dependiente es tasa_mortalidad y hay 3 variables independientes categóricas y 6 variables independientes numéricas o continuas.\nAnálisis de variables potencialmente explicativas\nAnalizaremos la relación entre las variables explicativas numéricas y la variable dependiente mediante un correlograma con el paquete GGally:\n\ndatos |&gt; \n  # selecciono variables numéricas\n  select(where(is.numeric)) |&gt; \n  \n  # correlograma\n  ggcorr(label = T)\n\n\n\n\n\n\n\nComo habíamos observado en el capítulo sobre covarianza y correlación, la tasa_mortalidad tiene correlación negativa moderada con mediana_ingresos, también que existe una correlación negativa fuerte entre pct_pobreza y mediana_ingresos y moderada con pct_salud_publica.\nEstrategia de construcción del modelo\nAl momento de seleccionar las variables independientes que formarán parte del modelo, una de las herramientas más utilizadas es el Criterio de Información de Akaike (AIC) que ajusta mediante máxima verosimilitud. Al penalizar la complejidad excesiva, el AIC ayuda a prevenir el sobreajuste y favorece la inclusión de variables relevantes buscando el modelo equilibrado que describa adecuadamente la relación y tenga el mínimo AIC. Podemos consultar el AIC de un modelo en R mediante la función AIC().\nComo vimos anteriormente, la selección de variables se puede hacer mediante métodos forward, backward o stepwise. La función step() de R base permite encontrar de forma automática el mejor modelo basado en AIC utilizando cualquiera de las 3 variantes del método paso a paso. Por otro lado, la función drop1() de R base nos permite realizar un proceso backward manual, eligiendo que variable quitar del modelo en cada caso según su contribución al AIC. Siempre es oportuno aclarar que estos últimos métodos se basan en cálculos matemático/estadísticos que no tienen en cuenta criterios conceptuales epidemiológicos que surjan del marco teórico, por lo que exigen un control especial del analista.\nel lenguaje R ofrece una variedad de funciones para abordar y facilitar la tarea de seleccionar el mejor modelo de regresión (más parsimonioso). Como existe correlación fuerte entre mediana_ingresos, pct_pobreza y pct_salud_publica sólo usaremos la que presente mayor correlación con tasa_mortalidad.\n\n# Modelo saturado\nmod_full &lt;- lm(tasa_mortalidad ~ mediana_ingresos + mediana_edad +\n             pct_sec_incompleta + pct_desempleo + estado,\n           data = datos)\n\n# Mediana ingresos\nmod1 &lt;- lm(tasa_mortalidad ~ mediana_ingresos, data = datos)\n\n# Mediana edad\nmod2 &lt;- lm(tasa_mortalidad ~ mediana_edad, data = datos)\n\n# Secundaria incompleta\nmod3 &lt;- lm(tasa_mortalidad ~ pct_sec_incompleta, data = datos)\n\n# Desempleo\nmod4 &lt;- lm(tasa_mortalidad ~ pct_desempleo, data = datos)\n\n# Estado\nmod5 &lt;- lm(tasa_mortalidad ~ estado, data = datos)\n\nMediante summary() observamos sus resultados:\n\n# Modelo saturado\nsummary(mod_full)\n\n\nCall:\nlm(formula = tasa_mortalidad ~ mediana_ingresos + mediana_edad + \n    pct_sec_incompleta + pct_desempleo + estado, data = datos)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-31.475  -7.254   1.161   7.192  29.261 \n\nCoefficients:\n                      Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)          1.488e+02  1.487e+01  10.006  &lt; 2e-16 ***\nmediana_ingresos    -3.810e-04  8.165e-05  -4.666 5.62e-06 ***\nmediana_edad         5.005e-01  2.588e-01   1.934  0.05453 .  \npct_sec_incompleta   1.652e-01  1.945e-01   0.849  0.39681    \npct_desempleo        1.643e+00  5.781e-01   2.842  0.00494 ** \nestadoMaine          1.462e+01  5.662e+00   2.582  0.01054 *  \nestadoMassachusetts  6.389e+00  5.344e+00   1.196  0.23328    \nestadoNew Hampshire  1.166e+01  5.963e+00   1.955  0.05198 .  \nestadoNew Jersey     1.020e+01  5.010e+00   2.036  0.04302 *  \nestadoNew York       7.909e+00  4.706e+00   1.680  0.09443 .  \nestadoPennsylvania   9.905e+00  4.867e+00   2.035  0.04315 *  \nestadoRhode Island   5.131e+00  7.359e+00   0.697  0.48646    \nestadoVermont        1.325e+01  5.814e+00   2.278  0.02377 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 11.93 on 200 degrees of freedom\nMultiple R-squared:  0.3373,    Adjusted R-squared:  0.2975 \nF-statistic: 8.483 on 12 and 200 DF,  p-value: 6.303e-13\n\n# Modelos simples\nsummary(mod1)\n\n\nCall:\nlm(formula = tasa_mortalidad ~ mediana_ingresos, data = datos)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-33.040  -8.210   1.093   7.637  35.932 \n\nCoefficients:\n                   Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       2.014e+02  3.521e+00  57.200  &lt; 2e-16 ***\nmediana_ingresos -5.200e-04  6.133e-05  -8.479 3.94e-15 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 12.32 on 211 degrees of freedom\nMultiple R-squared:  0.2542,    Adjusted R-squared:  0.2506 \nF-statistic:  71.9 on 1 and 211 DF,  p-value: 3.938e-15\n\nsummary(mod2)\n\n\nCall:\nlm(formula = tasa_mortalidad ~ mediana_edad, data = datos)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-31.870  -9.631   0.512   9.539  46.770 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  134.7306    10.9761  12.275  &lt; 2e-16 ***\nmediana_edad   0.8961     0.2600   3.447 0.000684 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 13.88 on 211 degrees of freedom\nMultiple R-squared:  0.05332,   Adjusted R-squared:  0.04883 \nF-statistic: 11.88 on 1 and 211 DF,  p-value: 0.0006835\n\nsummary(mod3)\n\n\nCall:\nlm(formula = tasa_mortalidad ~ pct_sec_incompleta, data = datos)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-33.903  -9.319   1.485   9.185  41.404 \n\nCoefficients:\n                   Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)        162.9089     2.8037  58.106  &lt; 2e-16 ***\npct_sec_incompleta   0.7202     0.1997   3.607 0.000387 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 13.84 on 211 degrees of freedom\nMultiple R-squared:  0.05808,   Adjusted R-squared:  0.05362 \nF-statistic: 13.01 on 1 and 211 DF,  p-value: 0.0003867\n\nsummary(mod4)\n\n\nCall:\nlm(formula = tasa_mortalidad ~ pct_desempleo, data = datos)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-35.594  -8.948   0.508   9.029  40.157 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   158.9950     4.1741  38.091  &lt; 2e-16 ***\npct_desempleo   1.7906     0.5418   3.305  0.00112 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 13.91 on 211 degrees of freedom\nMultiple R-squared:  0.04922,   Adjusted R-squared:  0.04471 \nF-statistic: 10.92 on 1 and 211 DF,  p-value: 0.001117\n\nsummary(mod5)\n\n\nCall:\nlm(formula = tasa_mortalidad ~ estado, data = datos)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-32.840  -9.533   0.360   9.100  36.641 \n\nCoefficients:\n                    Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)          157.713      4.785  32.960  &lt; 2e-16 ***\nestadoMaine           25.438      5.860   4.341 2.24e-05 ***\nestadoMassachusetts    7.087      5.998   1.182 0.238741    \nestadoNew Hampshire   13.167      6.420   2.051 0.041533 *  \nestadoNew Jersey      10.921      5.623   1.942 0.053491 .  \nestadoNew York        13.928      5.084   2.739 0.006700 ** \nestadoPennsylvania    17.347      5.075   3.418 0.000762 ***\nestadoRhode Island     8.263      8.288   0.997 0.319968    \nestadoVermont         18.559      5.998   3.094 0.002251 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 13.53 on 204 degrees of freedom\nMultiple R-squared:  0.1297,    Adjusted R-squared:  0.09555 \nF-statistic:   3.8 on 8 and 204 DF,  p-value: 0.0003514\n\n\nEn los resúmenes observamos que todos los coeficientes de las variables involucradas son significativas y que los valores mostrados tienen forma de lista, es decir no son tablas “ordenadas”, por lo que muchas veces cuando trabajamos con numerosas variables, se hace difícil la comparación de resultados.\nComparación de modelos\nEl comando compare_performance() del paquete performance permite comparar de manera sencilla entre modelos anidados:\n\ncompare_performance(mod_full,\n                    mod1, mod2, mod3, mod4, mod5,\n                    metrics = \"common\")\n\n# Comparison of Model Performance Indices\n\nName     | Model |  AIC (weights) |  BIC (weights) |    R2 | R2 (adj.) |   RMSE\n-------------------------------------------------------------------------------\nmod_full |    lm | 1675.0 (0.830) | 1722.1 (&lt;.001) | 0.337 |     0.298 | 11.557\nmod1     |    lm | 1678.2 (0.170) | 1688.3 (&gt;.999) | 0.254 |     0.251 | 12.261\nmod2     |    lm | 1729.0 (&lt;.001) | 1739.1 (&lt;.001) | 0.053 |     0.049 | 13.814\nmod3     |    lm | 1727.9 (&lt;.001) | 1738.0 (&lt;.001) | 0.058 |     0.054 | 13.779\nmod4     |    lm | 1729.9 (&lt;.001) | 1740.0 (&lt;.001) | 0.049 |     0.045 | 13.844\nmod5     |    lm | 1725.1 (&lt;.001) | 1758.7 (&lt;.001) | 0.130 |     0.096 | 13.245\n\n\nLas métricas comunes extraídas de los objetos de regresión, presentadas en una tabla, son:\n\nAIC: Criterio de información de Akaike\nBIC: Criterio de información bayesiano\nR2: Coeficiente de determinación \\(R^2\\)\nR2 (adj.): Coeficiente de determinación ajustado\nRMSE: Error cuadrático medio\n\nTambién podemos ordenar los modelos de mejor a peor con el argumento rank = TRUE:\n\ncompare_performance(mod_full,\n                    mod1, mod2, mod3, mod4, mod5,\n                    metrics = \"common\",\n                    rank = TRUE)\n\n# Comparison of Model Performance Indices\n\nName     | Model |    R2 | R2 (adj.) |   RMSE | AIC weights | BIC weights | Performance-Score\n---------------------------------------------------------------------------------------------\nmod_full |    lm | 0.337 |     0.298 | 11.557 |       0.830 |    4.58e-08 |            80.00%\nmod1     |    lm | 0.254 |     0.251 | 12.261 |       0.170 |       1.000 |            68.44%\nmod5     |    lm | 0.130 |     0.096 | 13.245 |    1.12e-11 |    5.15e-16 |            14.85%\nmod3     |    lm | 0.058 |     0.054 | 13.779 |    2.71e-12 |    1.60e-11 |             1.89%\nmod2     |    lm | 0.053 |     0.049 | 13.814 |    1.59e-12 |    9.36e-12 |             0.87%\nmod4     |    lm | 0.049 |     0.045 | 13.844 |    1.00e-12 |    5.90e-12 |         1.18e-10%\n\n\nPodemos ver que el modelo saturado tiene un menor AIC y performance global que los modelos univariados.\nColinealidad\nHay varias maneras de detectar la presencia de colinealidad entre variables independientes:\n\nAntes de ajustar el modelo, al comparar los coeficientes de correlación.\nUna vez ajustado el modelo, si tenemos un \\(R^2\\) alto y muchas variables independientes que no son estadísticamente significativas.\n\nCuando hemos intuido que tenemos multicolinealidad y queremos comprobar, el paquete performance nos ofrece la función check_collinearity() que implementa el método de factor de inflación de la varianza (VIF):\n\ncheck_collinearity(mod_full)\n\n# Check for Multicollinearity\n\nLow Correlation\n\n               Term  VIF   VIF 95% CI Increased SE Tolerance Tolerance 95% CI\n   mediana_ingresos 1.89 [1.60, 2.31]         1.38      0.53     [0.43, 0.62]\n       mediana_edad 1.34 [1.19, 1.63]         1.16      0.75     [0.61, 0.84]\n pct_sec_incompleta 1.28 [1.14, 1.56]         1.13      0.78     [0.64, 0.88]\n      pct_desempleo 1.55 [1.34, 1.88]         1.24      0.65     [0.53, 0.75]\n             estado 2.52 [2.09, 3.12]         1.59      0.40     [0.32, 0.48]\n\n\nLos resultados del ejemplo muestra VIF de entre 1.28 y 2.52 (no hay colinealidad). Recordemos que el umbral de detección parte de valores cercanos a 5 y un \\(VIF \\geq 10\\) indicaría que el modelo de regresión lineal presenta un grado de multicolinealidad preocupante.\nAnálisis de residuales\nComo vimos para la regresión lineal simple, podemos analizar gráficamente los residuales con la función check_model() del paquete performance:\n\ncheck_model(mod_full)\n\n\n\n\n\n\n\nPor otra parte, también tenemos las funciones de análisis para los supuestos (independencia, linealidad, normalidad, homocedasticidad).\n\n\nIndependencia: El paquete lmtest permite evaluar independencia según estadístico de Durbin-Watson.\n\n\ndwtest(mod_full)\n\n\n    Durbin-Watson test\n\ndata:  mod_full\nDW = 2.1769, p-value = 0.7772\nalternative hypothesis: true autocorrelation is greater than 0\n\n\n\n\nLinealidad: El paquete lmtest implementa el Ramsey’s RESET bajo la función resettest()\n\n\n\nresettest(mod_full)\n\n\n    RESET test\n\ndata:  mod_full\nRESET = 0.66971, df1 = 2, df2 = 198, p-value = 0.513\n\n\n\n\nNormalidad: El paquete nortest permite chequear normalidad mediante test de Lilliefors.\n\n\nlillie.test(mod_full$residuals)\n\n\n    Lilliefors (Kolmogorov-Smirnov) normality test\n\ndata:  mod_full$residuals\nD = 0.060065, p-value = 0.05954\n\n\n\n\nHomocedasticidad: El paquete performance permite chequear homocedasticidad con el test de Breush-Pagan.\n\n\ncheck_heteroscedasticity(mod_full)\n\nWarning: Heteroscedasticity (non-constant error variance) detected (p = 0.012).\n\n\nFinalmente presentamos un paquete interesante para validar supuestos de modelos lineales denominado gvlma(Pena y Slate 2019). Su función, de mismo nombre gvlma(), implementa un procedimiento global sobre vector residual estandarizado para probar los cuatro supuestos del modelo lineal.\nSi el procedimiento global indica una violación de al menos uno de los supuestos, los componentes se pueden utilizar para obtener información sobre qué supuestos se han violado.\n\ngvlma(mod_full)\n\n\nCall:\nlm(formula = tasa_mortalidad ~ mediana_ingresos + mediana_edad + \n    pct_sec_incompleta + pct_desempleo + estado, data = datos)\n\nCoefficients:\n        (Intercept)     mediana_ingresos         mediana_edad  \n         148.784543            -0.000381             0.500486  \n pct_sec_incompleta        pct_desempleo          estadoMaine  \n           0.165173             1.643031            14.617808  \nestadoMassachusetts  estadoNew Hampshire     estadoNew Jersey  \n           6.388818            11.657941            10.203316  \n     estadoNew York   estadoPennsylvania   estadoRhode Island  \n           7.908722             9.905042             5.131109  \n      estadoVermont  \n          13.245742  \n\n\nASSESSMENT OF THE LINEAR MODEL ASSUMPTIONS\nUSING THE GLOBAL TEST ON 4 DEGREES-OF-FREEDOM:\nLevel of Significance =  0.05 \n\nCall:\n gvlma(x = mod_full) \n\n                    Value p-value                Decision\nGlobal Stat        2.3043  0.6800 Assumptions acceptable.\nSkewness           0.9587  0.3275 Assumptions acceptable.\nKurtosis           0.0572  0.8110 Assumptions acceptable.\nLink Function      0.4246  0.5147 Assumptions acceptable.\nHeteroscedasticity 0.8639  0.3527 Assumptions acceptable.\n\n\nResumen del mejor modelo elegido\n\nsummary(mod_full)\n\n\nCall:\nlm(formula = tasa_mortalidad ~ mediana_ingresos + mediana_edad + \n    pct_sec_incompleta + pct_desempleo + estado, data = datos)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-31.475  -7.254   1.161   7.192  29.261 \n\nCoefficients:\n                      Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)          1.488e+02  1.487e+01  10.006  &lt; 2e-16 ***\nmediana_ingresos    -3.810e-04  8.165e-05  -4.666 5.62e-06 ***\nmediana_edad         5.005e-01  2.588e-01   1.934  0.05453 .  \npct_sec_incompleta   1.652e-01  1.945e-01   0.849  0.39681    \npct_desempleo        1.643e+00  5.781e-01   2.842  0.00494 ** \nestadoMaine          1.462e+01  5.662e+00   2.582  0.01054 *  \nestadoMassachusetts  6.389e+00  5.344e+00   1.196  0.23328    \nestadoNew Hampshire  1.166e+01  5.963e+00   1.955  0.05198 .  \nestadoNew Jersey     1.020e+01  5.010e+00   2.036  0.04302 *  \nestadoNew York       7.909e+00  4.706e+00   1.680  0.09443 .  \nestadoPennsylvania   9.905e+00  4.867e+00   2.035  0.04315 *  \nestadoRhode Island   5.131e+00  7.359e+00   0.697  0.48646    \nestadoVermont        1.325e+01  5.814e+00   2.278  0.02377 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 11.93 on 200 degrees of freedom\nMultiple R-squared:  0.3373,    Adjusted R-squared:  0.2975 \nF-statistic: 8.483 on 12 and 200 DF,  p-value: 6.303e-13\n\n\nEl modelo mod_full es capaz de explicar el 29,8% de la variabilidad observada en la tasa de mortalidad por cáncer en estos condados (\\(R^2_{ajustado} = 0,2975\\)). El test \\(F\\) global muestra que es significativo (\\(p&lt; 0,001\\)).\nLos coeficientes se pueden interpretar como:\n\nMediana de ingresos: Cada aumento de una unidad en la mediana de ingresos reduce la tasa de mortalidad en 0,0004 unidades.\nMediana de edad: Cuando aumento una unidad en la mediana de edad incrementa la tasa de mortalidad en 0,5 unidades.\nPorcentaje de población con secundaria incompleta: Cada aumento de una unidad en este porcentaje incrementa la tasa de mortalidad en 0,17 unidades, pero esta relación no es significativa.\nPorcentaje de desempleo: Cada aumento de una unidad en el desempleo incrementa la tasa de mortalidad en 1.64 unidades.\nEstados: Las tasas de mortalidad varían entre los estados, con algunos efectos significativos, como en Maine, New Jersey, Pennsylvania y Vermont, todos con tasas de mortalidad más altas que el estado de referencia. Es decir, la recta de regresión para cada estado tendrá un intercepto diferente."
  },
  {
    "objectID": "unidad_3/07_reg_lineal_multiple.html#funciones-para-modelado-automático",
    "href": "unidad_3/07_reg_lineal_multiple.html#funciones-para-modelado-automático",
    "title": "Regresión lineal múltiple",
    "section": "Funciones para modelado automático",
    "text": "Funciones para modelado automático\nEl uso de la función step() se enmarca en los procedimientos automáticos y puede ser forward, backward o mixto. La sintaxis básica de la función es:\n\nstep(object = modelo, direction = c(\"both\", \"backward\", \"forward\"))\n\ndonde:\n\n\nobject: es el modelo inicial de regresión lineal (nulo o saturado).\n\ndirection: es la dirección que indicamos. Puede ser \"forward\", \"backward\" o \"both\" (predeterminado, si no se define).\n\nGeneralmente conviene partir del modelo saturado y utilizar la dirección por defecto, donde se usan ambas direcciones.\n\nmodelo_step &lt;- step(mod_full, direction = \"both\")\n\nStart:  AIC=1068.56\ntasa_mortalidad ~ mediana_ingresos + mediana_edad + pct_sec_incompleta + \n    pct_desempleo + estado\n\n                     Df Sum of Sq   RSS    AIC\n- estado              8   1470.71 29922 1063.3\n- pct_sec_incompleta  1    102.58 28554 1067.3\n&lt;none&gt;                            28452 1068.6\n- mediana_edad        1    532.09 28984 1070.5\n- pct_desempleo       1   1149.28 29601 1075.0\n- mediana_ingresos    1   3096.96 31549 1088.6\n\nStep:  AIC=1063.3\ntasa_mortalidad ~ mediana_ingresos + mediana_edad + pct_sec_incompleta + \n    pct_desempleo\n\n                     Df Sum of Sq   RSS    AIC\n- pct_sec_incompleta  1      79.5 30002 1061.9\n&lt;none&gt;                            29922 1063.3\n- mediana_edad        1    1033.9 30956 1068.5\n- pct_desempleo       1    1036.6 30959 1068.5\n+ estado              8    1470.7 28452 1068.6\n- mediana_ingresos    1    6922.4 36845 1105.6\n\nStep:  AIC=1061.86\ntasa_mortalidad ~ mediana_ingresos + mediana_edad + pct_desempleo\n\n                     Df Sum of Sq   RSS    AIC\n&lt;none&gt;                            30002 1061.9\n+ pct_sec_incompleta  1      79.5 29922 1063.3\n+ estado              8    1447.7 28554 1067.3\n- pct_desempleo       1    1176.8 31179 1068.1\n- mediana_edad        1    1302.7 31305 1068.9\n- mediana_ingresos    1    7528.4 37530 1107.5\n\n\nObservamos que el proceso automático nos indica que debemos eliminar las variables explicativas pct_sec_incompleta y estado.\n\nsummary(modelo_step)\n\n\nCall:\nlm(formula = tasa_mortalidad ~ mediana_ingresos + mediana_edad + \n    pct_desempleo, data = datos)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-31.570  -7.621   1.269   7.564  32.218 \n\nCoefficients:\n                   Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       1.574e+02  1.271e+01  12.388  &lt; 2e-16 ***\nmediana_ingresos -4.517e-04  6.237e-05  -7.242 8.34e-12 ***\nmediana_edad      7.065e-01  2.345e-01   3.013  0.00291 ** \npct_desempleo     1.397e+00  4.881e-01   2.863  0.00462 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 11.98 on 209 degrees of freedom\nMultiple R-squared:  0.3012,    Adjusted R-squared:  0.2912 \nF-statistic: 30.03 on 3 and 209 DF,  p-value: 3.488e-16\n\n\nSi bien en el ejemplo mostramos como usar el método stepwise automático para la selección de variables, no se recomienda su uso ya que está basado únicamente en criterios matemáticos y no en la relevancia en el mundo real.\nAdemás, cuenta con las siguientes limitaciones:\n\nSensibilidad a la selección de variables\nPropensión al sobreajuste\nViolación de supuestos estadísticos\nInclusión de variables irrelevantes\n\nPara evitar estos inconvenientes, es recomendable usar métodos alternativos de selección de variables tales como:\n\nSelección manual de variables, según su relevancia teórica y conocimiento experto del tema.\nSelección de variables por AIC o BIC (Criterio de Información Bayesiano).\n\nRegularización por regresión LASSO (Least Absolute Shrinkage and Selection Operator), que penaliza los coeficientes de las variables menos importantes, favoreciendo modelos más simples y generalizables. Para conocer más sobre este método pueden entrar al siguiente enlace:\n➡️ Stopping stepwise\n\n\nÁrboles de decisión (random forest y gradient boosting), herramientas de aprendizaje automático que permiten manejar automáticamente la selección de variables con menor probabilidad de sobreajuste. Para conocer más sobre este método pueden entrar al siguiente enlace:\n➡️ Decision trees\n\n\nEstos dos últimos métodos escapan al alcance del curso. Sin embargo, invitamos a quienes tengan interés a explorar las múltiples posibilidades que ofrecen, en particular al momento de analizar datasets grandes y con múltiples variables explicativas.\n\nDe Irala, Martínez-González, y Guillén Grima (2001a)\nDe Irala, Martínez-González, y Guillén Grima (2001b)\nGordis (2017)\nKim (2019)\nKleinbaum et al. (1988)\nMacMahon (1972)\n (2025)\nWickham et al. (2019)\nSchloerke et al. (2024)\nLüdecke et al. (2022)\nZeileis y Hothorn (2002)"
  },
  {
    "objectID": "unidad_3/05_anova.html",
    "href": "unidad_3/05_anova.html",
    "title": "Análisis de la varianza (ANOVA)",
    "section": "",
    "text": "El análisis de varianza (ANOVA) es una extensión del modelo lineal general que se utiliza para comparar las medias de una variable dependiente continua (\\(Y\\)) entre diferentes niveles de una variable explicativa categórica (\\(X\\)), que debe tener al menos tres niveles.\nLa hipótesis nula (\\(H_0\\)) del test estadístico establece que las medias de la variable dependiente son iguales en todos los grupos, mientras que la hipótesis alternativa (\\(H_1\\)) plantea que al menos dos medias difieren significativamente:\n\n\\(H_0: \\mu_1 = \\mu_2 = ... = \\mu_i\\)\n\\(H_1\\): al menos una \\(\\mu_i \\not= \\mu_j\\)\n\nPor lo tanto, el ANOVA permite comparar múltiples medias, pero lo hace analizando la variabilidad entre y dentro de los grupos.\nLa variabilidad total se descompone en dos componentes:\n\nIntervarianza (SSB): Variabilidad entre los grupos.\nIntravarianza (SSE): Variabilidad dentro de los grupos.\n\nEl estadístico \\(F\\) del ANOVA, que sigue una distribución F de Fisher-Snedecor, compara estas dos fuentes de variabilidad:\n\\[\nF = \\frac{SSB/(k-1)}{SSE/(n-k)}\n\\]\ndonde:\n\n\\(k\\): el número de grupos.\n\\(n\\): número total de observaciones.\n\nSi se cumple \\(H_0\\), el estadístico \\(F\\) tiende a 1, ya que las varianzas entre y dentro de los grupos son similares. Si las medias difieren significativamente, la intervarianza será mayor que la intravarianza, resultando en valores de \\(F\\) superiores a 1."
  },
  {
    "objectID": "unidad_3/05_anova.html#introducción",
    "href": "unidad_3/05_anova.html#introducción",
    "title": "Análisis de la varianza (ANOVA)",
    "section": "",
    "text": "El análisis de varianza (ANOVA) es una extensión del modelo lineal general que se utiliza para comparar las medias de una variable dependiente continua (\\(Y\\)) entre diferentes niveles de una variable explicativa categórica (\\(X\\)), que debe tener al menos tres niveles.\nLa hipótesis nula (\\(H_0\\)) del test estadístico establece que las medias de la variable dependiente son iguales en todos los grupos, mientras que la hipótesis alternativa (\\(H_1\\)) plantea que al menos dos medias difieren significativamente:\n\n\\(H_0: \\mu_1 = \\mu_2 = ... = \\mu_i\\)\n\\(H_1\\): al menos una \\(\\mu_i \\not= \\mu_j\\)\n\nPor lo tanto, el ANOVA permite comparar múltiples medias, pero lo hace analizando la variabilidad entre y dentro de los grupos.\nLa variabilidad total se descompone en dos componentes:\n\nIntervarianza (SSB): Variabilidad entre los grupos.\nIntravarianza (SSE): Variabilidad dentro de los grupos.\n\nEl estadístico \\(F\\) del ANOVA, que sigue una distribución F de Fisher-Snedecor, compara estas dos fuentes de variabilidad:\n\\[\nF = \\frac{SSB/(k-1)}{SSE/(n-k)}\n\\]\ndonde:\n\n\\(k\\): el número de grupos.\n\\(n\\): número total de observaciones.\n\nSi se cumple \\(H_0\\), el estadístico \\(F\\) tiende a 1, ya que las varianzas entre y dentro de los grupos son similares. Si las medias difieren significativamente, la intervarianza será mayor que la intravarianza, resultando en valores de \\(F\\) superiores a 1."
  },
  {
    "objectID": "unidad_3/05_anova.html#anova-de-un-factor-o-de-una-vía",
    "href": "unidad_3/05_anova.html#anova-de-un-factor-o-de-una-vía",
    "title": "Análisis de la varianza (ANOVA)",
    "section": "ANOVA de un factor o de una vía",
    "text": "ANOVA de un factor o de una vía\nEl ANOVA de una vía puede considerare como una extensión de los t-test independientes para comparar más de dos grupos de un factor. En este contexto, factor se refiere a la variable categórica que define los grupos.\nPara que los resultados del ANOVA sean válidos, deben cumplirse los siguientes supuestos:\n\nAleatoriedad: Las observaciones deben ser aleatorias.\nIndependencia: Las observaciones entre grupos deben ser independientes.\nVariable dependiente: Cuantitativa continua.\nVariable explicativa: Categórica con más de dos niveles.\nNormalidad: La distribución de la variable dependiente dentro de cada grupo debe ser normal.\nHomocedasticidad: Las varianzas de los grupos deben ser homogéneas."
  },
  {
    "objectID": "unidad_3/05_anova.html#ejemplo-práctico-en-r",
    "href": "unidad_3/05_anova.html#ejemplo-práctico-en-r",
    "title": "Análisis de la varianza (ANOVA)",
    "section": "Ejemplo práctico en R",
    "text": "Ejemplo práctico en R\nNuevamente vamos a trabajar con el conjunto de datos “cancer_USA.txt”, que contiene información sobre la tasa de mortalidad por cáncer (por 100.000 habitantes) en distintos condados de la Costa Este de Estados Unidos. Nuestro objetivo será analizar la relación entre la tasa de mortalidad (tasa_mortalidad) y el estado de residencia (estado).\nComenzaremos por cargar los paquetes necesarios:\n\n# tablas salida del modelo\nlibrary(gtsummary)\n\n# chequeo de supuestos\nlibrary(easystats)\nlibrary(nortest)\nlibrary(pwr)\n\n# comparación de medias\nlibrary(emmeans)\n\n# análisis exploratorio\nlibrary(skimr)\nlibrary(dlookr)\n\n# paletas de colores aptas daltonismo\nlibrary(scico)\n\n# manejo de datos\nlibrary(janitor)\nlibrary(tidyverse) \n\nCargamos y exploramos los datos:\n\n# Carga datos\ndatos &lt;- read_csv2(\"datos/cancer_USA.txt\")\n\n# Explora datos\nglimpse(datos)\n\nRows: 213\nColumns: 10\n$ condado            &lt;chr&gt; \"Belknap County\", \"Carroll County\", \"Cheshire Count…\n$ estado             &lt;chr&gt; \"New Hampshire\", \"New Hampshire\", \"New Hampshire\", …\n$ tasa_mortalidad    &lt;dbl&gt; 182.6, 168.8, 162.8, 181.6, 155.0, 163.2, 173.1, 17…\n$ mediana_edad       &lt;dbl&gt; 46.1, 50.3, 42.0, 48.1, 41.9, 40.1, 42.6, 43.5, 37.…\n$ mediana_edad_cat   &lt;chr&gt; \"45+ años\", \"45+ años\", \"36-39 años\", \"45+ años\", \"…\n$ mediana_ingresos   &lt;dbl&gt; 59831, 57556, 56008, 42491, 56353, 71233, 62429, 79…\n$ pct_pobreza        &lt;dbl&gt; 10.0, 10.7, 11.8, 14.9, 11.6, 8.7, 9.5, 6.1, 11.8, …\n$ pct_salud_publica  &lt;dbl&gt; 16.8, 13.4, 12.7, 22.5, 14.0, 12.7, 13.1, 9.0, 12.4…\n$ pct_sec_incompleta &lt;dbl&gt; 15.4, 14.8, 6.1, 13.2, 6.6, 12.9, 10.9, 13.5, 6.1, …\n$ pct_desempleo      &lt;dbl&gt; 5.3, 5.8, 6.1, 6.9, 5.0, 5.9, 5.2, 5.6, 6.5, 5.8, 1…\n\n\nLa base de datos tiene observaciones y variables. La variable dependiente es tasa_mortalidad, mientras que estado es nuestra variable explicativa, categórica con 9 niveles.\nExploremos más en profundidad la estructura de los datos usando la función skim() del paquete skimr:\n\n# Resumen de los datos\nskim(datos)\n\n\nData summary\n\n\nName\ndatos\n\n\nNumber of rows\n213\n\n\nNumber of columns\n10\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n3\n\n\nnumeric\n7\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\ncondado\n0\n1\n10\n19\n0\n174\n0\n\n\nestado\n0\n1\n5\n13\n0\n9\n0\n\n\nmediana_edad_cat\n0\n1\n8\n10\n0\n4\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\ntasa_mortalidad\n0\n1\n172.43\n14.23\n138.8\n162.2\n173.3\n181.6\n216.7\n▂▆▇▃▁\n\n\nmediana_edad\n0\n1\n42.06\n3.67\n30.2\n39.9\n42.2\n44.4\n51.9\n▁▂▇▅▁\n\n\nmediana_ingresos\n0\n1\n55740.31\n13795.54\n33687.0\n45860.0\n52195.0\n60387.0\n103876.0\n▆▇▂▂▁\n\n\npct_pobreza\n0\n1\n13.25\n3.91\n4.7\n10.7\n13.2\n15.4\n31.5\n▃▇▃▁▁\n\n\npct_salud_publica\n0\n1\n17.86\n4.91\n7.1\n14.6\n17.7\n20.7\n38.2\n▃▇▃▁▁\n\n\npct_sec_incompleta\n0\n1\n13.21\n4.76\n3.1\n10.2\n12.7\n15.7\n34.2\n▃▇▃▁▁\n\n\npct_desempleo\n0\n1\n7.50\n1.76\n3.0\n6.4\n7.3\n8.6\n14.0\n▂▇▆▁▁\n\n\n\n\n# Frecuencia por estado\ntabyl(datos$estado)\n\n  datos$estado  n    percent\n   Connecticut  8 0.03755869\n         Maine 16 0.07511737\n Massachusetts 14 0.06572770\n New Hampshire 10 0.04694836\n    New Jersey 21 0.09859155\n      New York 62 0.29107981\n  Pennsylvania 64 0.30046948\n  Rhode Island  4 0.01877934\n       Vermont 14 0.06572770\n\n\nObservamos que la variable estado tiene 9 niveles y ningún valor ausente (n_missing = 0). La variable dependiente tiene valores entre 139 y 217 (casos/100 000 habitantes) y tampoco presenta valores NA.\nLos estados con menos observaciones (Connecticut, New Hampshire y Rhode Island) serán excluidos para evitar sesgos en el análisis. También convertiremos la variable estado a un factor ordenado con el comando fct_relevel() del paquete tidyverse:\n\n# Filtramos estados con pocas observaciones y convertimos a factor\ndatos &lt;- datos |&gt; \n  filter(!estado %in% c(\"New Hampshire\", \"Rhode Island\", \n                        \"Connecticut\")) |&gt; \n  \n  mutate(estado = fct_relevel(estado, \"New York\", after = 0))\n\n# Verificamos con class() y levels()\nclass(datos$estado)\n\n[1] \"factor\"\n\nlevels(datos$estado)\n\n[1] \"New York\"      \"Maine\"         \"Massachusetts\" \"New Jersey\"   \n[5] \"Pennsylvania\"  \"Vermont\"      \n\n\nGraficamos la distribución de la variable dependiente mediante un histograma y un boxplot:\n\n# histograma\ndatos |&gt; \n  ggplot(mapping = aes(x = tasa_mortalidad)) +\n  geom_histogram(binwidth = 10, \n                 color = \"white\", \n                 fill = \"#1E6590\") +\n  theme_minimal() # tema claro\n\n\n\n\n\n\n# boxplot \ndatos |&gt; \n  ggplot(mapping = aes(y = tasa_mortalidad)) +\n  geom_boxplot(fill = \"#B2D680\") +\n  theme_minimal() # tema claro\n\n\n\n\n\n\n\nLa distribución global es simétrica pareciera existir un valor atípico.\nAhora veamos como es la distribución de la tasa de mortalidad por estado.\n\n# Histograma por grupos\ndatos |&gt; \n  ggplot(mapping = aes(x = tasa_mortalidad, fill = estado)) +\n  geom_histogram(binwidth = 10, color = \"white\") +\n  scale_fill_scico_d() + # paleta colorblind-friendly\n  facet_grid(estado ~ .) + # variable estado en filas\n  theme_minimal() # tema claro\n\n\n\n\n\n\n# Boxplot por grupos\ndatos |&gt; \n  ggplot(mapping = aes(x = estado, y = tasa_mortalidad, \n                       fill = estado)) +\n  geom_boxplot() +\n  scale_fill_scico_d() + # paleta colorblind-friendly\n  theme_minimal() + # tema claro\n  theme(axis.text.x = element_text(angle = 90)) # gira etiquetas eje x\n\n\n\n\n\n\n\nObservamos que las distribuciones en algunos grupos parecen simétricas y hay diferencias visuales entre las medianas. Esto plantea la pregunta de si dichas diferencias son estadísticamente significativas.\nAnálisis de supuestos\nSi bien es más recomendable y habitual realizar el análisis de supuestos a partir de los residuales del modelo, podemos chequear que la variable dependiente cumpla con los supuestos de normalidad y homocedasticidad previo a realizar el análisis.\nNormalidad\nUsamos el test de Kolmogorov-Smirnov con corrección de Lilliefors para grupos con más de 50 observaciones con la función lillie.test() del paquete nortest. Como algunos grupos tienen menos de 50 observaciones, también podríamos utilizar el test de Shapiro-Wilk mediante la función shapiro.test() de R base:\n\ndatos |&gt; \n  filter(estado == \"New York\") |&gt; \n  pull(tasa_mortalidad) |&gt;  # con pull() extraemos datos de tasa_mortalidad como vector\n  lillie.test()  # aplicamos test de bondad de ajuste\n\n\n    Lilliefors (Kolmogorov-Smirnov) normality test\n\ndata:  pull(filter(datos, estado == \"New York\"), tasa_mortalidad)\nD = 0.11633, p-value = 0.03637\n\ndatos |&gt; \n  filter(estado == \"Maine\") |&gt; \n  pull(tasa_mortalidad) |&gt;  # con pull() extraemos datos de tasa_mortalidad como vector\n  lillie.test()  # aplicamos test de bondad de ajuste\n\n\n    Lilliefors (Kolmogorov-Smirnov) normality test\n\ndata:  pull(filter(datos, estado == \"Maine\"), tasa_mortalidad)\nD = 0.18801, p-value = 0.1364\n\ndatos |&gt; \n  filter(estado == \"Massachusetts\") |&gt; \n  pull(tasa_mortalidad) |&gt;  # con pull() extraemos datos de tasa_mortalidad como vector\n  lillie.test()  # aplicamos test de bondad de ajuste\n\n\n    Lilliefors (Kolmogorov-Smirnov) normality test\n\ndata:  pull(filter(datos, estado == \"Massachusetts\"), tasa_mortalidad)\nD = 0.1591, p-value = 0.4377\n\ndatos |&gt; \n  filter(estado == \"New Jersey\") |&gt; \n  pull(tasa_mortalidad) |&gt;  # con pull() extraemos datos de tasa_mortalidad como vector\n  lillie.test()  # aplicamos test de bondad de ajuste\n\n\n    Lilliefors (Kolmogorov-Smirnov) normality test\n\ndata:  pull(filter(datos, estado == \"New Jersey\"), tasa_mortalidad)\nD = 0.14095, p-value = 0.3387\n\ndatos |&gt; \n  filter(estado == \"Pennsylvania\") |&gt; \n  pull(tasa_mortalidad) |&gt;  # con pull() extraemos datos de tasa_mortalidad como vector\n  lillie.test()  # aplicamos test de bondad de ajuste\n\n\n    Lilliefors (Kolmogorov-Smirnov) normality test\n\ndata:  pull(filter(datos, estado == \"Pennsylvania\"), tasa_mortalidad)\nD = 0.066685, p-value = 0.6839\n\ndatos |&gt; \n  filter(estado == \"Vermont\") |&gt; \n  pull(tasa_mortalidad) |&gt;  # con pull() extraemos datos de tasa_mortalidad como vector\n  lillie.test()  # aplicamos test de bondad de ajuste\n\n\n    Lilliefors (Kolmogorov-Smirnov) normality test\n\ndata:  pull(filter(datos, estado == \"Vermont\"), tasa_mortalidad)\nD = 0.14856, p-value = 0.5497\n\n\nPodemos graficar el comportamiento de la variable dependiente en cada grupo mediante Q-Q plots usando el paquete ggplot2:\n\ndatos |&gt; \n  ggplot() +\n  geom_qq(mapping = aes(sample = tasa_mortalidad)) + \n  geom_qq_line(mapping = aes(sample = tasa_mortalidad)) +\n  facet_wrap(~ estado) +\n  theme_minimal()\n\n\n\n\n\n\n\nHomocedasticidad\nVerificamos la homogeneidad de varianzas por grupo con el test de Bartlett:\n\nbartlett.test(tasa_mortalidad ~ estado, # utiliza sintaxis fórmula\n              data = datos) \n\n\n    Bartlett test of homogeneity of variances\n\ndata:  tasa_mortalidad by estado\nBartlett's K-squared = 4.8185, df = 5, p-value = 0.4384\n\n\nLa variable respuesta cumple con el supuesto de homocedasticidad (\\(p = 0.438\\)).\nAnálisis de varianza\nSi bien podemos realizar un test ANOVA mediante la función aov() de R base, en el contexto de este curso utilizaremos la función lm() del paquete stats que usamos anteriormente para ajustar el modelo de regresión lineal simple:\n\nlm(tasa_mortalidad ~ estado, data = datos)\n\n\nCall:\nlm(formula = tasa_mortalidad ~ estado, data = datos)\n\nCoefficients:\n        (Intercept)          estadoMaine  estadoMassachusetts  \n            171.640               11.510               -6.840  \n   estadoNew Jersey   estadoPennsylvania        estadoVermont  \n             -3.007                3.419                4.631  \n\n\nSiempre que trabajemos con modelos, conviene que asignemos los resultados a objetos que luego nos servirán para aplicar otras funciones. Como nuestra variable explicativa es categórica y nuestro principal interés es conocer si existen diferencias significativas entre grupos, no analizaremos los coeficientes del modelo con summary(), sino la significancia del test F mediante la función anova().\n\n# ANOVA\nmodelo &lt;- lm(tasa_mortalidad ~ estado, data = datos)\n\n# Salida del modelo\nanova(modelo)\n\nAnalysis of Variance Table\n\nResponse: tasa_mortalidad\n           Df Sum Sq Mean Sq F value   Pr(&gt;F)   \nestado      5   3513  702.67  3.7103 0.003179 **\nResiduals 185  35036  189.38                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nLos resultados del test F nos muestran un \\(p = 0,003\\), sugiriendo que al menos dos grupos son diferentes entre sí.\nComparaciones múltiples\nUna vez que comprobamos que existen diferencias significativas entre grupos, nos interesa saber cuáles grupos son diferentes entre sí. Para ello, existen distintos algoritmos de comparaciones múltiples con sus respectivas correcciones, como el test de Diferencia Honestamente Significativa de Tukey, también llamado Tukey HSD o test de Tukey. Esta prueba se aplica para grupos equilibrados (mismo tamaño) y varianzas similares (homocedásticas). Es una prueba conservadora, dado que mantiene bajo el error de tipo I, sacrificando la capacidad de detectar diferencias existentes.\nSi las varianzas son homocedásticas pero los grupos difieren en tamaño, podemos usar el test de Tukey si tenemos que comparar entre varios grupos, o la corrección de Bonferroni para grupos más reducidos.\n\n\n\n\nCódigo\nRango\n\n\n\n***\n0 a 0,001\n\n\n**\n0,001 a 0,01\n\n\n*\n0,01 a 0,05\n\n\n.\n0,05 a 0,1\n\n\n\n0,1 a 1\n\n\n\n\n\nEl paquete emmeans (Lenth 2025) es una herramienta muy versátil y poderosa para realizar comparaciones múltiples. Comenzaremos utilizando la función emmeans() con el argumento specs = \"estado\" para crear un objeto que contenga las medias marginales por grupo:\n\ncomp &lt;- emmeans(modelo, specs = \"estado\")\n\nPara realizar las comparaciones múltiples mediante test de Tukey usamos el comando contrast() con el argumento method = \"pairwise\":\n\ncontrast(comp, method = \"pairwise\")\n\n contrast                     estimate   SE  df t.ratio p.value\n New York - Maine               -11.51 3.86 185  -2.983  0.0376\n New York - Massachusetts         6.84 4.07 185   1.680  0.5469\n New York - New Jersey            3.01 3.47 185   0.865  0.9542\n New York - Pennsylvania         -3.42 2.45 185  -1.394  0.7305\n New York - Vermont              -4.63 4.07 185  -1.137  0.8652\n Maine - Massachusetts           18.35 5.04 185   3.644  0.0046\n Maine - New Jersey              14.52 4.57 185   3.179  0.0211\n Maine - Pennsylvania             8.09 3.85 185   2.103  0.2901\n Maine - Vermont                  6.88 5.04 185   1.366  0.7474\n Massachusetts - New Jersey      -3.83 4.75 185  -0.807  0.9659\n Massachusetts - Pennsylvania   -10.26 4.06 185  -2.527  0.1217\n Massachusetts - Vermont        -11.47 5.20 185  -2.205  0.2402\n New Jersey - Pennsylvania       -6.43 3.46 185  -1.857  0.4322\n New Jersey - Vermont            -7.64 4.75 185  -1.609  0.5939\n Pennsylvania - Vermont          -1.21 4.06 185  -0.299  0.9997\n\nP value adjustment: tukey method for comparing a family of 6 estimates \n\n\nLos resultados del test de Tukey nos muestran diferencias significativas en la tasa de mortalidad por cáncer en el estado de Maine respecto de New York, Massachusetts y New Jersey.\nPara mayor claridad, podemos mostrar las comparaciones múltiples mediante un gráfico:\n\nplot(comp) +\n  theme_minimal() # tema claro\n\n\n\n\n\n\n\nEl rombo del medio representa la media marginal para cada grupo y el rectángulo azul, su intervalo de confianza, aquellos grupos en los que no se superponen los intervalos de confianza son estadísticamente diferentes entre sí.\nBondad de ajuste\nEl tamaño de efecto comúnmente utilizado para el caso de ANOVA es \\(\\eta^2\\) (eta-cuadrado), que se calcula como el cociente de la suma de cuadrados del efecto sobre la suma de cuadrados total.\n\nanova(modelo)\n\nAnalysis of Variance Table\n\nResponse: tasa_mortalidad\n           Df Sum Sq Mean Sq F value   Pr(&gt;F)   \nestado      5   3513  702.67  3.7103 0.003179 **\nResiduals 185  35036  189.38                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# calculo eta-cuadrado\neta2 &lt;- 3513/(3513 + 35036)\n\neta2\n\n[1] 0.09113077\n\n\nPodemos llegar al mismo valor usando la función r2() del paquete performance:\n\nr2(modelo)\n\n# R2 for Linear Regression\n       R2: 0.091\n  adj. R2: 0.067\n\n\nPotencia\nLos test de potencia permiten determinar la probabilidad de encontrar diferencias significativas entre las medias para un determinado nivel de significancia indicando el tamaño de los grupos.\nEn base al eta-cuadrado obtenido anteriormente, calculamos el tamaño de efecto convencional mediante la función cohen.ES() del paquete pwr (Champely 2020):\n\ncohen.ES(test = \"anov\", size = \"small\")\n\n\n     Conventional effect size from Cohen (1982) \n\n           test = anov\n           size = small\n    effect.size = 0.1\n\n\nLa función pwr.anova.test() del paquete pwr nos permite calcular la potencia en base al número de grupos, número de observaciones por grupo, tamaño de efecto y nivel de significancia (habitualmente 0,05).\nPara grupos de igual tamaño usamos el siguiente código:\n\npwr.anova.test(k = k, n = n, f = eta_cuadrado, sig.level = 0.05)\n\nEn nuestro ejemplo, donde los tamaños de grupo son diferentes, primero debemos calcular el tamaño de grupo efectivo:\n\n# Número de grupos\nk &lt;- nlevels(datos$estado)\n  \n# Número de observaciones por grupo\nn &lt;- tabyl(datos$estado)$n\n\n# Tamaño de grupo efectivo\nneff &lt;- sum(n)^2 / sum(n^2)\n\nCalculamos la potencia:\n\npwr.anova.test(k = k, n = neff, f = eta2, sig.level = 0.05)\n\n\n     Balanced one-way analysis of variance power calculation \n\n              k = 6\n              n = 4.040425\n              f = 0.09113077\n      sig.level = 0.05\n          power = 0.05670154\n\nNOTE: n is number in each group\n\n\nAnálisis de residuales\nPara poder dar por válidos los resultados del ANOVA es necesario verificar que se satisfacen las condiciones analizando los residuos del modelo. Para ello, usaremos el mismo procedimiento que vimos para la regresión lineal simple.\nMétodo gráfico con paquete performance:\n\ncheck_model(modelo)\n\n\n\n\n\n\n\nChequeo normalidad de residuales mediante test de Lilliefors:\n\nlillie.test(modelo$residuals)\n\n\n    Lilliefors (Kolmogorov-Smirnov) normality test\n\ndata:  modelo$residuals\nD = 0.046833, p-value = 0.388\n\n\nChequeo homocedasticidad (test de Breusch-Pagan):\n\ncheck_heteroscedasticity(modelo)\n\nOK: Error variance appears to be homoscedastic (p = 0.597).\n\n\nChequeo presencia de outliers (distancia de Cook):\n\ncheck_outliers(modelo)\n\nOK: No outliers detected.\n- Based on the following method and threshold: cook (0.5).\n- For variable: (Whole model)"
  },
  {
    "objectID": "unidad_3/05_anova.html#conclusión",
    "href": "unidad_3/05_anova.html#conclusión",
    "title": "Análisis de la varianza (ANOVA)",
    "section": "Conclusión",
    "text": "Conclusión\nSe ha realizado un ANOVA de una vía para comparar la tasa de mortalidad por cáncer en seis estados de Estados Unidos. Se encontraron diferencias estadísticamente significativas entre las medias de los grupos (\\(F = 3,71, p = 0.003\\)). El eta-cuadrado fue de 0.091, lo que indica que el estado de residencia explica un 9.1% de la varianza total en los valores de la tasa de mortalidad por cáncer. La potencia de la prueba estadística fue del 5.7%.\nSe realizó la prueba post hoc de Tukey HSD y se encontró que media de mortalidad para el estado de Maine fue estadísticamente superiror respecto de los estados de Massachusetts, New York y New Jersey (\\(p &lt; 0.05\\)).\n\nAgresti (2015)\nTriola (2018)\nWickham et al. (2019)\nSjoberg et al. (2021)\nLüdecke et al. (2022)\nGross y Ligges (2015)\nFirke (2024)\n(Ryu 2024; Lenth 2025; Firke 2024; Waring et al. 2022)"
  },
  {
    "objectID": "unidad_3/03_covarianza_correlacion.html",
    "href": "unidad_3/03_covarianza_correlacion.html",
    "title": "Covarianza y correlación",
    "section": "",
    "text": "Una parte fundamental del análisis epidemiológico es detectar relación entre dos variables numéricas. Este análisis permite identificar patrones, tendencias y posibles asociaciones. Por ejemplo:\n\nPresión sanguínea y edad\nEstatura y peso\nConcentración de un medicamento y frecuencia cardíaca\n\nEstablecer estas relaciones facilita la identificación de factores de riesgo y/o la planificación de intervenciones. Para ello, se emplean dos herramientas estadísticas clave: la covarianza y la correlación.\n\nCovarianza: Mide si dos variables aumentan o disminuyen simultáneamente.\nCorrelación: Evalúa la dirección e intensidad de la relación, lo que permite interpretar y comparar con mayor claridad."
  },
  {
    "objectID": "unidad_3/03_covarianza_correlacion.html#introducción",
    "href": "unidad_3/03_covarianza_correlacion.html#introducción",
    "title": "Covarianza y correlación",
    "section": "",
    "text": "Una parte fundamental del análisis epidemiológico es detectar relación entre dos variables numéricas. Este análisis permite identificar patrones, tendencias y posibles asociaciones. Por ejemplo:\n\nPresión sanguínea y edad\nEstatura y peso\nConcentración de un medicamento y frecuencia cardíaca\n\nEstablecer estas relaciones facilita la identificación de factores de riesgo y/o la planificación de intervenciones. Para ello, se emplean dos herramientas estadísticas clave: la covarianza y la correlación.\n\nCovarianza: Mide si dos variables aumentan o disminuyen simultáneamente.\nCorrelación: Evalúa la dirección e intensidad de la relación, lo que permite interpretar y comparar con mayor claridad."
  },
  {
    "objectID": "unidad_3/03_covarianza_correlacion.html#covarianza",
    "href": "unidad_3/03_covarianza_correlacion.html#covarianza",
    "title": "Covarianza y correlación",
    "section": "Covarianza",
    "text": "Covarianza\nLa covarianza es una medida estadística que indica el grado de variabilidad conjunta de dos variables cuantitativas \\(X\\) e \\(Y\\). Matemáticamente se define como:\n\\[\nS_{XY} = \\frac{1}{n} \\sum_{i=1}^{n} (x_i - \\bar{x})(y_i - \\bar{y})\n\\]\ndonde:\n\n\\(x_i\\) e \\(y_i\\) son los valores individuales de cada variable.\n\\(\\bar{x_i}\\) e \\(\\bar{y_i}\\) son las medias de las variables \\(x\\) e \\(y\\).\n\\(n\\) es el número de pares de datos.\n\nRepresentación gráfica\nLa covarianza suele representarse gráficamente usando diagramas de dispersión (scatterplots), que muestran la dirección de la asociación y permiten detectar valores extremos (outliers). Cuando examinamos un diagrama de dispersión, debemos observar si existe un patrón general en los puntos graficados y señalar su dirección. Es decir, mientras una variable se incrementa, ¿la otra parece aumentar o disminuir? Además, debemos observar si hay datos distantes, que son puntos que se ubican muy lejos de todos los demás.\nConsideremos la nube de puntos formadas por las \\(n\\) parejas de datos (\\(x_i\\), \\(y_i\\)). El centro de gravedad de esta nube de puntos es (\\(\\bar{x}\\) , \\(\\bar{y}\\)). Trasladamos los ejes XY al nuevo centro de coordenadas. Los puntos que se encuentran en el primer y tercer cuadrante contribuyen positivamente al valor de \\(S_{XY}\\), y los que se encuentran en el segundo y el cuarto lo hacen negativamente.\nCovarianza positiva\nEl siguiente gráfico muestra una relación positiva entre minutos de actividad física semanal (\\(X\\)) y capacidad aeróbica (\\(Y\\)). Podemos observar que, cuando crece \\(X\\) también crece \\(Y\\) y casi todos los puntos pertenecen a los cuadrantes primero y tercero (\\(S_{XY}&gt;0\\)).\n\n\n\n\n\n\n\n\nCovarianza negativa\nEn este gráfico, a medida que aumenta el consumo de alcohol semanal (\\(X\\) ), disminuyen los niveles de enzimas hepáticas (\\(Y\\)). Casi todos los puntos pertenecen a los cuadrantes segundo y cuarto (\\(S_{XY}&lt;0\\)).\n\n\n\n\n\n\n\n\nCovarianza cercana a cero\nEn el diagrama de dispersión se puede observar que los cambios en las horas de sueño (\\(X\\)) no tienen relación con el nivel de colesterol en sangre (\\(Y\\)). No se observa un patrón, sino que los puntos se reparten de modo más o menos homogéneo entre los cuadrantes (\\(S_{XY}\\simeq 0\\)).\n\n\n\n\n\n\n\n\nLimitaciones de la covarianza\nLa covarianza está afectada por las unidades en las que se miden las variables, lo que puede dificultar la interpretación de su magnitud. Para resolver este problema, es necesario utilizar una medida que no esté afectada por las unidades de medida de las variables: la correlación."
  },
  {
    "objectID": "unidad_3/03_covarianza_correlacion.html#correlación",
    "href": "unidad_3/03_covarianza_correlacion.html#correlación",
    "title": "Covarianza y correlación",
    "section": "Correlación",
    "text": "Correlación\nLa correlación trata de establecer la relación o dependencia que existe entre las dos variables que intervienen en una distribución bidimensional. Es decir, determinar si los cambios en una de las variables influyen en los cambios de la otra. En caso de que suceda, diremos que las variables están correlacionadas o que hay correlación entre ellas.\nLa correlación, como cuantificación del grado de relación que hay entre dos variables, es un valor entre -1 y +1, pasando, por el cero. Hay, por lo tanto, correlaciones positivas y negativas. El signo es, pues, el primer elemento básico a tener en cuenta.\n\nCorrelación positiva: existe relación directa; ambas variables aumentan o disminuyen simultáneamente. El coeficiente de correlación tiene valores entre 0 y 1.\nCorrelación negativa: existe relación inversa; una variable aumenta mientras la otra disminuye. El coeficiente de correlación tiene valores entre -1 y 0.\nCorrelación cercana a cero: no hay relación lineal, aunque podría existir una relación no lineal.\n\nLo segundo a tener en cuenta en la correlación es la magnitud. Y esto lo marca el valor absoluto de la correlación. En la magnitud se valora la correlación sin el signo, valorando la magnitud del número puro. Esto significa que cuanto más cerca estemos de los extremos del intervalo de valores posibles: -1 y +1, más correlación tenemos.\nLa correlación más usada para variables cuantitativas es la correlación de Pearson. Es especialmente apropiada cuando la distribución de las variables es la normal. Si no se cumple la normalidad o si las variables son ordinales es más apropiado usar la correlación de Spearman o la correlación de Kendall.\nCorrelación de Pearson\nMide la relación lineal entre dos variables, eliminando la influencia de las unidades de medida. Es una medida adimensional, obtenida al estandarizar la covarianza entre dos variables \\(X\\) e \\(Y\\):\n\\[r = \\frac{Cov_{XY}}{S_xS_y} \\]\ndonde:\n\n\\(Cov_{XY}\\) es la covarianza entre las variables \\(X\\) e \\(Y\\).\n\\(S_x\\) y \\(S_y\\) son las desviaciones estándar de las variables \\(X\\) e \\(Y\\).\n\nEl coeficiente de correlación \\(r\\) entonces se interpreta como:\n\n\\(0 &lt; r \\leq 1\\): correlación positiva; ambas variables aumentan o disminuyen simultáneamente.\n\\(-1 \\leq r &gt; 0\\): correlación negativa; una variable aumenta mientras la otra disminuye.\n\\(r ≈ 0\\): no hay relación lineal, aunque podría existir una relación no lineal.\n\n¿A partir de qué valores de \\(r\\) se considera que hay “buena correlación”? La respuesta no es simple. Hay que tener en cuenta la presencia de observaciones anómalas y si la varianza se mantiene homogénea. En reglas generales se acepta que:\n\n\\(r ≈ ±1\\): correlación perfecta.\n\\(r \\geq ±0.7\\): correlación fuerte.\n\\(±0.4 \\leq r &lt; ±0.7\\): correlación moderada.\n\\(r \\leq ±0.4\\): correlación débil.\n\nCorrelación positiva perfecta entre \\(X\\) e \\(Y\\)\n\n\n\n\n\n\n\n\n\nCorrelación positiva fuerte entre \\(X\\) e \\(Y\\)\n\n\n\n\n\n\n\n\n\nCorrelación positiva moderada entre \\(X\\) e \\(Y\\)\n\n\n\n\n\n\n\n\n\nCorrelación negativa perfecta entre \\(X\\) e \\(Y\\)\n\n\n\n\n\n\n\n\n\nCorrelación negativa fuerte entre \\(X\\) e \\(Y\\)\n\n\n\n\n\n\n\n\n\nCorrelación negativa moderada entre \\(X\\) e \\(Y\\)\n\n\n\n\n\n\n\n\n\n\n\\(X\\) e \\(Y\\) no correlacionadas\n\n\n\n\n\n\n\n\nRelación no lineal entre \\(X\\) e \\(Y\\)\n\n\n\n\n\n\n\n\n\nCorrelogramas\nLos correlogramas son gráficos que muestra la relación entre cada par de variables numéricas en un conjunto de datos. Se utiliza para analizar la correlación entre variables, identificar patrones y controlar la aleatoriedad de los datos. Existen diversos paquetes en R que permiten generar correlogramas, como correlation, que forma parte del ecosistema de paquetes easystats(Lüdecke et al. 2022), o GGally(Schloerke et al. 2024) que usa como base el paquete ggplot2.\nCorrelación de Spearman\nMide la correlación entre dos variables basada en los rangos (orden) de los valores. Se utiliza cuando los datos no presentan una relación lineal o tienen una relación monótona. Matemáticamente se expresa como:\n\\[\n\\rho = \\frac{1 - 6\\sum d_i^2}{n(n^2-1)}\n\\]\ndonde:\n\n\\(d_i\\) es la diferencia en los rangos de cada par de observaciones.\n\\(n\\) es el número de observaciones.\nCorrelación de Kendall\nMide la relación ordinal entre dos variables numéricas en base a la concordancia y discordancia entre pares. Se utiliza cuando hay datos con valores repetidos o la muestra es pequeña. Es más robusta frente a datos con valores atípicos en comparación con Spearman. Matemáticamente se expresa como:\n\\[\n\\tau = \\frac{n_c - n_d}{n(n-1)/2}\n\\]\ndonde:\n\n\n\\(n_c\\) y \\(n_d\\) son los pares concordantes o discordantes."
  },
  {
    "objectID": "unidad_3/03_covarianza_correlacion.html#ejemplo-práctico-en-r",
    "href": "unidad_3/03_covarianza_correlacion.html#ejemplo-práctico-en-r",
    "title": "Covarianza y correlación",
    "section": "Ejemplo práctico en R",
    "text": "Ejemplo práctico en R\nTomemos como ejemplo la base cancer_USA.txt que contiene información sobre la tasa de mortalidad por cáncer cada 100.000 habitantes de distintos condados de la Costa Este de Estados Unidos.\nComenzaremos por cargar los paquetes necesarios para el análisis:\n\n# Gráficos de correlación\nlibrary(easystats)\n\nlibrary(GGally)\n\n# Manejo de datos\nlibrary(tidyverse)\n\nCargamos los datos y exploramos su estructura:\n\n# Carga de datos\ndatos &lt;- read_csv2(\"datos/cancer_USA.txt\")\n\n# Explora datos\nglimpse(datos)\n\nRows: 213\nColumns: 10\n$ condado            &lt;chr&gt; \"Belknap County\", \"Carroll County\", \"Cheshire Count…\n$ estado             &lt;chr&gt; \"New Hampshire\", \"New Hampshire\", \"New Hampshire\", …\n$ tasa_mortalidad    &lt;dbl&gt; 182.6, 168.8, 162.8, 181.6, 155.0, 163.2, 173.1, 17…\n$ mediana_edad       &lt;dbl&gt; 46.1, 50.3, 42.0, 48.1, 41.9, 40.1, 42.6, 43.5, 37.…\n$ mediana_edad_cat   &lt;chr&gt; \"45+ años\", \"45+ años\", \"36-39 años\", \"45+ años\", \"…\n$ mediana_ingresos   &lt;dbl&gt; 59831, 57556, 56008, 42491, 56353, 71233, 62429, 79…\n$ pct_pobreza        &lt;dbl&gt; 10.0, 10.7, 11.8, 14.9, 11.6, 8.7, 9.5, 6.1, 11.8, …\n$ pct_salud_publica  &lt;dbl&gt; 16.8, 13.4, 12.7, 22.5, 14.0, 12.7, 13.1, 9.0, 12.4…\n$ pct_sec_incompleta &lt;dbl&gt; 15.4, 14.8, 6.1, 13.2, 6.6, 12.9, 10.9, 13.5, 6.1, …\n$ pct_desempleo      &lt;dbl&gt; 5.3, 5.8, 6.1, 6.9, 5.0, 5.9, 5.2, 5.6, 6.5, 5.8, 1…\n\n\nLa base de datos tiene 213 observaciones y 10 variables. La variable dependiente es tasa_mortalidad y hay 3 variables independientes categóricas y 6 variables independientes numéricas o continuas:\n\ntasa_mortalidad: Tasa de mortalidad por cáncer a nivel de condado (datos agregados).\npct_salud_publica: Porcentaje de personas con cobertura exclusiva de salud pública en el condado.\npct_pobreza: Porcentaje de personas por debajo de la línea de pobreza en el condado.\npct_sec_incompleta: Porcentaje de personas con estudios secundarios incompletos en el condado.\nmediana_edad: Mediana de la edad de fallecimiento por cáncer en el condado.\nmediana_edad_cat: Versión categorizada de la mediana de edad al fallecer por cáncer en el condado.\ncondado: Condado de residencia.\nestado: Estado de residencia.\n\nLa función cor() estima la correlación entre dos variables. El método predeterminado devuelve la correlación de Pearson y puede modificarse el argumento method para obtener la correlación de Kendall o Spearman:\n\ncor(datos$tasa_mortalidad, datos$pct_pobreza,\n    method = \"pearson\")\n\n[1] 0.3301532\n\n\nLa función cor.test() determina si la prueba de correlación de Pearson calculada es significativa mediante el estadístico \\(t\\) de Student:\n\ncor.test(datos$tasa_mortalidad, datos$pct_pobreza)\n\n\n    Pearson's product-moment correlation\n\ndata:  datos$tasa_mortalidad and datos$pct_pobreza\nt = 5.0806, df = 211, p-value = 8.266e-07\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.2048117 0.4448414\nsample estimates:\n      cor \n0.3301532 \n\n\nLos resultados de la función son:\n\nEl valor del estadístico \\(t\\)\n\nEl valor de \\(p\\) para el estadístico\nEl valor de la correlación de Pearson\nLos intervalos de confianza para la correlación\n\nLos argumentos predeterminados para la función cor.test() son:\n\nalternative = \"two.sided\" - indica la hipotesis alternativa, también puede ser \"greater\" para asociación positiva, \"less\" para asociación negativa.\nconf.level = 0.95 - determina el nivel de confianza (se puede modificar).\nmethod = \"pearson\" - especifíca el tipo de test de correlación. También permite \"kendall\" o \"spearman\".\n\nGráficos de correlación\nPodemos generar diagramas de dispersión para cada par de variables numéricas usando la función ggpairs() del paquete GGally (Schloerke et al. 2024):\n\ndatos |&gt; \n  # selecciono soloamente las variables numéricas\n  select(where(is.numeric)) |&gt; \n  \n  # diagramas de dispersión\n  ggpairs()\n\n\n\n\n\n\n\nPodemos generar los correlogramas con el paquete correlation usando el siguiente código:\n\ndatos |&gt; \n  correlation() |&gt; \n  summary() |&gt; \n  plot()\n\n\n\n\n\n\n\nEl comando ggcorr() del paquete GGally también nos permite generar correlogramas:\n\ndatos |&gt; \n  # selecciono soloamente las variables numéricas\n  select(where(is.numeric)) |&gt; \n  \n  # correlograma\n  ggcorr(label = T)\n\n\n\n\n\n\n\nPodemos hacerlos más atractivos visualmente modificando los argumentos nbreaks, geom, size, max_size y palette:\n\ndatos |&gt; \n  # selecciono soloamente las variables numéricas\n  select(where(is.numeric)) |&gt; \n  \n  # correlograma\n  ggcorr(label = T, # coeficientes\n         size = 3, # tamaño de texto\n         geom = \"circle\", # forma del gráfico\n         max_size = 15, # tamaño máximo de las formas\n         nbreaks = 6, # número de categorías de color\n         palette = \"PRGn\") # color de las formas\n\n\n\n\n\n\n\nObservamos que la variable dependiente (tasa_mortalidad) tiene correlación negativa moderada con mediana_ingresos, también que existe una correlación negativa fuerte entre pct_pobreza y mediana_ingresos y moderada con pct_salud_publica. Las correlaciones entre las demás variables son débiles o nulas.\nPara visualizar el correlograma con el coeficiente de Spearman, añadimos el argumento method = c(\"pairwise\", \"spearman\") al código del ejemplo anterior:\n\ndatos |&gt; \n  # selecciono soloamente las variables numéricas\n  select(where(is.numeric)) |&gt; \n  \n  # correlograma\n  ggcorr(label = T, # coeficientes\n         size = 3, # tamaño de texto\n         geom = \"circle\", # forma del gráfico\n         max_size = 15, # tamaño máximo de las formas\n         nbreaks = 6, # número de categorías de color\n         palette = \"PRGn\", # color de las formas\n         method = c(\"pairwise\", \"spearman\")) # coeficiente de Spearman\n\n\n\n\n\n\n\nDe igual modo, si queremos aplicar el coeficiente de Spearman, añadimos el argumento method = c(\"pairwise\", \"kendall\"):\n\ndatos |&gt; \n  # selecciono soloamente las variables numéricas\n  select(where(is.numeric)) |&gt; \n  \n  # correlograma\n  ggcorr(label = T, # coeficientes\n         size = 3, # tamaño de texto\n         geom = \"circle\", # forma del gráfico\n         max_size = 15, # tamaño máximo de las formas\n         nbreaks = 6, # número de categorías de color\n         palette = \"PRGn\", # color de las formas\n         method = c(\"pairwise\", \"kendall\")) # coeficiente de Kendall\n\n\n\n\n\n\n\nPara explicar la forma de esta correlación e incluso predecir los valores que puede alcanzar una variable dependiente numérica en función de una variable independiente también numérica podemos utilizar la regresión lineal simple.\n\nTriola (2018)\nAgresti (2015)\n(2025; Wickham et al. 2019)"
  },
  {
    "objectID": "unidad_3/01_est_ecologicos.html",
    "href": "unidad_3/01_est_ecologicos.html",
    "title": "Estudios ecológicos",
    "section": "",
    "text": "Por definición, un estudio ecológico es aquel en el cual las unidades de análisis son poblaciones o agrupamientos de individuos, no los individuos propiamente dichos. Estos conglomerados pueden estar definidos en un contexto espacial (ciudad, provincia, país, región, etc), institucional (hospitales, escuelas, etc), social, temporal, etc. La característica principal de este tipo de diseños es que se cuenta con información sobre la exposición y el evento para el conglomerado en su totalidad, pero se desconoce la información a escala individual.\nEn este tipo de estudios se asigna la misma exposición (generalmente una exposición promedio) a todo el conglomerado. Algo similar ocurre con el evento considerado, tenemos un número de eventos correspondiente al conglomerado, pero no se sabe si los individuos expuestos son los que tienen el evento.\nRepasemos las principales ventajas y desventajas de este diseño:\n\n\n\n\nVentajas\nDesventajas\n\n\n\nSe pueden estudiar grandes grupos poblacionales\nNo se tiene información del individuo,por lo que no se puede ajustar por diferencias a nivel individual\n\n\nRelativamente fáciles de realizar\nNo se puede saber quién sí está expuesto o quién sí desarrolló el evento de interés\n\n\nAumenta el poder estadístico\nNo se tiene información sobre factores de confusión y no se puede corregir por esto\n\n\nAumenta la variabilidad en exposición\n\n\n\nSe puede utilizar información de estadísticas vitales\n\n\n\n\n\n\nRecordemos también que puede incurrirse en la llamada falacia ecológica:\n\n\n\n\n\n\nFalacia ecológica (ecological fallacy)\n\n\n\nSesgo que puede aparecer al observar una asociación a partir de un estudio ecológico pero que no representa una asociación causal a nivel individual.\n\n\nEn un estudio ecológico, existen distintos niveles de medición de las variables de grupo:\n\nMedidas agregadas: Se trata de resumir las observaciones sobre individuos en cada grupo, expresándolas como medias o proporciones (ejemplo la proporción de usuarios de cinturón de seguridad).\nMediciones ambientales: Son características generalmente del lugar en el que los miembros del grupo viven o trabajan (ejemplo: niveles de contaminación ambiental). Cada medición ambiental tiene su análogo a nivel individual y la exposición individual puede variar dentro del grupo.\nMediciones globales: Son características de grupos, organizaciones o áreas para las que no existe una analogía con el nivel individual (ejemplo: densidad de población).\n\nEsta distinción es importante, porque para las dos primeras siempre existe el recurso de la medición individual, en tanto que en las mediciones globales, el diseño ecológico es la única alternativa viable."
  },
  {
    "objectID": "unidad_3/01_est_ecologicos.html#introducción",
    "href": "unidad_3/01_est_ecologicos.html#introducción",
    "title": "Estudios ecológicos",
    "section": "",
    "text": "Por definición, un estudio ecológico es aquel en el cual las unidades de análisis son poblaciones o agrupamientos de individuos, no los individuos propiamente dichos. Estos conglomerados pueden estar definidos en un contexto espacial (ciudad, provincia, país, región, etc), institucional (hospitales, escuelas, etc), social, temporal, etc. La característica principal de este tipo de diseños es que se cuenta con información sobre la exposición y el evento para el conglomerado en su totalidad, pero se desconoce la información a escala individual.\nEn este tipo de estudios se asigna la misma exposición (generalmente una exposición promedio) a todo el conglomerado. Algo similar ocurre con el evento considerado, tenemos un número de eventos correspondiente al conglomerado, pero no se sabe si los individuos expuestos son los que tienen el evento.\nRepasemos las principales ventajas y desventajas de este diseño:\n\n\n\n\nVentajas\nDesventajas\n\n\n\nSe pueden estudiar grandes grupos poblacionales\nNo se tiene información del individuo,por lo que no se puede ajustar por diferencias a nivel individual\n\n\nRelativamente fáciles de realizar\nNo se puede saber quién sí está expuesto o quién sí desarrolló el evento de interés\n\n\nAumenta el poder estadístico\nNo se tiene información sobre factores de confusión y no se puede corregir por esto\n\n\nAumenta la variabilidad en exposición\n\n\n\nSe puede utilizar información de estadísticas vitales\n\n\n\n\n\n\nRecordemos también que puede incurrirse en la llamada falacia ecológica:\n\n\n\n\n\n\nFalacia ecológica (ecological fallacy)\n\n\n\nSesgo que puede aparecer al observar una asociación a partir de un estudio ecológico pero que no representa una asociación causal a nivel individual.\n\n\nEn un estudio ecológico, existen distintos niveles de medición de las variables de grupo:\n\nMedidas agregadas: Se trata de resumir las observaciones sobre individuos en cada grupo, expresándolas como medias o proporciones (ejemplo la proporción de usuarios de cinturón de seguridad).\nMediciones ambientales: Son características generalmente del lugar en el que los miembros del grupo viven o trabajan (ejemplo: niveles de contaminación ambiental). Cada medición ambiental tiene su análogo a nivel individual y la exposición individual puede variar dentro del grupo.\nMediciones globales: Son características de grupos, organizaciones o áreas para las que no existe una analogía con el nivel individual (ejemplo: densidad de población).\n\nEsta distinción es importante, porque para las dos primeras siempre existe el recurso de la medición individual, en tanto que en las mediciones globales, el diseño ecológico es la única alternativa viable."
  },
  {
    "objectID": "unidad_3/01_est_ecologicos.html#clasificación-de-estudios-ecológicos",
    "href": "unidad_3/01_est_ecologicos.html#clasificación-de-estudios-ecológicos",
    "title": "Estudios ecológicos",
    "section": "Clasificación de estudios ecológicos",
    "text": "Clasificación de estudios ecológicos\n\n\nExploratorios: En los estudios exploratorios se comparan las tasas de enfermedad entre muchas regiones durante un mismo periodo, o se compara la frecuencia de la enfermedad a través del tiempo en una misma región. Se podría decir que se buscan patrones regionales o temporales, que pudieran dar origen a alguna hipótesis.\n➡️ Ocaña-Riola, R., et al. (2016). Geographical and Temporal Variations in Female Breast Cancer Mortality in the Municipalities of Andalusia (Southern Spain). International journal of environmental research and public health, 13(11), 1162.\n\n\nEstudios de grupos múltiples: Este es el tipo de estudio ecológico más común. Se evalúa la asociación entre los niveles de exposición promedio y la frecuencia de la enfermedad entre varios grupos. La fuente de datos suele ser las estadísticas de morbilidad y mortalidad rutinarias.\n➡️ Vinikoor-Imler, L. C., et al. (2011). An ecologic analysis of county-level PM2.5 concentrations and lung cancer incidence and mortality. International journal of environmental research and public health, 8(6), 1865–1871.\n\n\nEstudios de series temporales: Se comparan las variaciones temporales de los niveles de exposición con otra serie de tiempo que refleja los cambios en la frecuencia de la enfermedad en la población de un área geográfica determinada.\n➡️ Ballester, F., et al. (2001). Air pollution and emergency hospital admissions for cardiovascular diseases in Valencia, Spain.Journal of Epidemiology & Community Health, 55, 57-65.\n\n\nEstudios mixtos: En esta categoría se incluyen los estudios de series de tiempo combinadas con la evaluación de grupos múltiples.\n➡️ Schneider, M. C., et al. (2017). Leptospirosis in Latin America: Exploring the first set of regional data. Revista Panamericana de Salud Pública, 41, 1.\n\n\nAlgunos autores presentan la perspectiva que mostramos en la siguiente tabla, para una comprensión más rápida de las distintas posibilidades de los diseños ecológicos:\n\n\n\n\nTipo\nDiseño\nMarco temporal\n\n\n\nTransversal\nA lo largo de diferentes comunidades\nMismo período\n\n\nSeries temporales\nDentro de la misma comunidad\nA lo largo del tiempo\n\n\nDescriptivo\nA lo largo de diferentes comunidades o dentro de la misma comunidad\nUn punto en el tiempo o a lo largo del tiempo"
  },
  {
    "objectID": "unidad_3/01_est_ecologicos.html#análisis-de-estudios-ecológicos",
    "href": "unidad_3/01_est_ecologicos.html#análisis-de-estudios-ecológicos",
    "title": "Estudios ecológicos",
    "section": "Análisis de estudios ecológicos",
    "text": "Análisis de estudios ecológicos\nEn un análisis ecológico completo, todas las variables son medidas ecológicas (exposición o exposiciones, enfermedad u otras variables incluidas), ya que la unidad de análisis es el grupo. Ello implica que desconocemos la distribución conjunta de cualquier combinación de variables a nivel individual.\nEn un análisis ecológico parcial de tres o más variables puede tenerse información de la distribución conjunta de alguna de las variables en cada grupo. Por ejemplo, en el estudio de incidencia de cáncer se conocen la edad y el sexo de los casos (información individual) pero la exposición derivada de la residencia en un área concreta (municipio) es información ecológica.\nEl análisis multinivel es un tipo especial de modelización que combina el análisis efectuado a dos o más niveles. Ejemplo: la modelización de la incidencia de cáncer incluyendo el sexo y la edad como variables explicativas, además de variables socio-demográficas.\nComo podemos apreciar, tenemos muchas posibilidades para contemplar. En este capítulo, nos centraremos en aquellos estudios ecológicos completos donde el objetivo principal sea encontrar una relación entre la exposición y la enfermedad, es decir un estudio de grupos múltiples.\nLa manera usual de evaluación de la asociación en estudios de grupos múltiples es mediante modelos lineales de regresión, de hecho algunos autores se refieren a estos estudios como “Estudios de correlación”. Dependiendo del diseño y la distribución de los datos se pueden emplear otros modelos no lineales o no aditivos. Como las tasas de morbilidad y mortalidad en las regiones geográficas que se comparan comúnmente son eventos raros o que ocurren a bajas frecuencias, éstos semejan una distribución Poisson; así que la regresión de Poisson también puede ser usada.\nEn este capítulo abordaremos a modo de introducción el modelo lineal general, la covarianza y correlación, para luego entender los modelos de regresión lineal simple, análisis de la varianza y regresión lineal múltiple, ampliamente usados en el análisis de los estudios ecológicos, aunque no exclusivos de ellos.\n\nBallester Díez y Tenías Burillo (2003)\nEscuela Nacional de Sanidad (ENS). Instituto de Salud Carlos III. Ministerio de Ciencias e Innovación. Madrid (2009)\nDaniel (2002)\nHernández-Ávila (2011)\nTriola (2018)\nWeisberg (2005)"
  },
  {
    "objectID": "unidad_2/06_fun_condicionales.html",
    "href": "unidad_2/06_fun_condicionales.html",
    "title": "Funciones condicionales en R",
    "section": "",
    "text": "Continuamos incorporando funciones útiles para el manejo de datos, enfocándonos en la creación de nuevas variables y en la transformación de variables cuantitativas a variables categóricas, ya sean nominales u ordinales.\nAlgunas de estas operaciones serán necesarias para alcanzar los objetivos del trabajo práctico grupal de esta unidad, y seguramente también se utilizarán en las unidades siguientes.\nComo vimos dentro del universo tidyverse, para crear nuevas variables a partir de cálculos utilizamos la función-verbo mutate():\n\ndatos &lt;- datos  |&gt;   \n  mutate(variable_nueva = funcion(var1))\n\nFrecuentemente necesitaremos agrupar, resumir o discretizar variables continuas, dividiendo su rango en categorías que pueden ser dicotómicas o politómicas, según los objetivos del análisis."
  },
  {
    "objectID": "unidad_2/06_fun_condicionales.html#introducción",
    "href": "unidad_2/06_fun_condicionales.html#introducción",
    "title": "Funciones condicionales en R",
    "section": "",
    "text": "Continuamos incorporando funciones útiles para el manejo de datos, enfocándonos en la creación de nuevas variables y en la transformación de variables cuantitativas a variables categóricas, ya sean nominales u ordinales.\nAlgunas de estas operaciones serán necesarias para alcanzar los objetivos del trabajo práctico grupal de esta unidad, y seguramente también se utilizarán en las unidades siguientes.\nComo vimos dentro del universo tidyverse, para crear nuevas variables a partir de cálculos utilizamos la función-verbo mutate():\n\ndatos &lt;- datos  |&gt;   \n  mutate(variable_nueva = funcion(var1))\n\nFrecuentemente necesitaremos agrupar, resumir o discretizar variables continuas, dividiendo su rango en categorías que pueden ser dicotómicas o politómicas, según los objetivos del análisis."
  },
  {
    "objectID": "unidad_2/06_fun_condicionales.html#condicional-simple-if_else",
    "href": "unidad_2/06_fun_condicionales.html#condicional-simple-if_else",
    "title": "Funciones condicionales en R",
    "section": "\nCondicional simple: if_else()\n",
    "text": "Condicional simple: if_else()\n\nPara obtener salidas dicotómicas, podemos usar la función condicional if_else() del paquete dplyr. Esta función es una versión simplificada y segura del clásico condicional if que existe en la mayoría de los lenguajes de programación, así como en hojas de cálculo como Microsoft Excel® y Google Sheets®, entre otras.\nSupongamos que tenemos un dataframe llamado datos, con una variable cuantitativa denominada var1:\n\n# Cargar tidyverse\nlibrary(tidyverse)\n\n# Datos ejemplo\nset.seed(123)\n\ndatos &lt;- tibble(\n  var1 = sample(1:90, size = 50, replace = TRUE)\n  )\n\nQueremos crear una variable cualitativa dentro de datos, llamada variable_nueva, cuyos valores estén dados por el cumplimiento de alguna condición de var1. Por ejemplo, si los valores de var1 son mayores a 10, entonces variable_nueva, tomará el valor “mayor a 10”, en caso contrario, tomará el valor “menor o igual a 10”:\n\n# Crear variable_nueva\ndatos &lt;- datos |&gt;   \n  mutate(variable_nueva = if_else(condition = var1 &gt; 10,  \n                                  true = \"mayor a 10\",  \n                                  false = \"menor o igual a 10\"))\n\n# Explorar variable_nueva\ntabyl(datos, variable_nueva)\n\n     variable_nueva  n percent\n         mayor a 10 44    0.88\n menor o igual a 10  6    0.12\n\n\nLa función if_else() tiene tres argumentos obligatorios:\n\ncondition: la condición a evaluar.\ntrue: valor que tomará la nueva variable si se cumple la condición.\nfalse: valor que tomará la nueva variable si no se cumple la condición.\n\nEste proceso se conoce como dicotomización de una variable, ya que el resultado posible consta de solo dos categorías.\nLos valores de salida pueden ser de distintos tipos (carácter, numérico o lógico). Sin embargo, cuando discretizamos una variable cuantitativa, habitualmente generamos una variable cualitativa de tipo ordinal, con categorías expresadas como texto (tipo character).\nAhora bien, al ser ordinal estas categorías de la variable_nueva deben “ordenarse” en la forma de los valores de la variable, pero el lenguaje R no sabe con que estamos trabajando y respeta siempre el ordenamiento alfanumérico. Por lo tanto, en este ejemplo las categorías se van a estar ordenando al reves del orden numérico natural (de menor a mayor).\nLa categoría \"mayor a 10” se ordena alfabéticamente antes de “menor o igual a 10”, porque luego del empate de las letras m, le siguen la a en el primer caso y la e en el segundo.\nPara ordenar estas categorías debemos transformar la variable de character a factor. Esto se puede hacer en un solo paso dentro del mutate():\n\ndatos &lt;- datos |&gt;   \n  mutate(variable_nueva = if_else(condition = var1 &gt; 10,  \n                                  true = \"mayor a 10\",  \n                                  false = \"menor o igual a 10\"),\n         \n         variable_nueva = factor(variable_nueva,\n                                 levels = c(\"menor o igual a 10\", \n                                            \"mayor a 10\")))\n\n# Explorar variable_nueva\ntabyl(datos, variable_nueva)\n\n     variable_nueva  n percent\n menor o igual a 10  6    0.12\n         mayor a 10 44    0.88\n\n\nOtra forma más artesanal, igualmente válido, es “forzar” el ordenamiento con las categorías así:\n\ndatos &lt;- datos |&gt;   \n  mutate(variable_nueva = if_else(condition = var1 &gt; 10,  \n                                  true = \"2.mayor a 10\",  \n                                  false = \"1.menor o igual a 10\"))\n\n# Explorar variable_nueva\ntabyl(datos, variable_nueva)\n\n       variable_nueva  n percent\n 1.menor o igual a 10  6    0.12\n         2.mayor a 10 44    0.88\n\n\nAquí agregamos números iniciales a las etiquetas de las categorías para darle el orden que deseamos, sin necesidad de convertir a factor.\nFunción cut_interval()\n\nEl ecosistema tidyverse ofrece la función cut_interval() para la creación de intervalos regulares.\nEs una adaptación de la función cut() de R base para tidy data y sus argumentos son similares. Volviendo al ejemplo anterior:\n\ndatos &lt;- datos |&gt; \n  mutate(grupo_var = cut_interval(x = var1,\n                                  length = 10,\n                                  right = TRUE, \n                                  ordered_result = TRUE)\n         )\n\n# Explora grupo_var\ntabyl(datos, grupo_var)\n\n grupo_var n percent\n    [0,10] 6    0.12\n   (10,20] 4    0.08\n   (20,30] 7    0.14\n   (30,40] 6    0.12\n   (40,50] 6    0.12\n   (50,60] 5    0.10\n   (60,70] 4    0.08\n   (70,80] 7    0.14\n   (80,90] 5    0.10\n\n\nLos argumentos función son:\n\nx: conjunto de datos numéricos de entrada (obligatorio).\nlength: longitud de cada intervalo regular (obligatorio).\nn: cantidad de intervalos a crear (obligatorio, omitir si se definió length).\nright: indica si los intervalos son cerrados a la derecha o viceversa (valor por defecto TRUE, intervalos cerrados a la derecha).\nlabels: etiquetas de los intervalos automáticas o numéricas (por defecto intervalos matemáticos).\nordered_result: determina si el resultado es un factor ordenado. Por defecto vale FALSE (la salida es tipo carácter).\n\nNo es necesario definir los argumentos opcionales siempre y cuando los valores por defecto sirvan para nuestra tarea."
  },
  {
    "objectID": "unidad_2/06_fun_condicionales.html#función-case_when",
    "href": "unidad_2/06_fun_condicionales.html#función-case_when",
    "title": "Funciones condicionales en R",
    "section": "Función case_when()\n",
    "text": "Función case_when()\n\nCuando las condiciones no son simples, es decir, el resultado no es dicotómico y además los intervalos son irregulares, utilizamos la función case_when() que es una vectorización de la función if_else().\nSupongamos que queremos agrupar la variable cuantitativa de números enteros (var1) en tres grupos irregulares:\n\ndatos &lt;- datos |&gt; \n  mutate(grupo_var = case_when( \n    var1 %in% c(0:24)  ~  \"Grupo 1\", \n    var1 %in% c(25:64) ~    \"Grupo 2\", \n    var1 &gt;= 65 ~ \"Grupo 3\")\n    )\n\n# Explora grupo_var\ntabyl(datos, grupo_var)\n\n grupo_var  n percent\n   Grupo 1 12    0.24\n   Grupo 2 23    0.46\n   Grupo 3 15    0.30\n\n\nExiste una condición por cada grupo creado, como si fuese un if_else() donde el valor declarado siempre es el verdadero. Se utilizan operadores de comparación como mayor ( &gt; ), menor ( &lt; ) y/o igual ( == ) y conectores lógicos como & (AND), | (NOT) e %in%. En cada línea va una virgulilla similar a la usada en la sintaxis formula (~) y luego la etiqueta que tomarán las observaciones que cumplan con esa condición en la nueva variable (grupo_var).\nEsta evaluación es secuencial y su funcionamiento provoca que el usuario del lenguaje tenga el control de lo que esta sucediendo, por lo que cualquier mala definición de las condiciones puede provocar resultados incorrectos.\nSi incorporamos el argumento .default podemos indicar que valor toma si no se cumple ninguna de las condiciones anteriores.\nPor ejemplo, podríamos tener algún valor perdido (NA) en var1 y queremos que la variable grupo_var etiquete esos valores perdidos como \"Sin dato\".\n\ndatos &lt;- datos |&gt; \n  # Generar valores NA\n  mutate(var1 = na_if(var1, 50)) |&gt; \n  \n  mutate(grupo_var = case_when( \n    var1 %in% c(0:24)  ~  \"Grupo 1\", \n    var1 %in% c(25:64) ~    \"Grupo 2\", \n    var1 &gt;= 65 ~ \"Grupo 3\",\n    .default = \"Sin dato\")\n    )\n\n# Explora grupo_var\ntabyl(datos, grupo_var)\n\n grupo_var  n percent\n   Grupo 1 12    0.24\n   Grupo 2 22    0.44\n   Grupo 3 15    0.30\n  Sin dato  1    0.02\n\n\nOtra forma de definir los valores que no cumplen con ninguna condición es usando TRUE ~ valor:\n\ndatos &lt;- datos |&gt; \n  # Generar valores NA\n  mutate(var1 = na_if(var1, 50)) |&gt; \n  \n  mutate(grupo_var = case_when( \n    var1 %in% c(0:24)  ~  \"Grupo 1\", \n    var1 %in% c(25:64) ~    \"Grupo 2\", \n    var1 &gt;= 65 ~ \"Grupo 3\",\n    TRUE ~ \"Sin dato\")\n    )\n\n# Explora grupo_var\ntabyl(datos, grupo_var)\n\n grupo_var  n percent\n   Grupo 1 12    0.24\n   Grupo 2 22    0.44\n   Grupo 3 15    0.30\n  Sin dato  1    0.02\n\n\nLas salidas son de tipo carácter (chr) y debemos manejar el ordenamiento de las etiquetas como vimos anteriormente, por medio de factores o comenzando con caracteres ordenados alfabéticamente.\nPara simplificar el trabajo de estos intervalos de clase irregulares y no provocar errores en la confección de las condiciones, tidyverse tiene a la función between().\nIntervalos: función between()\n\nLa función between() básicamente opera como un atajo para condiciones de intervalos. Define dentro de los argumentos los límites inferior y superior de un intervalo y se utiliza dentro de una función de condición tipo if_else() o case_when().\nAplicado sobre el ejemplo anterior se vería así:\n\ndatos &lt;- datos |&gt; \n  mutate(grupo_var = case_when( \n    between(var1, 0, 24)   ~  \"Grupo 1\", \n        between(var1, 25, 64)  ~    \"Grupo 2\", \n        between(var1, 65, Inf) ~    \"Grupo 3\",\n        .default = \"Sin dato\"))\n\n# Explora grupo_var\ntabyl(datos, grupo_var)\n\n grupo_var  n percent\n   Grupo 1 12    0.24\n   Grupo 2 22    0.44\n   Grupo 3 15    0.30\n  Sin dato  1    0.02\n\n\nLos valores declarados como límites quedan incluidos siempre dentro del intervalo (son cerrados ambos). También podemos utilizar valores reservados como Inf o -Inf cuando desconocemos con que valor máximo o mínimo nos vamos a encontrar en la variable cuantitativa original.\nIntervalos: función age_categories()\n\nEl paquete epikit (Spina, Kamvar, y Schumacher 2024), contiene herramientas útiles para trabajar con datos en salud públicas. La función age_categories(), transforma una variable numérica discreta en grupos de edad. Volviendo al ejemplo anterior, supongamos que var1 contiene edades de pacientes:\n\n# Cargar paquete\nlibrary(epikit)\n\ndatos &lt;- datos |&gt; \n  mutate(edad_cat = age_categories(x = var1,\n                                   lower = 1,\n                                   upper = 80,\n                                   by = 10,\n                                   separator = \" a \",\n                                   above.char = \" y más\"))\n\n# Explora edad_cat\ntabyl(datos, edad_cat)\n\n edad_cat n percent valid_percent\n   1 a 10 6    0.12    0.12244898\n  11 a 20 4    0.08    0.08163265\n  21 a 30 7    0.14    0.14285714\n  31 a 40 6    0.12    0.12244898\n  41 a 50 5    0.10    0.10204082\n  51 a 60 5    0.10    0.10204082\n  61 a 70 4    0.08    0.08163265\n  71 a 79 7    0.14    0.14285714\n 80 y más 5    0.10    0.10204082\n     &lt;NA&gt; 1    0.02            NA\n\n\nLos argumentos principales de la función son:\n\nx: vector numérico (obligatorio).\nlower: límite inferior de edades (por defecto 0).\nupper: límite superior de edades.\nby: intervalo de años para los grupos.\nbreakers: alternativo a by, para definir manualmente los grupos.\nseparator: carácter que separa el rango de edades (por defecto \"-\").\nceiling: define si incluir el valor más alto en los grupos (por defecto FALSE).\nabove.char: el valor que queremos que aparezca para las edades por fuera del grupo de edad más alto (por defecto +). Solo funciona si ceiling = FALSE."
  },
  {
    "objectID": "unidad_2/06_fun_condicionales.html#ejemplo-práctico-en-r",
    "href": "unidad_2/06_fun_condicionales.html#ejemplo-práctico-en-r",
    "title": "Funciones condicionales en R",
    "section": "Ejemplo práctico en R",
    "text": "Ejemplo práctico en R\nTomemos un caso clásico como la variable edad medida en años, variable que generalmente tenemos en toda tabla de datos vinculada a personas. Trabajaremos con la base de datos “edad.txt” que contiene 106 observaciones de pacientes:\n\n# Cargar dataset\ndatos &lt;- read_csv2(\"datos/edad.txt\")\n\n# Explorar datos\nglimpse(datos)\n\nRows: 106\nColumns: 3\n$ id_mujer         &lt;dbl&gt; 115971, 1689710, 114710, 3058900, 3223547, 3234027, 3…\n$ fecha_nacimiento &lt;chr&gt; \"30/05/1967\", \"23/04/1991\", \"06/11/1962\", \"09/03/1964…\n$ fecha_test       &lt;chr&gt; \"17/01/2022\", \"01/02/2022\", \"16/03/2022\", \"23/02/2022…\n\n\nA continuación, vamos a transformar las variables cuyo nombre contenga “fecha_” a formato fecha para poder calcular edades:\n\ndatos &lt;- datos |&gt; \n  # Transformar a formato fecha\n  mutate(across(.cols = starts_with(\"fecha_\"),\n                .fns = ~ dmy(.x))) |&gt; \n  # Calcular edad al momento del testeo\n  mutate(edad = round(\n    as.duration(fecha_test - fecha_nacimiento) / dyears(1)))\n\nUna primera posibilidad es dicotomizar la edad usando el valor de la mediana:\n\ndatos |&gt;  \n  summarise(mediana = median(edad))\n\n# A tibble: 1 × 1\n  mediana\n    &lt;dbl&gt;\n1      56\n\n\nAplicando el valor de la mediana dentro de un if_else() podríamos hacer:\n\ndatos &lt;- datos |&gt;  \n  mutate(grupo_edad1 = if_else(condition = edad &gt; 56, \n                                  true = \"mayor a la mediana\", \n                                  false = \"menor o igual a la mediana\"))\n\n# Explorar grupo_edad1\ndatos |&gt; \n  count(grupo_edad1)\n\n# A tibble: 2 × 2\n  grupo_edad1                    n\n  &lt;chr&gt;                      &lt;int&gt;\n1 mayor a la mediana            52\n2 menor o igual a la mediana    54\n\n\nObservamos en el conteo que grupo_edad1 se construyó adecuadamente pero el orden de los niveles no es correcto si queremos que siga el ordenamiento natural de edad (de menor a mayor).\nUna de las formas que vimos es convertir a factor:\n\ndatos &lt;- datos |&gt; \n  mutate(grupo_edad1 = if_else(condition = edad &gt; 56, \n                                  true = \"mayor a la mediana\", \n                                  false = \"menor o igual a la mediana\"),\n         grupo_edad1 = factor(grupo_edad1, \n                                 levels = c(\"menor o igual a la mediana\",\n                                            \"mayor a la mediana\")))\n\n# Explorar grupo_edad1\ndatos |&gt; \n  count(grupo_edad1)\n\n# A tibble: 2 × 2\n  grupo_edad1                    n\n  &lt;fct&gt;                      &lt;int&gt;\n1 menor o igual a la mediana    54\n2 mayor a la mediana            52\n\n\nVemos que en el conteo el formato de la variable ya no es chr sino fct y el orden de las etiquetas siguen la forma “menor a mayor”.\nOtra forma es:\n\ndatos &lt;- datos |&gt; \n  mutate(grupo_edad1 = if_else(condition = edad &gt; 56, \n                                  true = \"2.mayor a la mediana\", \n                                  false = \"1.menor o igual a la mediana\"))\n\n# Explorar grupo_edad1\ndatos |&gt; \n  count(grupo_edad1)\n\n# A tibble: 2 × 2\n  grupo_edad1                      n\n  &lt;chr&gt;                        &lt;int&gt;\n1 1.menor o igual a la mediana    54\n2 2.mayor a la mediana            52\n\n\nSi en cambio necesitamos que los grupos sean mas de dos y que estos intervalos de clase sean regulares, podemos usar cut_interval():\n\ndatos &lt;- datos |&gt;  \n  mutate(grupo_edad2 = cut_interval(x = edad, \n                                    length = 10))\n\n# Explorar grupo_edad2\ndatos |&gt; \n  count(grupo_edad2)\n\n# A tibble: 8 × 2\n  grupo_edad2     n\n  &lt;fct&gt;       &lt;int&gt;\n1 [0,10]          3\n2 (10,20]         3\n3 (20,30]         2\n4 (30,40]         3\n5 (40,50]        13\n6 (50,60]        52\n7 (60,70]        27\n8 (70,80]         3\n\n\nLa salida muestra ocho grupos etarios con etiquetas ordenadas con notación matemática, donde un corchete indica que el límite del intervalo es cerrado, es decir contiene el valor y un paréntesis es abierto y no lo hace.Así es que el primer grupo va de 0 a 10 años y el segundo de 11 a 20.\nEstos sucede así porque en forma predeterminada el argumento right = TRUE. Veamos que pasa si lo cambiamos a FALSE:\n\ndatos &lt;- datos |&gt;  \n  mutate(grupo_edad2 = cut_interval(x = edad, \n                                    length = 10,\n                                    right = F))\n\n# Explorar grupo_edad2\ndatos |&gt; \n  count(grupo_edad2)\n\n# A tibble: 8 × 2\n  grupo_edad2     n\n  &lt;fct&gt;       &lt;int&gt;\n1 [0,10)          3\n2 [10,20)         3\n3 [20,30)         2\n4 [30,40)         3\n5 [40,50)        10\n6 [50,60)        48\n7 [60,70)        32\n8 [70,80]         5\n\n\nEn esta salida el primer grupo va de 0 a 9 y el segundo de 10 a 19.\nHasta ahora la variable grupo_edad2 es de tipo carácter, pero si deseamos que la salida sea factor podemos incorporar el argumento ordered_result = TRUE:\n\ndatos &lt;- datos |&gt; \n  mutate(grupo_edad2 = cut_interval(x = edad, \n                                    length = 10,\n                                    ordered_result = T))\n\n# Explorar grupo_edad2\ndatos |&gt; \n  count(grupo_edad2)\n\n# A tibble: 8 × 2\n  grupo_edad2     n\n  &lt;ord&gt;       &lt;int&gt;\n1 [0,10]          3\n2 (10,20]         3\n3 (20,30]         2\n4 (30,40]         3\n5 (40,50]        13\n6 (50,60]        52\n7 (60,70]        27\n8 (70,80]         3\n\n\nConstruimos así una variable factor ordenada &lt;ord&gt;.\nPor último, con el argumento labels= FALSE hacemos que las etiquetas de los ocho grupos sean numéricas:\n\ndatos &lt;- datos |&gt; \n  mutate(grupo_edad2 = cut_interval(x = edad, \n                                    length = 10,\n                                    labels = F))\n\n# Explorar grupo_edad2\ndatos |&gt; \n  count(grupo_edad2)\n\n# A tibble: 8 × 2\n  grupo_edad2     n\n        &lt;int&gt; &lt;int&gt;\n1           1     3\n2           2     3\n3           3     2\n4           4     3\n5           5    13\n6           6    52\n7           7    27\n8           8     3\n\n\nOtro ejemplo, podría ser aplicando case_when() donde discretizamos la edad en cuatro grupos irregulares, forzando sus etiquetas para lograr el orden adecuado:\n\ndatos &lt;- datos |&gt;  \n  mutate(grupo_edad3 = case_when(\n    edad &lt; 13              ~ \"1.Niño\",\n    edad &gt; 12 & edad &lt; 26  ~ \"2.Adolescente\",\n    edad &gt; 25 & edad &lt; 65  ~ \"3.Adulto_joven\",\n    edad &gt; 64              ~ \"4.Adulto_mayor\"\n  ))\n\n# Explorar grupo_edad3\ndatos |&gt; \n  count(grupo_edad3) \n\n# A tibble: 4 × 2\n  grupo_edad3        n\n  &lt;chr&gt;          &lt;int&gt;\n1 1.Niño             3\n2 2.Adolescente      5\n3 3.Adulto_joven    86\n4 4.Adulto_mayor    12\n\n\nSi no hubiésemos etiquetado con los números por delante el orden alfabético hacía que Niño fuese a parar al final del conteo.\nDe la misma forma pero más sencillo y controlado es:\n\ndatos &lt;- datos |&gt;  \n  mutate(grupo_edad3 = case_when(\n    between(edad, 0, 12)   ~ \"1.Niño\",\n    between(edad, 13, 25)  ~ \"2.Adolescente\",\n    between(edad, 26, 64)  ~ \"3.Adulto_joven\",\n    between(edad, 65, Inf) ~ \"4.Adulto_mayor\"\n  ))\n\n# Explorar grupo_edad3\ndatos |&gt; \n  count(grupo_edad3)\n\n# A tibble: 4 × 2\n  grupo_edad3        n\n  &lt;chr&gt;          &lt;int&gt;\n1 1.Niño             3\n2 2.Adolescente      5\n3 3.Adulto_joven    86\n4 4.Adulto_mayor    12\n\n\n\n\n\n\n\n\nEstas funciones condicionales que tratamos en este documento no se limitan a la tarea de construir agrupamientos de variables cuantitativas sino que sirven para cualquier situación donde a partir de una o más condiciones se produzcan una o más valores como respuesta.\n\n\n\nFinalmente veamos como quedarían los grupos de edad con la función age_categories():\n\ndatos &lt;- datos |&gt;  \n  mutate(grupo_edad4 = age_categories(x = edad,\n                                      breakers = c(0, 12, 25, 65),\n                                      above.char = \" y más\")\n  )\n\n# Explorar grupo_edad4\ndatos |&gt; \n  count(grupo_edad4)\n\n# A tibble: 4 × 2\n  grupo_edad4     n\n  &lt;fct&gt;       &lt;int&gt;\n1 0-11            3\n2 12-24           4\n3 25-64          87\n4 65 y más       12\n\n\nEn este caso se genera una variable de tipo factor con los grupos de edad ordenados de mayor a menor."
  },
  {
    "objectID": "unidad_2/04_normalidad_heterocedasticidad.html",
    "href": "unidad_2/04_normalidad_heterocedasticidad.html",
    "title": "Análisis de normalidad y homocedasticidad",
    "section": "",
    "text": "Este material es una continuación del capítulo sobre Análisis exploratorio de datos de la Unidad 1. Recordemos que uno de los objetivos del EDA es conocer cómo se distribuyen los valores de las variables de interés.\nLas características fundamentales a la hora de decidir si utilizaremos métodos paramétricos o no paramétricos para la inferencia estadística, es que los datos se ajusten a una distribución normal y conocer si tienen una dispersión homogénea o heterogénea. Ahora que conocemos el funcionamiento interno de los test de hipótesis, podemos usar pruebas de bondad de ajuste para evaluar los supuestos de normalidad y homocedasticidad."
  },
  {
    "objectID": "unidad_2/04_normalidad_heterocedasticidad.html#introducción",
    "href": "unidad_2/04_normalidad_heterocedasticidad.html#introducción",
    "title": "Análisis de normalidad y homocedasticidad",
    "section": "",
    "text": "Este material es una continuación del capítulo sobre Análisis exploratorio de datos de la Unidad 1. Recordemos que uno de los objetivos del EDA es conocer cómo se distribuyen los valores de las variables de interés.\nLas características fundamentales a la hora de decidir si utilizaremos métodos paramétricos o no paramétricos para la inferencia estadística, es que los datos se ajusten a una distribución normal y conocer si tienen una dispersión homogénea o heterogénea. Ahora que conocemos el funcionamiento interno de los test de hipótesis, podemos usar pruebas de bondad de ajuste para evaluar los supuestos de normalidad y homocedasticidad."
  },
  {
    "objectID": "unidad_2/04_normalidad_heterocedasticidad.html#normalidad",
    "href": "unidad_2/04_normalidad_heterocedasticidad.html#normalidad",
    "title": "Análisis de normalidad y homocedasticidad",
    "section": "Normalidad",
    "text": "Normalidad\nDeterminar que una distribución es aproximadamente normal nos permite decidirnos por test de comparaciones paramétricos.\nExisten tres enfoques que debemos analizar simultáneamente:\n\nMétodos gráficos\nMétodos analíticos\nPruebas de bondad de ajuste\n\nMétodos gráficos\nEl gráfico por excelencia para evaluar normalidad es el Q-Q Plot que consiste en comparar los cuantiles de la distribución observada con los cuantiles teóricos de una distribución normal con la misma media y desviación estándar que los datos.\nCuanto más se aproximen los datos a una normal, más alineados están los puntos entorno a la recta.\nEn el lenguaje R hay varios paquetes que tienen funciones para construirlos:\n\nlibrary(dlookr)\nlibrary(ggpubr)\nlibrary(moments)\nlibrary(nortest)\nlibrary(car)\nlibrary(tidyverse)\n\nCargamos datos de ejemplo:\n\ndatos &lt;- read_csv2(\"datos/datos_normalidad.csv\")\n\nEvaluación gráfica de normalidad usando la función plot_normality() del paquete dlookr:\n\ndatos |&gt; \n  plot_normality(peso) \n\n\n\n\n\n\n\nA simple vista observamos que los puntos de la variable peso se ajustan bastante bien a la recta.\nTambién podemos generar el qqplot usando las funciones geom_qq_line() y geom_qq() de ggplot2:\n\ndatos |&gt; \n  ggplot(mapping = aes(sample = peso)) +\n  geom_qq_line() +\n  geom_qq() +\n  theme_minimal()\n\n\n\n\n\n\n\nLa función ggqqplot() del paquete ggpubr (Kassambara 2023) nos permite agregar intervalos de confianza (zona gris alrededor de la recta) que nos orienta mejor sobre “donde caen” los puntos de la variable analizada:\n\ndatos$peso |&gt; \n  ggqqplot()\n\n\n\n\n\n\n\nUn ejemplo donde la variable parece no cumplir con el suspuesto de normalidad en estos datos de prueba es edad:\n\ndatos$edad |&gt; \n  ggqqplot()\n\n\n\n\n\n\n\nMétodos analíticos\nMedidas de forma\nExisten dos medidas de forma útiles que podemos calcular mediante funciones de R.\n\nLa curtosis (kurtosis)\nLa asimetría (skewness)\n\nLa curtosis mide el grado de agudeza o achatamiento de una distribución con relación a la distribución normal.\n\n&lt; 0 Distribución platicúrtica (apuntamiento negativo): baja concentración de valores\n&gt; 0 Distribución leptocúrtica (apuntamiento positivo): gran concentración de valores\n= 0 Distribución mesocúrtica (apuntamiento normal): concentración como en la distribución normal.\n\nEl paquete moments (Komsta y Novomestky 2022) posee algunas funciones interesantes para analizar medidas de forma, como el estimador de Pearson para curtosis:\n\ndatos |&gt; \n  summarise(kurtosis_edad = kurtosis(edad, na.rm = T),\n            kurtosis_peso = kurtosis(peso, na.rm = T))\n\n# A tibble: 1 × 2\n  kurtosis_edad kurtosis_peso\n          &lt;dbl&gt;         &lt;dbl&gt;\n1          8.05          2.72\n\n\nEn los dos casos estamos frente a una distribución leptocúrtica pero de magnitudes bien diferentes. Muy alta en el caso de la variable edad (8,0) y mucho menor para la variable peso (2,7).\nEl índice de asimetría es un indicador que permite establecer el grado de asimetría que presenta una distribución. Los valores menores que 0 indican distribución asimétrica negativa; los mayores a 0: distribución asimetrica positiva y cuando sea 0, o muy próximo a 0, distribución simétrica:\n\ndatos |&gt;  \n  summarise(asimetria_edad = skewness(edad, na.rm = T),\n            asimetria_peso = skewness(peso, na.rm = T))\n\n# A tibble: 1 × 2\n  asimetria_edad asimetria_peso\n           &lt;dbl&gt;          &lt;dbl&gt;\n1           2.19          0.122\n\n\nLos valores obtenidos con la función skewness() del paquete moments nos informan que la distribución de la edad tienen una asimetría positiva (2,2) y que los valores de peso se distribuyen bastante simétricos (0,1).\nEstas características de las distribuciones también se pueden ver mediante histogramas o gráficos de densidad:\n\ndatos  |&gt;  \n  plot_normality(edad, col = \"forestgreen\") \n\n\n\n\n\n\ndatos |&gt; \n  plot_normality(peso, col = \"royalblue\") \n\n\n\n\n\n\n\nLos histogramas que se acerquen a la clásica “campana de Gauss” tendrán curtosis y asimetrías alrededor del valor cero.\nPruebas de bondad de ajuste\nUna prueba de bondad de ajuste permite testear la hipótesis de que una variable aleatoria sigue cierta distribución de probabilidad y se utiliza en situaciones donde se requiere comparar una distribución observada con una teórica o hipotética.\nEl mecanismo es idéntico a cualquier test de hipótesis salvo que aquí esperamos no descartar la hipótesis nula de igualdad, por lo que obtener valores p de probabilidad mayores a 0,05 es signo de que la distribución de la variable analizada se ajusta.\nA continuación, presentaremos los test de hipótesis más utilizados para analizar normalidad.\nTest de Shapiro-Wilk\nLleva el nombre de sus autores (Samuel Shapiro y Martin Wilk) y es usado preferentemente para muestras de hasta 50 observaciones.\nLa función se encuentra desarrollada en el paquete stats y se llama shapiro.test():\n\nshapiro.test(datos$edad)\n\n\n    Shapiro-Wilk normality test\n\ndata:  datos$edad\nW = 0.69517, p-value = 4.912e-13\n\nshapiro.test(datos$peso)\n\n\n    Shapiro-Wilk normality test\n\ndata:  datos$peso\nW = 0.98615, p-value = 0.383\n\n\nInterpretación: Siendo la hipótesis nula que la población está distribuida normalmente, si el p-valor es menor a \\(\\alpha\\) (nivel de significancia, convencionalmente un 0,05) entonces la hipótesis nula es rechazada (se concluye que los datos no provienen de una distribución normal). Si el p-valor es mayor a \\(\\alpha\\), se concluye que no se puede rechazar dicha hipótesis.\nEn función de esta interpretación (que es común a todos los test de hipótesis de normalidad), podemos decir que la distribución de la variable edad no se ajusta a la normal y no podemos rechazar que la distribución de la variable peso se ajuste.\nTest de Kolmogorov-Smirnov\nEl test de Kolmogorov-Smirnov permite estudiar si una muestra procede de una población con una determinada distribución que no está limitado únicamente a la distribución normal.\nEl test asume que se conoce la media y varianza poblacional, lo que en la mayoría de los casos no es posible. Para resolver este problema, se realizó una modificación conocida como test Lilliefors.\nTest de Lilliefors\nEl test de Lilliefors asume que la media y varianza son desconocidas y está especialmente desarrollado para contrastar la normalidad.\nEs la alternativa al test de Shapiro-Wilk cuando el número de observaciones es mayor de 50.\nLa función lillie.test() del paquete nortest (Gross y Ligges 2015) permite aplicarlo:\n\nlillie.test(datos$edad)\n\n\n    Lilliefors (Kolmogorov-Smirnov) normality test\n\ndata:  datos$edad\nD = 0.24892, p-value &lt; 2.2e-16\n\nlillie.test(datos$peso)\n\n\n    Lilliefors (Kolmogorov-Smirnov) normality test\n\ndata:  datos$peso\nD = 0.049534, p-value = 0.7905\n\n\nLos resultados son coincidentes con los obtenidos anteriormente.\nTest de D’agostino\nEsta prueba se basa en las transformaciones de la curtosis y la asimetría de la muestra, y solo tiene poder frente a las alternativas de que la distribución sea sesgada.\nEl paquete moments la tiene implementada en agostino.test():\n\nagostino.test(datos$edad)\n\n\n    D'Agostino skewness test\n\ndata:  datos$edad\nskew = 2.1947, z = 6.3465, p-value = 2.203e-10\nalternative hypothesis: data have a skewness\n\nagostino.test(datos$peso)\n\n\n    D'Agostino skewness test\n\ndata:  datos$peso\nskew = 0.12160, z = 0.52683, p-value = 0.5983\nalternative hypothesis: data have a skewness\n\n\nLos resultados coinciden con la observación de asimetría que efectuamos con los métodos analíticos, confirmando que la variable edad no se ajusta a una curva simétrica y la variable peso si lo hace.\nCuando estos test se emplean con la finalidad de verificar las condiciones de métodos paramétricos es importante tener en cuenta que, al tratarse de valores probabilidad, cuanto mayor sea el tamaño de la muestra más poder estadístico tienen y más fácil es encontrar evidencias en contra de la hipótesis nula de normalidad.\nPor otra parte, cuanto mayor sea el tamaño de la muestra, menos sensibles son los métodos paramétricos a la falta de normalidad. Por esta razón, es importante no basar las conclusiones únicamente en los resultados de los test, sino también considerar los otros métodos (gráfico y analítico) y no olvidar el tamaño de la muestra."
  },
  {
    "objectID": "unidad_2/04_normalidad_heterocedasticidad.html#homocedasticidad",
    "href": "unidad_2/04_normalidad_heterocedasticidad.html#homocedasticidad",
    "title": "Análisis de normalidad y homocedasticidad",
    "section": "Homocedasticidad",
    "text": "Homocedasticidad\nLa homogeneidad de varianzas es un supuesto que considera constante la varianza en los distintos grupos que queremos comparar.\nEsta homogeneidad es condición necesaria antes de aplicar algunos test de hipótesis de comparaciones o bien para aplicar correcciones mediante los argumentos de las funciones de R.\nExisten diferentes test de bondad de ajuste que permiten evaluar la distribución de la varianza. Todos ellos consideran como \\(H_0\\) que la varianza es igual entre los grupos y como \\(H_1\\) que no lo es.\nLa diferencia entre ellos es el estadístico de centralidad que utilizan:\n\nMedia de la varianza: son los más potentes pero se aplican en distribuciones que se aproximan a la normal.\nMediana de la varianza: son menos potentes pero consiguen mejores resultados en distribuciones asimétricas.\n\nF-test\nEste test es un contraste de la razón de varianzas, mediante el estadístico F que sigue una distribución F-Snedecor.\nSe utiliza cuando las distribuciones se aproximan a la “normal” y en R base se la encuentra en la función var.test() que permite utilizar la sintaxis de fórmula:\n\nvariable_cuantitativa ~ variable_categórica_grupos\n\nPor ejemplo:\n\nvar.test(formula = peso ~ sexo, data = datos)\n\n\n    F test to compare two variances\n\ndata:  peso by sexo\nF = 0.67914, num df = 50, denom df = 48, p-value = 0.1781\nalternative hypothesis: true ratio of variances is not equal to 1\n95 percent confidence interval:\n 0.384712 1.194943\nsample estimates:\nratio of variances \n         0.6791414 \n\n\nComparamos las varianzas de la variable peso entre el grupo de mujeres y hombres. El valor \\(p\\) del test indica que no podemos descartar la igualdad de varianzas entre los grupos (\\(H_0\\)) o lo que es lo mismo el test no encuentra diferencias significativas entre las varianzas de los dos grupos.\nTest de Bartlett\nEste test se puede utilizar como alternativa al F-test, sobre todo porque nos permite aplicarlo cuando tenemos más de 2 grupos de comparación. Al igual que el anterior es sensible a las desviaciones de la normalidad.\nLa función en R base es bartlett.test() y también se pueden usar argumentos tipo fórmula:\n\nbartlett.test(formula = peso ~ sexo, data = datos)\n\n\n    Bartlett test of homogeneity of variances\n\ndata:  peso by sexo\nBartlett's K-squared = 1.8082, df = 1, p-value = 0.1787\n\n\nEl resultado es coincidente con el mostrado por var.test(). No se encuentran diferencias significativas entres las varianzas de los pesos en los dos grupos (Mujer - Varon)\nTest de Levene\nEl test de Levene sirve para comparar la varianza de 2 o más grupos pero además permite elegir distintos estadísticos de tendencia central. Por lo tanto, la podemos adaptar a distribuciones alejadas de la normalidad seleccionando por ejemplo la mediana.\nLa función leveneTest() se encuentra disponible en el paquete car. La vemos aplicada sobre peso para los diferentes grupos de sexo y utilizando la media como estadístico de centralidad, dado que la distribución de peso se aproxima a la normal.\n\nleveneTest(y = peso ~ sexo, data = datos, center = \"mean\")\n\nLevene's Test for Homogeneity of Variance (center = \"mean\")\n      Df F value Pr(&gt;F)\ngroup  1       2 0.1605\n      98               \n\n\nLa conclusión es la misma que la encontrada anteriormente.\nAhora vamos aplicarla sobre la variable edad, de la que habíamos descartado “normalidad”. Lo hacemos usando el argumento center con \"mean\" (media) y con \"median\" (mediana).\n\nleveneTest(y = edad ~ sexo, data = datos, center = \"mean\")\n\nLevene's Test for Homogeneity of Variance (center = \"mean\")\n      Df F value Pr(&gt;F)  \ngroup  1  4.6784  0.033 *\n      97                 \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nleveneTest(y = edad ~ sexo, data = datos, center = \"median\")\n\nLevene's Test for Homogeneity of Variance (center = \"median\")\n      Df F value Pr(&gt;F)\ngroup  1  2.4413 0.1214\n      97               \n\n\nLos resultados son diferentes. Mientras con el centrado en la media nos da un p valor significativo menor a 0,05 con el centrado en la mediana no nos permite descartar homocedasticidad.\nObservamos aquí las distorsiones sobre la media y las formas paramétricas que devienen de distribuciones asimétricas y alejadas de la curva normal. El código correcto para este caso (variable edad) es usar el centrado en la mediana (center = \"median\")."
  },
  {
    "objectID": "unidad_2/04_normalidad_heterocedasticidad.html#ejemplo-práctico-en-r",
    "href": "unidad_2/04_normalidad_heterocedasticidad.html#ejemplo-práctico-en-r",
    "title": "Análisis de normalidad y homocedasticidad",
    "section": "Ejemplo práctico en R",
    "text": "Ejemplo práctico en R\nLos difenilos policlorados (PCB) son una clase de contaminantes ambientales con efectos adversos en la salud humana. Se ha asociado su exposición intrauterina con deterioro cognitivo en niños, y se encuentran entre los contaminantes más abundantes en el tejido graso humano.\nUn estudio realizado por Tu Binh Minh y colaboradores en la Universidad Nacional de Vietnam analizó la concentración de PCB en la grasa de un grupo de 14 adultos. Los valores de PCB hallados en las personas estudiadas se encuentran almacenadas en el archivo “PCB.txt“ y se nos pide realizar los siguientes items:\n\nLeer el archivo y explorar su contenido.\nDescribir estadísticamente las mediciones.\nEvaluar si los datos provienen de una distribución normal.\nCalcular los intervalos de confianza (IC) del 90% y 95% para la media de la concentración de PCB.\n\nVeamos como llevamos adelante estos pasos mediante el lenguaje R:\nLectura y exploración de datos\nCarga de paquetes:\n\nlibrary(dlookr)\nlibrary(tidyverse)\n\nCarga de datos:\n\ndatos &lt;- read_csv2(\"datos/PCB.txt\")\n\nVisualizamos la estructura de la tabla de datos:\n\nglimpse(datos)\n\nRows: 14\nColumns: 2\n$ individuo     &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14\n$ concentracion &lt;dbl&gt; 930, 2300, 2300, 1700, 720, 2500, 560, 1800, 1800, 2600,…\n\n\nTenemos una tabla con 14 observaciones (mediciones en individuos) y la variable de interés se llama concentracion (nos informan que está expresada como nanogramo de PCB/gramo de lípido)\nEstadísticos descriptivos\nCon la función describe() de dlookr podemos obtener los estadísticos de esta variable cuantitativa continua:\n\ndatos  |&gt; \n  describe(concentracion)\n\n# A tibble: 1 × 26\n  described_variables     n    na  mean    sd se_mean   IQR skewness kurtosis\n  &lt;chr&gt;               &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 concentracion          14     0 1709.  825.    220. 1278.   0.0348   -0.852\n# ℹ 17 more variables: p00 &lt;dbl&gt;, p01 &lt;dbl&gt;, p05 &lt;dbl&gt;, p10 &lt;dbl&gt;, p20 &lt;dbl&gt;,\n#   p25 &lt;dbl&gt;, p30 &lt;dbl&gt;, p40 &lt;dbl&gt;, p50 &lt;dbl&gt;, p60 &lt;dbl&gt;, p70 &lt;dbl&gt;,\n#   p75 &lt;dbl&gt;, p80 &lt;dbl&gt;, p90 &lt;dbl&gt;, p95 &lt;dbl&gt;, p99 &lt;dbl&gt;, p100 &lt;dbl&gt;\n\n\nLa media (1709,29 ng/g) y la mediana (1750,0 ng/g) son cercanas. El desvío estandar es de 824,81 ng/g.\nEvaluación de normalidad\nPara verificar si los datos siguen una distribución normal, aplicamos el test de Shapiro-Wilk y visualizamos un gráfico de normalidad:\n\nshapiro.test(datos$concentracion)\n\n\n    Shapiro-Wilk normality test\n\ndata:  datos$concentracion\nW = 0.95246, p-value = 0.5996\n\n\nUn p-valor mayor a 0,05 habla de un ajuste a la curva normal.\n\ndatos  |&gt; \n  plot_normality(concentracion)\n\n\n\n\n\n\n\nSi bien son pocos datos, el p-valor &gt; 0.05 y el gráfico no muestra una desviación evidente, por lo que asumimos que los datos siguen una distribución normal y podemos usar métodos paramétricos.\nCálculo del intervalo de confianza (IC)\nEstamos frente al calculo de IC de una muestra de 14 observaciones (menor a 30 observaciones) y que cumple con el supuesto de normalidad, por lo tanto debemos aplicar un método paramétrico basado en la distribución \\(t\\) de Student.\nLa distribución\\(t\\) es similar a la normal, con la media y la varianza como parámetros, pero ajusta sus colas según el tamaño muestral mediante los grados de libertad (n-1). Para muestras pequeñas, esto mejora la precisión de los intervalos de confianza.\nEn la practica, cuando trabajamos sobre inferencia de la media (para IC o para pruebas de hipótesis para comparar) y el tamaño muestral es mayor a 30 igualmente se hace con la distribución \\(t\\) de Student, dado que con 30 o más grados de libertad es prácticamente igual a una distribución normal.\nLa función t.test(), incluída en el paquete stats de R, sirve apara ejecutar test de hipótesis de medias de una y dos poblaciones (independientes o pareadas) pero además nos calcula automáticamente el intervalo de confianza requerido, basado en la distribución \\(t\\) de Student.\nPara este ejemplo debemos utilizar como argumentos:\n\nla variable de interés que contiene los valores de la muestra\nel nivel de confianza con el que necesitamos el IC\n\n\nt.test(x = datos$concentracion, \n       conf.level = 0.95)\n\n\n    One Sample t-test\n\ndata:  datos$concentracion\nt = 7.754, df = 13, p-value = 3.143e-06\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n 1233.053 2185.518\nsample estimates:\nmean of x \n 1709.286 \n\n\nEn la lista de resultados devuelta debemos observar los valores que figuran debajo de \"95 percent confidence interval:“. En este caso, el resultado es de una media de 1709,3 (IC 95%: 1233,0-2185,5 ng/g).\nSi cambiamos el argumento conf.level a 0.90 obtendremos el IC 90%:\n\nt.test(x = datos$concentracion, \n       conf.level = 0.90)\n\n\n    One Sample t-test\n\ndata:  datos$concentracion\nt = 7.754, df = 13, p-value = 3.143e-06\nalternative hypothesis: true mean is not equal to 0\n90 percent confidence interval:\n 1318.901 2099.671\nsample estimates:\nmean of x \n 1709.286 \n\n\nEste va de 1318,9 a 2099,7 ng/g, lógicamente más estrecho que el anterior porque redujimos el intervalo de confianza.\n\n(2025; Wickham et al. 2019)"
  },
  {
    "objectID": "unidad_2/02_est_transversales.html",
    "href": "unidad_2/02_est_transversales.html",
    "title": "Estudios de corte transversal",
    "section": "",
    "text": "Los estudios transversales se caracterizan por realizar mediciones en un único momento en el tiempo, sin llevar a cabo un seguimiento de los participantes. En este diseño, el investigador mide de forma simultánea la exposición y el evento en los sujetos seleccionados mediante criterios de inclusión y exclusión predefinidos.\nSe trata de un diseño observacional de base individual, que puede tener un propósito descriptivo, analítico, o ambos. También se los conoce como estudios de prevalencia o encuestas transversales. Los estudios transversales entonces pueden dividirse en:\n\nDescriptivos: se enfocan en determinar la frecuencia y distribución de eventos de salud y enfermedad en un momento determinado, midiendo una o más variables o condiciones.\nAnalíticos: se orientan a explorar asociaciones y generar hipótesis de investigación. Algunos autores ubican este diseño en el límite entre los estudios descriptivos y los analíticos.\n\nEn los estudios transversales, la medida de frecuencia más utilizada es la prevalencia. Para evaluar asociaciones se pueden calcular dos medidas principales:\n\nRazón de prevalencia (prevalence ratio): que aproxima el riesgo relativo observado en estudios de cohortes.\nOdds ratio de prevalencia (prevalence odds ratio): que se utiliza como aproximación al odds ratio calculado en estudios de casos y controles.\n\nA continuación se resumen las ventajas y desventajas de este diseño:\n\n\n\n\nVentajas\nDesventajas\n\n\n\nEficientes para estudiar la prevalencia de enfermedades en la población\nProblemas para definir y medir exposición\n\n\nSe pueden estudiar varias exposiciones\nSesgos de selección\n\n\nSon poco costosos y se pueden realizar en poco tiempo\nSesgos por casos prevalentes\n\n\nSe puede estimar la prevalencia del evento\nLa relación causa efecto no siempre es verificable\n\n\n\nSobrerrepresentación de enfermos con tiempos prolongados de sobrevida o con manifestaciones con mejor curso clínico\n\n\n\nSe puede presentar causalidad débil\n\n\n\n\n\n\nNota: Los aspectos metodológicos esenciales para el reporte de estudios transversales pueden consultarse en la Declaración STROBE, sección Métodos."
  },
  {
    "objectID": "unidad_2/02_est_transversales.html#introducción",
    "href": "unidad_2/02_est_transversales.html#introducción",
    "title": "Estudios de corte transversal",
    "section": "",
    "text": "Los estudios transversales se caracterizan por realizar mediciones en un único momento en el tiempo, sin llevar a cabo un seguimiento de los participantes. En este diseño, el investigador mide de forma simultánea la exposición y el evento en los sujetos seleccionados mediante criterios de inclusión y exclusión predefinidos.\nSe trata de un diseño observacional de base individual, que puede tener un propósito descriptivo, analítico, o ambos. También se los conoce como estudios de prevalencia o encuestas transversales. Los estudios transversales entonces pueden dividirse en:\n\nDescriptivos: se enfocan en determinar la frecuencia y distribución de eventos de salud y enfermedad en un momento determinado, midiendo una o más variables o condiciones.\nAnalíticos: se orientan a explorar asociaciones y generar hipótesis de investigación. Algunos autores ubican este diseño en el límite entre los estudios descriptivos y los analíticos.\n\nEn los estudios transversales, la medida de frecuencia más utilizada es la prevalencia. Para evaluar asociaciones se pueden calcular dos medidas principales:\n\nRazón de prevalencia (prevalence ratio): que aproxima el riesgo relativo observado en estudios de cohortes.\nOdds ratio de prevalencia (prevalence odds ratio): que se utiliza como aproximación al odds ratio calculado en estudios de casos y controles.\n\nA continuación se resumen las ventajas y desventajas de este diseño:\n\n\n\n\nVentajas\nDesventajas\n\n\n\nEficientes para estudiar la prevalencia de enfermedades en la población\nProblemas para definir y medir exposición\n\n\nSe pueden estudiar varias exposiciones\nSesgos de selección\n\n\nSon poco costosos y se pueden realizar en poco tiempo\nSesgos por casos prevalentes\n\n\nSe puede estimar la prevalencia del evento\nLa relación causa efecto no siempre es verificable\n\n\n\nSobrerrepresentación de enfermos con tiempos prolongados de sobrevida o con manifestaciones con mejor curso clínico\n\n\n\nSe puede presentar causalidad débil\n\n\n\n\n\n\nNota: Los aspectos metodológicos esenciales para el reporte de estudios transversales pueden consultarse en la Declaración STROBE, sección Métodos."
  },
  {
    "objectID": "unidad_2/02_est_transversales.html#análisis-de-estudios-transversales",
    "href": "unidad_2/02_est_transversales.html#análisis-de-estudios-transversales",
    "title": "Estudios de corte transversal",
    "section": "Análisis de estudios transversales",
    "text": "Análisis de estudios transversales\nAl planificar un estudio transversal, es fundamental definir con precisión la población de estudio. La selección de los sujetos depende directamente de la pregunta de investigación y los objetivos planteados. Una identificación adecuada de la población garantiza la validez externa de los resultados y su potencial generalización.\nEn la práctica, estos estudios rara vez incluyen a todos los miembros de la población objetivo, sino que se trabaja con una muestra. Solo una muestra representativa, que refleje las características de la población base, permite desarrollar el componente analítico y exploratorio del diseño transversal, de modo que las conclusiones sean relevantes y aplicables.\nUna vez obtenidos los datos, éstos pueden representarse gráficamente, resumirse mediante estadísticas descriptivas (como medidas de tendencia central y dispersión) e incluso utilizarse para modelar relaciones entre variables.\n\nSi el objetivo es describir los resultados en la muestra, las herramientas descriptivas pueden ser suficientes.\nSi se busca extrapolar a la población general, es necesario aplicar métodos de inferencia estadística, fundamentados en la teoría de la probabilidad.\n\n\nRodríguez y Mendivelso (2018)\nHernández-Ávila (2011)"
  },
  {
    "objectID": "unidad_1/05_hoja_estilo.html",
    "href": "unidad_1/05_hoja_estilo.html",
    "title": "Hoja de Estilo del lenguaje R",
    "section": "",
    "text": "R es bastante indulgente con la forma en que escribimos código, a diferencia de otros lenguajes como Python, donde un espacio mal puesto puede arruinar el script. Sin embargo, adoptar una guía de estilo mejora la legibilidad, facilita la colaboración y reduce errores.\nLas siguientes líneas de código producen el mismo resultado, pero no todas son igual de claras:\n\n# Opción 1\nmpg |&gt; \n  filter(cty &gt; 10, class == \"compact\")\n\n# Opción 2\nmpg |&gt; filter(cty &gt; 10, class == \"compact\")\n\n# Opción 3\nmpg |&gt; \n  filter(cty &gt; 10, \n         class == \"compact\")\n\n# Opción 4\nmpg |&gt; filter(cty&gt;10, class==\"compact\")\n\n# Opción 5\nfilter(mpg,cty&gt;10,class==\"compact\")\n\n# Opción 6\nmpg |&gt; \nfilter(cty &gt; 10, \n                        class == \"compact\")\n\n# Opción 7\nfilter ( mpg,cty&gt;10,     class==\"compact\" )\n\nLas tres primeras versiones son más legibles. El resto, aunque válidas, son difíciles de seguir, especialmente en trabajos colaborativos o materiales docentes.",
    "crumbs": [
      "Unidad 1",
      "Hoja de Estilo del lenguaje R"
    ]
  },
  {
    "objectID": "unidad_1/05_hoja_estilo.html#convenciones-de-estilo-r",
    "href": "unidad_1/05_hoja_estilo.html#convenciones-de-estilo-r",
    "title": "Hoja de Estilo del lenguaje R",
    "section": "",
    "text": "R es bastante indulgente con la forma en que escribimos código, a diferencia de otros lenguajes como Python, donde un espacio mal puesto puede arruinar el script. Sin embargo, adoptar una guía de estilo mejora la legibilidad, facilita la colaboración y reduce errores.\nLas siguientes líneas de código producen el mismo resultado, pero no todas son igual de claras:\n\n# Opción 1\nmpg |&gt; \n  filter(cty &gt; 10, class == \"compact\")\n\n# Opción 2\nmpg |&gt; filter(cty &gt; 10, class == \"compact\")\n\n# Opción 3\nmpg |&gt; \n  filter(cty &gt; 10, \n         class == \"compact\")\n\n# Opción 4\nmpg |&gt; filter(cty&gt;10, class==\"compact\")\n\n# Opción 5\nfilter(mpg,cty&gt;10,class==\"compact\")\n\n# Opción 6\nmpg |&gt; \nfilter(cty &gt; 10, \n                        class == \"compact\")\n\n# Opción 7\nfilter ( mpg,cty&gt;10,     class==\"compact\" )\n\nLas tres primeras versiones son más legibles. El resto, aunque válidas, son difíciles de seguir, especialmente en trabajos colaborativos o materiales docentes.",
    "crumbs": [
      "Unidad 1",
      "Hoja de Estilo del lenguaje R"
    ]
  },
  {
    "objectID": "unidad_1/05_hoja_estilo.html#guía-de-estilo-de-tidyverse",
    "href": "unidad_1/05_hoja_estilo.html#guía-de-estilo-de-tidyverse",
    "title": "Hoja de Estilo del lenguaje R",
    "section": "Guía de estilo de tidyverse\n",
    "text": "Guía de estilo de tidyverse\n\nPara ayudar a mejorar la legibilidad y facilitar el compartir código con otros, el equipo de Tidyverse publicó una guía concisa con ejemplos claros de buenas y malas formas de escribir código, nombres de variables, sangría, líneas largas, y más:\n➡️ https://style.tidyverse.org/\nAdemás, RStudio incluye herramientas para aplicar estas convenciones automáticamente. Por ejemplo, seleccionando el código y presionando Ctrl + i (Windows) se puede reidentar el texto. No siempre es perfecto, pero es realmente útil para lograr la sangría correcta sin tener que presionar manualmente espacio muchas veces.\nEspaciado\nColocar espacios después de las comas:\n✔️ Correcto\n\nfilter(mpg, cty &gt; 10)\n\n✖️ Incorrecto\n\nfilter(mpg , cty &gt; 10)\n\nfilter(mpg ,cty &gt; 10)\n\nfilter(mpg,cty &gt; 10)\n\nColocar espacios después de comas y alrededor de operadores (+, -, &gt;, =, etc.) mejora la legibilidad. También se deben evitar espacios innecesarios dentro de paréntesis:\n✔️ Correcto\n\nfilter(mpg, cty &gt; 10)\n\n✖️ Incorrecto\n\nfilter(mpg, cty&gt;10)\n\nfilter(mpg, cty&gt; 10)\n\nfilter(mpg, cty &gt;10)\n\nNo colocar espacios alrededor de paréntesis que sean parte de funciones:\n✔️ Correcto\n\nfilter(mpg, cty &gt; 10)\n\n✖️ Incorrecto\n\nfilter (mpg, cty &gt; 10)\n\nfilter ( mpg, cty &gt; 10)\n\nfilter( mpg, cty &gt; 10 )\n\nLíneas largas\nEn general, es una buena práctica no tener líneas de código muy largas. Se recomienda limitar las líneas a 80 caracteres. Para visualizarlo, en RStudio vamos a Tools &gt; Global Options &gt; Code &gt; Displayy seleccionamos la casilla Show margin.\nSe sugiere agregar saltos de línea dentro de las líneas de código más largas, los mismos deben colocarse luego de las comas y los argumentos se deben alinear dentro de la función:\n✔️ Correcto\n\nfilter(mpg, cty &gt; 10, class == \"compact\")\n\n\nfilter(mpg, cty &gt; 10, \n       class == \"compact\")\n\n\nfilter(mpg,\n       cty &gt; 10,\n       class == \"compact\")\n\nfilter(mpg, \n       cty &gt; 10, \n       class %in% c(\"compact\", \"pickup\", \"midsize\", \"subcompact\", \n                    \"suv\", \"2seater\", \"minivan\"))\n\n✖️ Incorrecto\n\nfilter(mpg, cty &gt; 10, class %in% c(\"compact\", \"pickup\", \"midsize\", \"subcompact\", \"suv\", \"2seater\", \"minivan\"))\n\nTuberías y capas ggplot2\n\nColocar cada paso de la tubería (%&gt;% ó |&gt;) en una línea separada, con sangría de dos espacios debajo del operador:\n✔️ Correcto\n\nmpg |&gt; \n  filter(cty &gt; 10) |&gt; \n  group_by(class) |&gt; \n  summarize(avg_hwy = mean(hwy))\n\n✖️ Incorrecto\n\n# Mal\nmpg |&gt; filter(cty &gt; 10) |&gt; group_by(class) |&gt; \n  summarize(avg_hwy = mean(hwy))\n\n# Muy mal\nmpg |&gt; filter(cty &gt; 10) |&gt; group_by(class) |&gt; summarize(avg_hwy = mean(hwy))\n\n# Tan mal que no funciona\nmpg |&gt; \n  filter(cty &gt; 10)\n  |&gt; group_by(class)\n  |&gt; summarize(avg_hwy = mean(hwy))\n\nLo mismo aplica para las capas de gráficos de ggplot2, usando el conector + al final de la línea y debajo sangría de dos espacios:\n✔️ Correcto\n\nggplot(mpg, aes(x = cty, y = hwy, color = class)) +\n  geom_point() +\n  geom_smooth() +\n  theme_bw()\n\n# Mal\nggplot(mpg, aes(x = cty, y = hwy, color = class)) +\n  geom_point() + geom_smooth() +\n  theme_bw()\n\n# Muy mal\nggplot(mpg, aes(x = cty, y = hwy, color = class)) + geom_point() + geom_smooth() + theme_bw()\n\n# Tan mal que no funciona\nggplot(mpg, aes(x = cty, y = hwy, color = class))\n  + geom_point()\n  + geom_smooth() \n  + theme_bw()\n\n✖️ Incorrecto\n\n# Mal\nggplot(mpg, aes(x = cty, y = hwy, color = class)) +\n  geom_point() + geom_smooth() +\n  theme_bw()\n\n# Muy mal\nggplot(mpg, aes(x = cty, y = hwy, color = class)) + geom_point() + geom_smooth() + theme_bw()\n\n# Tan mal que no funciona\nggplot(mpg, aes(x = cty, y = hwy, color = class))\n  + geom_point()\n  + geom_smooth() \n  + theme_bw()\n\nComentarios\nLos comentarios deben comenzar con # seguido de un espacio:\n✔️ Correcto\n\n# Bien\n\n✖️ Incorrecto\n\n#Mal\n\n    #Mal\n\nSi el comentario es corto, se puede incluir en la misma línea, separado por al menos dos espacios para mejorar la legibilidad:\n\nmpg |&gt; \n  filter(cty &gt; 10) |&gt;  # filtro filas donde cty es 10 o más\n  group_by(class) |&gt;  # estratifica por class\n  summarize(avg_hwy = mean(hwy))  # resume la media de hwy por cada grupo\n\nSe puede agregar espacios adicionales para alinear los comentarios en línea, si lo deseamos:\n\nmpg |&gt; \n  filter(cty &gt; 10) |&gt;             # filtro filas donde cty es 10 o más\n  group_by(class) |&gt;              # estratifica por class\n  summarize(avg_hwy = mean(hwy))  # resume la media de hwy por cada grupo\n\nSi el comentario es muy largo, podemos dividirlo en varias líneas. RStudio incluye una herramienta útil para comentarios largos: Code &gt; Reflow Comment los ajusta automáticamente al ancho deseado.",
    "crumbs": [
      "Unidad 1",
      "Hoja de Estilo del lenguaje R"
    ]
  },
  {
    "objectID": "unidad_1/03_intro_tidyverse.html#introducción",
    "href": "unidad_1/03_intro_tidyverse.html#introducción",
    "title": "Introducción a tidyverse",
    "section": "Introducción",
    "text": "Introducción\nTidyverse (Wickham et al. 2019) es el nombre que recibe el conjunto de paquetes desarrollados y/o promovidos por Hadley Wickham (jefe científico en Posit/RStudio) y su equipo, orientado al trabajo de ciencia de datos con R. Estos paquetes están diseñados para integrarse de manera coherente, compartiendo una misma filosofía de diseño conocida como The tidy tools manifesto.\nLos cuatro principios básicos sobre los que se construye tidyverse son:\n\nReutilización de estructuras de datos\nResolución de problemas complejos combinando varias piezas sencillas\nUso de programación funcional\nDiseño orientado a las personas\n\nLos paquetes incluidos cubren todas las etapas del análisis de datos dentro de R: importación y ordenamiento de los datos (tidy data), transformación, visualización, modelado y la posterior comunicación de resultados.\nLa palabra tidy se traduce como “ordenado”, y hace referencia a una estructura específica que deben cumplir los datos:\n\nCada variable es una columna de la tabla de datos.\nCada observación es una fila de la tabla de datos.\nCada tabla responde a una unidad de observación o análisis.\n\n\n\n\n\nAdemás de los paquetes principales, al instalar tidyverse se incluyen otros que permiten trabajar con fechas, cadenas de caracteres o factores, también siguiendo los mismos principios.\nUno de los objetivos de los desarrolladores fue dotar a la sintaxis de estos paquetes de una gramática clara: funciones cuyos nombres y argumentos permiten construir “frases” que sean semánticamente comprensibles. Un ejemplo de esto se ve en el paquete dplyr, donde la mayoría de las funciones son verbos en inglés como filter(), mutate(), summarise(), lo que facilita su lectura y comprensión.\nEl paquete tidyverse (versión 2.0.0) puede instalarse desde el repositorio oficial CRAN mediante el menú Packages de RStudio, o ejecutando el siguiente código:\n\ninstall.packages(\"tidyverse\")\n\nUna vez instalado, se activa mediante:\n\nlibrary(tidyverse)\n\nAl activarlo, se muestra un mensaje con la versión instalada, la lista de paquetes que se cargan automáticamente y posibles conflictos de nombres entre funciones. Esto es habitual cuando se utilizan múltiples paquetes, ya que algunas funciones pueden llamarse igual. Por ejemplo, la función filter() existe tanto en el paquete stats como en dplyr. Al cargar tidyverse, R avisa de esta superposición:\n✖️dplyr::filter() masks stats::filter()\nCuando necesitamos asegurarnos de que estamos usando la función de un paquete específico, se recomienda usar la notación ::, por ejemplo:\n\n# Función filter() del paquete stats\nstats::filter()\n\n# Función filter() del paquete tidyverse\ndplyr::filter()\n\nUna estrategia útil cuando trabajamos con varios paquetes es cargar tidyverse al final de la lista de paquetes, para que sus funciones sobrescriban las de otros paquetes si fuese necesario:\n\nlibrary(stats)\nlibrary(tidyverse)\n\nLos paquetes que se instalan con la versión actual de tidyverse pueden consultarse ejecutando:\n\ntidyverse_packages()\n\n [1] \"broom\"         \"conflicted\"    \"cli\"           \"dbplyr\"       \n [5] \"dplyr\"         \"dtplyr\"        \"forcats\"       \"ggplot2\"      \n [9] \"googledrive\"   \"googlesheets4\" \"haven\"         \"hms\"          \n[13] \"httr\"          \"jsonlite\"      \"lubridate\"     \"magrittr\"     \n[17] \"modelr\"        \"pillar\"        \"purrr\"         \"ragg\"         \n[21] \"readr\"         \"readxl\"        \"reprex\"        \"rlang\"        \n[25] \"rstudioapi\"    \"rvest\"         \"stringr\"       \"tibble\"       \n[29] \"tidyr\"         \"xml2\"          \"tidyverse\"    \n\n\nAdemás, existen muchos otros paquetes que siguen la misma filosofía pero no están incluidos por defecto. En esos casos, deben instalarse y activarse individualmente.\n\n\n\n\n\n\nPara profundizar el uso de tidyverse, se recomienda consultar las siguientes fuentes:\n\nSitio oficial: https://www.tidyverse.org/\nLibro R para Ciencia de Datos: r4ds o la nueva versión r4ds 2e (por ahora solo disponible en inglés).\nEpiRhandbook en español",
    "crumbs": [
      "Unidad 1",
      "Introducción a tidyverse"
    ]
  },
  {
    "objectID": "unidad_1/03_intro_tidyverse.html#dataframes-con-tibble",
    "href": "unidad_1/03_intro_tidyverse.html#dataframes-con-tibble",
    "title": "Introducción a tidyverse",
    "section": "\nDataframes con tibble\n",
    "text": "Dataframes con tibble\n\n\n\n\n\nUno de los paquetes que forman parte del núcleo básico de tidyverse es tibble (Müller y Wickham 2023), que introduce una versión moderna del objeto data.frame. Todas las funciones que generan tablas de datos en tidyverse devuelven objetos tibble (tbl_df), los cuales son más eficientes y amigables para el flujo de trabajo.\nLas principales ventajas de trabajar con tibble son:\n\nImpresión en consola más legible y controlada (muestran un número limitado de filas y columnas).\nNo cambian automáticamente el tipo de datos.\nPermiten nombres de columnas con espacios o caracteres especiales si se encierran entre comillas invertidas ` (aunque no se recomienda).\n\nPara crear un objeto tibble manualmente usamos el siguiente código:\n\ndatos &lt;- tibble(\n  nombre = c(\"Ana\", \"Luis\", \"María\"),\n  edad = c(34, 28, 45),\n  altura = c(1.65, 1.80, 1.70)\n)",
    "crumbs": [
      "Unidad 1",
      "Introducción a tidyverse"
    ]
  },
  {
    "objectID": "unidad_1/03_intro_tidyverse.html#tuberías-con-magrittr",
    "href": "unidad_1/03_intro_tidyverse.html#tuberías-con-magrittr",
    "title": "Introducción a tidyverse",
    "section": "Tuberías con magrittr\n",
    "text": "Tuberías con magrittr\n\n\n\n\n\nUna de las incorporaciones más útiles y transversales del ecosistema tidyverse es el uso de “tuberías” o pipe operators. Una tubería conecta un bloque de código con otro, permitiendo encadenar operaciones de manera legible. El operador %&gt;%, proveniente del paquete magrittr (Bache y Wickham 2022), transforma llamadas de funciones anidadas (con múltiples paréntesis) en una secuencia de pasos más simple de leer y escribir.\nA partir de la versión 4.1.0 de R, también se incorporó una tubería nativa (|&gt;), con un comportamiento muy similar. Ambas opciones son válidas y su uso es prácticamente equivalente.\nEste enfoque refleja el principio de que cada función representa un paso en una secuencia lógica de transformación de datos. La forma de trabajar se puede ver en el siguiente esquema general:\n\n\n\n\nA continuación, mostramos un ejemplo comparativo de cómo cambia la sintaxis usando el dataset incorporado en R mtcars, que contiene datos sobre autos:\n\nhead(sqrt(mtcars)) \n\n                       mpg      cyl     disp        hp     drat       wt\nMazda RX4         4.582576 2.449490 12.64911 10.488088 1.974842 1.618641\nMazda RX4 Wag     4.582576 2.449490 12.64911 10.488088 1.974842 1.695582\nDatsun 710        4.774935 2.000000 10.39230  9.643651 1.962142 1.523155\nHornet 4 Drive    4.626013 2.449490 16.06238 10.488088 1.754993 1.793042\nHornet Sportabout 4.324350 2.828427 18.97367 13.228757 1.774824 1.854724\nValiant           4.254409 2.449490 15.00000 10.246951 1.661325 1.860108\n                      qsec vs am     gear     carb\nMazda RX4         4.057093  0  1 2.000000 2.000000\nMazda RX4 Wag     4.125530  0  1 2.000000 2.000000\nDatsun 710        4.313931  1  1 2.000000 1.000000\nHornet 4 Drive    4.409082  1  0 1.732051 1.000000\nHornet Sportabout 4.125530  0  0 1.732051 1.414214\nValiant           4.496665  1  0 1.732051 1.000000\n\n\nEn la línea de código anterior estamos pidiendo mostrar la cabecera (6 primeras observaciones de la tabla de datos) de la raíz cuadrada de los valores de la tabla mtcars, en formato del lenguaje clásico (anidado).\nAhora activamos magrittr y ejecutamos la línea anterior en formato tubería:\n\n# Activa paquete\nlibrary(magrittr) \n\n# Formato tubería\nmtcars %&gt;%\n  sqrt() %&gt;%\n  head()\n\n                       mpg      cyl     disp        hp     drat       wt\nMazda RX4         4.582576 2.449490 12.64911 10.488088 1.974842 1.618641\nMazda RX4 Wag     4.582576 2.449490 12.64911 10.488088 1.974842 1.695582\nDatsun 710        4.774935 2.000000 10.39230  9.643651 1.962142 1.523155\nHornet 4 Drive    4.626013 2.449490 16.06238 10.488088 1.754993 1.793042\nHornet Sportabout 4.324350 2.828427 18.97367 13.228757 1.774824 1.854724\nValiant           4.254409 2.449490 15.00000 10.246951 1.661325 1.860108\n                      qsec vs am     gear     carb\nMazda RX4         4.057093  0  1 2.000000 2.000000\nMazda RX4 Wag     4.125530  0  1 2.000000 2.000000\nDatsun 710        4.313931  1  1 2.000000 1.000000\nHornet 4 Drive    4.409082  1  0 1.732051 1.000000\nHornet Sportabout 4.125530  0  0 1.732051 1.414214\nValiant           4.496665  1  0 1.732051 1.000000\n\n\nPodemos hacer lo mismo con la tubería nativa de R sin activar ningún paquete (revisar que esté activada desde Tools &gt; Global Options):\n\n# Tubería nativa\nmtcars |&gt; \n  sqrt() |&gt; \n  head()\n\n                       mpg      cyl     disp        hp     drat       wt\nMazda RX4         4.582576 2.449490 12.64911 10.488088 1.974842 1.618641\nMazda RX4 Wag     4.582576 2.449490 12.64911 10.488088 1.974842 1.695582\nDatsun 710        4.774935 2.000000 10.39230  9.643651 1.962142 1.523155\nHornet 4 Drive    4.626013 2.449490 16.06238 10.488088 1.754993 1.793042\nHornet Sportabout 4.324350 2.828427 18.97367 13.228757 1.774824 1.854724\nValiant           4.254409 2.449490 15.00000 10.246951 1.661325 1.860108\n                      qsec vs am     gear     carb\nMazda RX4         4.057093  0  1 2.000000 2.000000\nMazda RX4 Wag     4.125530  0  1 2.000000 2.000000\nDatsun 710        4.313931  1  1 2.000000 1.000000\nHornet 4 Drive    4.409082  1  0 1.732051 1.000000\nHornet Sportabout 4.125530  0  0 1.732051 1.414214\nValiant           4.496665  1  0 1.732051 1.000000\n\n\nLas tuberías le dan mucha mas claridad al código separándolo en partes, como si fuesen oraciones de un párrafo.",
    "crumbs": [
      "Unidad 1",
      "Introducción a tidyverse"
    ]
  },
  {
    "objectID": "unidad_1/03_intro_tidyverse.html#lectura-y-escritura-de-datos",
    "href": "unidad_1/03_intro_tidyverse.html#lectura-y-escritura-de-datos",
    "title": "Introducción a tidyverse",
    "section": "Lectura y escritura de datos",
    "text": "Lectura y escritura de datos\nArchivos de texto plano con readr\n\n\n\n\n\nEl paquete readr (Wickham, Hester, y Bryan 2024) contiene funciones similares a las de la familia read.table() de R base, pero desarrollados bajo el ecosistema tidyverse.\nLos archivos de texto plano (ASCII u otras codificaciones) son universalmente utilizados por la mayoría de los gestores de bases de datos y planillas de cálculo. Generalmente se encuentran con extensiones .txt o .csv (por comma-separated values) y son el tipo de archivo de datos más habitual en R.\nEstos datos planos tienen dos características principales:\n\nLa cabecera (en inglés header).\nEl carácter o símbolo separador que indica la separación de columnas: pueden estar separadas por comas, punto y coma, tabulación, etc.\n\nLa presencia o no de una cabecera se maneja con los argumentos col_names y skip:\n\ncol_names = TRUE indica que la primera fila contiene los nombres de las columnas (cabecera).\ncol_names = FALSE indica que no hay cabecera y las columnas se nombran automáticamente (X1, X2, etc).\nskip = 0 (valor por defecto) lee los datos desde la primera fila, pero si hay encabezados complejos (por ejemplo, títulos y subtítulos ), se puede indicar cuántas filas deben omitirse. Ejemplo: skip = 5 omite las primeras 5 filas del archivo.\n\nOtro aspecto a considerar es el carácter separador utilizado para indicar la separación entre columnas. Los separadores más comunes son:\n\ncoma (,)\npunto y coma (;)\ntabulación (TAB)\nespacio (\" \")\nbarra vertical o pipe (|)\n\nFunciones de lectura\nAlgunas de las funciones del paquete asumen un separador particular. Por caso read_csv() lee separados por coma y read_tsv() separado por tabulaciones, pero la función read_delim() permite que definamos el separador a través del argumento delim.\nEn forma detallada el paquete readr soporta siete formatos de archivo a partir de siete funciones:\n\n\nread_csv(): archivos separados por comas (CSV).\n\nread_tsv(): archivos separados por tabulaciones.\n\nread_delim(): archivos separados con delimitadores generales.\n\nread_fwf(): archivos con columnas de ancho fijo.\n\nread_table(): archivos formato tabla con columnas separadas por espacios.\n\nread_log(): archivos log web.\n\nEn comparación con las funciones R base, las funciones de readr:\n\nUsan un esquema de nombres consistente de parámetros.\nSon más rápidas.\nAnalizan eficientemente los formatos de datos comunes (especialmente fechas y horas).\nMuestra una barra de progreso para archivos grandes.\nVienen incluidas dentro de tidyverse pero también pueden usarse de forma independiente:\n\n\nlibrary(readr)\n\nA modo de ejemplo, leeremos un archivo sin cabecera separado por comas bajo el nombre datos:\n\ndatos &lt;- read_csv(\"datos/ejemplo-datos.csv\", \n                  col_names = F)\ndatos\n\n# A tibble: 4 × 5\n     X1 X2       X3       X4    X5        \n  &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;    &lt;chr&gt; &lt;date&gt;    \n1     9 Leone    Fernando M     1958-12-24\n2    26 Garcia   Esteban  M     1954-01-21\n3    35 Salamone Nicolas  M     1993-06-27\n4    48 Gonzalez Viviana  F     1965-06-21\n\n\nLeemos el mismo archivo con cabecera y separado por punto y comas, bajo el nombre info:\n\ninfo &lt;- read_csv2(\"datos/ejemplo-datos-header.csv\",\n                  col_names = T)\ninfo\n\n# A tibble: 4 × 5\n   Iden Apellido Nombre   Sexo  FNac      \n  &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;    &lt;chr&gt; &lt;date&gt;    \n1     9 Leone    Fernando M     1958-12-24\n2    26 Garcia   Laura    M     1954-01-21\n3    35 Salamone Nicolas  M     1993-06-27\n4    48 Gonzalez Viviana  F     1965-06-21\n\n\nEn estos ejemplos:\n\nread_csv() espera comas como separador\nread_csv2() espera punto y coma como separador\n\nAl leer un archivo, readr intenta adivinar automáticamente el tipo de dato de cada columna (parse). Si no hay cabecera, los nombres de columna serán X1, X2, etc.\nPodemos inspeccionar la estructura de un dataframe con glimpse():\n\nglimpse(info)\n\nRows: 4\nColumns: 5\n$ Iden     &lt;dbl&gt; 9, 26, 35, 48\n$ Apellido &lt;chr&gt; \"Leone\", \"Garcia\", \"Salamone\", \"Gonzalez\"\n$ Nombre   &lt;chr&gt; \"Fernando\", \"Laura\", \"Nicolas\", \"Viviana\"\n$ Sexo     &lt;chr&gt; \"M\", \"M\", \"M\", \"F\"\n$ FNac     &lt;date&gt; 1958-12-24, 1954-01-21, 1993-06-27, 1965-06-21\n\n\nLos tipos de datos posibles son:\n\ncharacter (&lt;chr&gt;)\ninteger, double o numeric (&lt;int&gt;, &lt;dbl&gt;)\nlogical (&lt;lgl&gt;)\ndate, datetime, etc.\n\nPor ejemplo, columnas con enteros pueden aparecer como &lt;dbl&gt; si se interpretan como double, y las fechas como &lt;date&gt;.\nAgregamos unos argumentos más y ejemplificamos la sintaxis con read_delim() para leer un archivo con cabecera compleja (la tabla comienza en la fila 9) separado por caracteres | (pipes).\n\nread_delim(\"ejemplo-datos-header-skip.txt\", \n           col_names = T, \n           skip = 8, \n           delim = \"|\")\n\n\nImportante: No olvides asignar la lectura a un nombre para guardar el dataframe dentro del entorno de trabajo (por ejemplo: datos &lt;-).\n\nFunciones de escritura\nEl paquete también incluye funciones para escribir archivos de texto plano, con formatos espejo de las funciones de lectura más comunes:\n\n\nwrite_csv(): escribe archivos separados por comas\n\nwrite_csv2(): escribe archivos separados por punto y comas\n\nwrite_tsv(): escribe archivos separados por tabulaciones\n\nwrite_delim(): escribe archivos separados con delimitadores definidos por el usuario\n\nLos argumentos son generales y para el caso del último más extensos, dado que hay que definir cual es el separador que deseamos en el archivo. Podemos consultarlos con el siguiente código:\n\nargs(write_delim)\n\nfunction (x, file, delim = \" \", na = \"NA\", append = FALSE, col_names = !append, \n    quote = c(\"needed\", \"all\", \"none\"), escape = c(\"double\", \n        \"backslash\", \"none\"), eol = \"\\n\", num_threads = readr_threads(), \n    progress = show_progress(), path = deprecated(), quote_escape = deprecated()) \nNULL\n\n\nPor ejemplo para exportar un conjunto de datos en texto plano al que denominaremos “ejemplo.csv“ con separador punto y coma y cabecera incluida podemos hacer:\n\nwrite_delim(x = datos, file = \"ejemplo.csv\", delim = \";\")\n\no más sencillo, usando la función específica write_csv2():\n\nwrite_csv2(datos, \"ejemplo.csv\") # define cabecera y separador ;\n\nLectura de hojas de cálculo con readxl\n\n\n\n\n\nUno de los formatos más comunes para almacenar datos son las hojas de cálculo, en particular las creadas con Microsoft Excel. El paquete readxl (Wickham y Bryan 2025), parte del ecosistema tidyverse, permite leer este tipo de archivos.\nreadxl es compatible con hojas de cálculo de Excel 97-2003, con extensión .xls, y con versiones más recientes, con extensión .xlsx.\nUna primera función útil es excel_sheets(), que permite conocer y listar los nombres de las hojas contenidas en un archivo Excel (también llamado libro o workbook).\nPor ejemplo, supongamos que tenemos un archivo denominado “datos.xlsx“ y queremos saber por cuantas hojas está compuesto y que nombre tienen:\n\nlibrary(readxl) # hay que activarlo independientemente de tidyverse\n\nexcel_sheets(\"datos/datos.xlsx\")\n\n[1] \"diabetes\"   \"vigilancia\" \"mortalidad\"\n\n\nEsto devuelve, por ejemplo, tres hojas: \"diabetes\", \"vigilancia\" y \"mortalidad\".\nPara leer una de estas hojas utilizamos la función read_excel(), cuyos argumentos principales son:\n\nargs(read_excel)\n\nfunction (path, sheet = NULL, range = NULL, col_names = TRUE, \n    col_types = NULL, na = \"\", trim_ws = TRUE, skip = 0, n_max = Inf, \n    guess_max = min(1000, n_max), progress = readxl_progress(), \n    .name_repair = \"unique\") \nNULL\n\n\nEntre los más relevantes encontramos:\n\npath: nombre del archivo y su ubicación (entre comillas)\nsheet: nombre de la hoja o su número de orden\ncol_names: si es TRUE, toma la primera fila como nombres de las columnas\nskip: permite saltear un número determinado de filas antes de comenzar la lectura\n\nAl ejecutar read_excel(), internamente se utiliza la función excel_format() para detectar si el archivo es .xls o .xlsx, y luego se aplica la función específica para cada caso: read_xls() o read_xlsx(). Estas funciones también pueden usarse directamente si se desea.\nSupongamos ahora que queremos leer la hoja llamada \"diabetes\":\n\ndiabetes &lt;- read_excel(path = \"datos/datos.xlsx\", \n                       sheet = \"diabetes\",\n                       col_names = T)\n\n# mostramos las 6 primeras observaciones\nhead(diabetes)\n\n# A tibble: 6 × 8\n    A1C  hba1 GLUCB   SOG Tol_Glucosa    DM    SM  HOMA\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;       &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1  6.17   7.9   101   122 IFG             0     1  4.04\n2  5.58   7.2   103   100 IFG             0     0  5.03\n3  5.38   7.1   103    90 IFG             0     1  2.92\n4  5.38   6.6   109    96 IFG             0     1  4.79\n5  5.19   6.3   107    69 IFG             0     1  3.06\n6  4.89   6      NA   117 IFG             0     0  5.77\n\n\nObservemos que en los argumentos escribimos el nombre del archivo que se encuentra en nuestro proyecto y por lo tanto en nuestra carpeta activa, el nombre de la hoja y nos aseguramos que la primer fila representa a la cabecera de la tabla (sus nombres de variables).\nComo readxl forma parte del ecosistema tidyverse el formato de salida es un tibble. En este caso de 23 observaciones por 8 variables.\nAhora leamos la segunda hoja de nombre \"vigilancia\":\n\nvigilancia &lt;- read_excel(path = \"datos/datos.xlsx\", \n                         sheet = 2, \n                         col_names = F)\n\n# mostramos las 6 primeras observaciones\nhead(vigilancia)\n\n# A tibble: 6 × 9\n   ...1 ...2        ...3      ...4  ...5  ...6  ...7 ...8  ...9                 \n  &lt;dbl&gt; &lt;chr&gt;      &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;                \n1   875 09/28/2015  2015 544080000     1    31     1 F     VIGILANCIA EN SALUD …\n2   875 42317       2015 544080000     1    35     1 F     VIGILANCIA EN SALUD …\n3   875 42317       2015 544080000     1    47     1 F     VIGILANCIA EN SALUD …\n4   307 09/26/2015  2015 544005273     1    23     1 M     VIGILANCIA INTEGRADA…\n5   307 09/24/2015  2015 544005273     1    19     1 M     VIGILANCIA INTEGRADA…\n6   875 09/28/2015  2015 544080000     1    63     1 F     VIGILANCIA EN SALUD …\n\n\nEn este caso, en lugar del nombre de la hoja usamos un 2 que es su ubicación y especificamos col_names = FALSE porque el conjunto de datos no tiene cabecera. readxl asignará nombres genéricos como ...1, ...2, etc.\nFinalmente leamos la última hoja disponible del archivo:\n\nmortalidad &lt;- read_excel(path = \"datos/datos.xlsx\", \n                         sheet = \"mortalidad\",\n                         col_names = T, \n                         skip = 1)\n\n# mostramos las 6 primeras observaciones\nhead(mortalidad) \n\n# A tibble: 5 × 10\n  grupo_edad grupo.I.1.1 grupo.II.1.1 grupo.III.1.1 grupo.I.2.1 grupo.II.2.1\n  &lt;chr&gt;            &lt;dbl&gt;        &lt;dbl&gt;         &lt;dbl&gt;       &lt;dbl&gt;        &lt;dbl&gt;\n1 30-44               41          202           222         539         1438\n2 45-59               99         1071           181         759         6210\n3 60-69              114         1782           119         985         9238\n4 70-79              221         2336           119        1571        12369\n5 80+                362         2492            81        2523        14642\n# ℹ 4 more variables: grupo.III.2.1 &lt;dbl&gt;, grupo.I.3.1 &lt;dbl&gt;,\n#   grupo.II.3.1 &lt;dbl&gt;, grupo.III.3.1 &lt;dbl&gt;\n\n\nLo novedoso de esta lectura es el argumento skip = 1 que debimos incorporar dado que, en este caso, la hoja de Excel comienza con una línea de título que no pertenece al conjunto de datos. También que el argumento sheet permite el nombre de la hoja elegida entre comillas.\nAdemás de los argumentos generales de read_xl(), podemos mencionar estos otros:\n\nn_max: número máximo de filas a leer.\nrange: rango de celdas a importar (como en Excel, por ejemplo \"B3:D87\").\ncol_types: define el tipo de datos de cada columna. Valores posibles: \"numeric\", \"logical\", \"text\", \"date\", \"skip\" (no leer la columna), \"guess\" (modo predeterminado: la función decide automáticamente el tipo).\nna: carácter o vector de caracteres que se deben interpretar como valores perdidos (NA). Por defecto, las celdas vacías se interpretan así.",
    "crumbs": [
      "Unidad 1",
      "Introducción a tidyverse"
    ]
  },
  {
    "objectID": "unidad_1/03_intro_tidyverse.html#gestión-de-datos-con-dplyr",
    "href": "unidad_1/03_intro_tidyverse.html#gestión-de-datos-con-dplyr",
    "title": "Introducción a tidyverse",
    "section": "Gestión de datos con dplyr\n",
    "text": "Gestión de datos con dplyr\n\n\n\n\n\nEl paquete dplyr (Wickham et al. 2023) fue desarrollado por Hadley Wickham como una versión optimizada del paquete plyr (Wickham 2011).\nSu principal contribución es ofrecer una gramática para la manipulación de datos, basada en funciones que actúan como verbos, lo que facilita la lectura y comprensión del código.\nLas funciones clave del paquete permiten realizar las siguientes acciones (verbos):\n\n\nselect(): selecciona un conjunto de columnas (variables)\n\nrename(): renombra variables en un conjunto de datos\n\nfilter(): selecciona un conjunto de filas (observaciones) según una o varias condiciones lógicas\n\narrange(): reordena las filas de un conjunto de datos\n\nmutate(): añade nuevas variables/columnas o transforma variables existentes\n\nsummarise()/summarize(): genera resúmenes estadísticos de diferentes variables en el conjunto de datos\n\ngroup_by(): agrupa las observaciones en función de una o más variables, lo que permite realizar operaciones por grupo\n\ncount(): contabiliza valores que se repiten, generando una tabla de frecuencias\n\nAdemás, al ser parte del ecosistema tidyverse, dplyr integra al operador %&gt;% (pipe) formando una única secuencia de procesamiento o pipeline.\nArgumentos comunes en las funciones dplyr\n\nTodas las funciones, básicamente, tienen en común una serie de argumentos.\n\nEl primer argumento es el nombre del conjunto de datos (objeto donde esta nuestra tabla de datos).\nLos otros argumentos describen que hacer con el conjunto de datos especificado en el primer argumento, podemos referirnos a las columnas en el objeto directamente sin utilizar el operador $, es decir sólo con el nombre de la columna/variable.\nEl valor de retorno es un nuevo conjunto de datos.\nLos conjuntos de datos deben estar bien organizados/estructurados, es decir debe existir una observación por columna y, cada columna representar una variable, medida o característica de esa observación. Es decir, debe cumplir con tidy data.\nActivación del paquete\ndplyr está incluído en el núcleo base de tidyverse, por lo que se encuentra disponible si tenemos activado a este último.\nTambién se puede activar en forma independiente:\n\nlibrary(dplyr)\n\nConjunto de datos para ejemplo\nPara visualizar y comprender el funcionamiento de estos “verbos” de manipulación, resulta muy útil contar con ejemplos concretos. Por eso, en esta unidad trabajaremos con un conjunto de datos que nos permitirá practicar el uso de las funciones del paquete.\n\nRecuerden que pueden descargar los datos utilizados en los ejemplos del curso y descomprimirlos en la carpeta donde tengan guardado su proyecto de RStudio.\n\nUno de los archivos incluidos, “noti-vih.csv”, contiene registros de notificaciones de VIH por jurisdicción en Argentina correspondientes a los años 2015 y 2016.\n\n# asignamos la lectura a datos\ndatos &lt;- read_csv(\"datos/noti-vih.csv\") \n\n# mostramos las 6 primeras observaciones\nhead(datos)\n\n# A tibble: 6 × 4\n  jurisdiccion   año casos      pob\n  &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n1 Buenos Aires  2015  1513 16626374\n2 Buenos Aires  2016   957 16789474\n3 CABA          2015   901  3054237\n4 CABA          2016   427  3050000\n5 Catamarca     2015    69   396552\n6 Catamarca     2016    51   401575\n\n\nFunción select()\n\nLa función select() permite elegir columnas específicas de un conjunto de datos, devolviendo una versión “recortada por columnas” del mismo.\nA continuación, exploramos algunas formas útiles de seleccionar variables:\nSeleccionar todas las variables excepto pob:\n\ndatos |&gt; \n  select(-pob)\n\n# A tibble: 48 × 3\n   jurisdiccion   año casos\n   &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt;\n 1 Buenos Aires  2015  1513\n 2 Buenos Aires  2016   957\n 3 CABA          2015   901\n 4 CABA          2016   427\n 5 Catamarca     2015    69\n 6 Catamarca     2016    51\n 7 Chaco         2015    15\n 8 Chaco         2016     9\n 9 Chubut        2015   110\n10 Chubut        2016    89\n# ℹ 38 more rows\n\n\nOtra forma para el mismo resultado anterior (mediante el operador rango :):\n\ndatos |&gt; \n  select(jurisdiccion:casos)\n\n# A tibble: 48 × 3\n   jurisdiccion   año casos\n   &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt;\n 1 Buenos Aires  2015  1513\n 2 Buenos Aires  2016   957\n 3 CABA          2015   901\n 4 CABA          2016   427\n 5 Catamarca     2015    69\n 6 Catamarca     2016    51\n 7 Chaco         2015    15\n 8 Chaco         2016     9\n 9 Chubut        2015   110\n10 Chubut        2016    89\n# ℹ 38 more rows\n\n\nSeleccionar solamente las variables jurisdiccion y casos:\n\ndatos |&gt; \n  select(jurisdiccion, casos)\n\n# A tibble: 48 × 2\n   jurisdiccion casos\n   &lt;chr&gt;        &lt;dbl&gt;\n 1 Buenos Aires  1513\n 2 Buenos Aires   957\n 3 CABA           901\n 4 CABA           427\n 5 Catamarca       69\n 6 Catamarca       51\n 7 Chaco           15\n 8 Chaco            9\n 9 Chubut         110\n10 Chubut          89\n# ℹ 38 more rows\n\n\nLo mismo que el ejemplo anterior, pero usando la posición de las columnas:\n\ndatos |&gt; \n  select(1, 3)\n\n# A tibble: 48 × 2\n   jurisdiccion casos\n   &lt;chr&gt;        &lt;dbl&gt;\n 1 Buenos Aires  1513\n 2 Buenos Aires   957\n 3 CABA           901\n 4 CABA           427\n 5 Catamarca       69\n 6 Catamarca       51\n 7 Chaco           15\n 8 Chaco            9\n 9 Chubut         110\n10 Chubut          89\n# ℹ 38 more rows\n\n\nMover la variable año al inicio y mantener todas las demás:\n\ndatos |&gt; \n  select(\"año\", everything())\n\n# A tibble: 48 × 4\n     año jurisdiccion casos      pob\n   &lt;dbl&gt; &lt;chr&gt;        &lt;dbl&gt;    &lt;dbl&gt;\n 1  2015 Buenos Aires  1513 16626374\n 2  2016 Buenos Aires   957 16789474\n 3  2015 CABA           901  3054237\n 4  2016 CABA           427  3050000\n 5  2015 Catamarca       69   396552\n 6  2016 Catamarca       51   401575\n 7  2015 Chaco           15  1153846\n 8  2016 Chaco            9  1125000\n 9  2015 Chubut         110   567010\n10  2016 Chubut          89   577922\n# ℹ 38 more rows\n\n\nOtros posibles argumentos son:\n\nstarts_with(): selecciona todas las columnas que comiencen con el patrón indicado.\nends_with(): selecciona todas las columnas que terminen con el patrón indicado.\ncontains(): selecciona las columnas que posean el patrón indicado.\nmatches(): similar a contains(), pero permite poner una expresión regular.\nall_of(): selecciona las variables pasadas en un vector (todos los nombres deben estar presentes o devuelve un error).\nany_of(): idem anterior excepto que no se genera ningún error para los nombres que no existen.\nnum_range(): selecciona variables con nombre combinados con caracteres y números (ejemplo: num_range(\"x\", 1:3) selecciona las variables x1, x2 y x3.\nwhere(): aplica una función a todas las variables y selecciona aquellas para las cuales la función regresa TRUE (por ejemplo: is.numeric() para seleccionar todas las variables numéricas).\nFunción rename()\n\nLa función rename() puede considerarse una extensión de select(). Si bien select() también permite renombrar variables, no resulta muy útil para este fin, ya que descarta todas las variables que no se mencionan explícitamente.\nEn cambio, rename() permite cambiar el nombre de una o más variables sin eliminar las demás. Solo se modifican los nombres indicados, y el resto del conjunto de datos permanece sin cambios.\nEjemplo: renombrar la variable pob como población:\n\ndatos |&gt;\n  rename(\"población\" = pob)\n\n# A tibble: 48 × 4\n   jurisdiccion   año casos población\n   &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;\n 1 Buenos Aires  2015  1513  16626374\n 2 Buenos Aires  2016   957  16789474\n 3 CABA          2015   901   3054237\n 4 CABA          2016   427   3050000\n 5 Catamarca     2015    69    396552\n 6 Catamarca     2016    51    401575\n 7 Chaco         2015    15   1153846\n 8 Chaco         2016     9   1125000\n 9 Chubut        2015   110    567010\n10 Chubut        2016    89    577922\n# ℹ 38 more rows\n\n\nFunción filter()\n\nLa función filter() permite seleccionar filas de un conjunto de datos, produciendo un subconjunto de observaciones.\nVeamos un ejemplo sencillo con nuestros datos:\n\ndatos |&gt;\n  filter(jurisdiccion == \"Tucuman\")\n\n# A tibble: 2 × 4\n  jurisdiccion   año casos     pob\n  &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;\n1 Tucuman       2015   258 1592593\n2 Tucuman       2016   246 1618421\n\n\nUtiliza los mismos operadores de comparación propios del lenguaje R:\n\n\n\n\nOperador\nDescripción\n\n\n\n&lt;\nMenor que\n\n\n&gt;\nMenor que\n\n\n&lt;=\nMenor o igual que\n\n\n&gt;=\nMayor o igual que\n\n\n==\nIgual que\n\n\n!=\nNo igual que\n\n\n%in%\nEs parte de\n\n\nis.na()\nEs un valor ausente\n\n\n!is.na()\nNo es un valor ausente\n\n\n\n\n\nLo mismo con los operadores lógicos que se utilizan como conectores entre las expresiones:\n\n\n\n\nOperador\nDescripción\n\n\n\n&\nAND booleano\n\n\n|\nOR booleano\n\n\nxor()\nOR exclusivo\n\n\n!\nNOT\n\n\nany()\ncualquier TRUE\n\n\nall()\ntodos TRUE\n\n\n\n\n\nCuando usamos múltiples argumentos separados por coma dentro de filter(), estas se combinan con un operador AND implícito, es decir, cada expresión debe ser verdadera para que la fila sea incluida en la salida.\nPor ejemplo, filtramos las observaciones que cumplan que casos sea mayor a 100 y que pob sea menor a 1.000.000:\n\ndatos |&gt;\n  filter(casos &gt; 100, pob &lt; 1000000)\n\n# A tibble: 7 × 4\n  jurisdiccion   año casos    pob\n  &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n1 Chubut        2015   110 567010\n2 Jujuy         2015   160 727273\n3 Jujuy         2016   133 734807\n4 Neuquen       2015   109 619318\n5 Neuquen       2016   101 627329\n6 Rio Negro     2015   112 700000\n7 Rio Negro     2016   105 709459\n\n\nPara combinar condiciones dentro de una misma variable usamos el operador OR (|) o, de forma más práctica, %in%:\n\n# Con OR\ndatos |&gt;\n  filter(jurisdiccion == \"Buenos Aires\" | jurisdiccion == \"La Pampa\")\n\n# A tibble: 4 × 4\n  jurisdiccion   año casos      pob\n  &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n1 Buenos Aires  2015  1513 16626374\n2 Buenos Aires  2016   957 16789474\n3 La Pampa      2015    57   343373\n4 La Pampa      2016    67   345361\n\n# Con %in%\ndatos |&gt;\n  filter(jurisdiccion %in% c(\"Buenos Aires\", \"La Pampa\"))\n\n# A tibble: 4 × 4\n  jurisdiccion   año casos      pob\n  &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n1 Buenos Aires  2015  1513 16626374\n2 Buenos Aires  2016   957 16789474\n3 La Pampa      2015    57   343373\n4 La Pampa      2016    67   345361\n\n\nEn el siguiente ejemplo, filtramos observaciones del año 2016 con más de 200 casos. El uso de & es equivalente al uso de coma:\n\ndatos |&gt;\n  filter(año == \"2016\" & casos &gt; 200)\n\n# A tibble: 6 × 4\n  jurisdiccion   año casos      pob\n  &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n1 Buenos Aires  2016   957 16789474\n2 CABA          2016   427  3050000\n3 Cordoba       2016   368  3607843\n4 Mendoza       2016   254  1909774\n5 Salta         2016   230  1352941\n6 Tucuman       2016   246  1618421\n\n\nPor último, podemos usar xor() para seleccionar observaciones que cumplan solo una de las condiciones, pero no ambas. Por ejemplo, el siguiente filtro selecciona registros donde el año sea 2016 ó los casos sean mayores a 200, pero no ambos al mismo tiempo (es decir que no se den ambos en TRUE):\n\ndatos |&gt; \n  filter(xor(año == \"2016\", casos &gt; 200))\n\n# A tibble: 25 × 4\n   jurisdiccion   año casos      pob\n   &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n 1 Buenos Aires  2015  1513 16626374\n 2 CABA          2015   901  3054237\n 3 Catamarca     2016    51   401575\n 4 Chaco         2016     9  1125000\n 5 Chubut        2016    89   577922\n 6 Cordoba       2015   468  3572519\n 7 Corrientes    2016    99  1076087\n 8 Entre Rios    2016   109  1329268\n 9 Formosa       2016    60   582524\n10 Jujuy         2016   133   734807\n# ℹ 15 more rows\n\n\nFunción arrange()\n\nLa función arrange() se utiliza para ordenar las filas de un conjunto de datos de acuerdo a una o varias columnas/variables. Por defecto, el ordenamiento es ascendente alfanumérico.\nOrdenamos la tabla por la variable pob (forma ascendente predeterminada):\n\ndatos |&gt;\n  arrange(pob)\n\n# A tibble: 48 × 4\n   jurisdiccion       año casos    pob\n   &lt;chr&gt;            &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n 1 Tierra del Fuego  2015    36 152542\n 2 Tierra del Fuego  2016    34 156682\n 3 Santa Cruz        2015    65 320197\n 4 Santa Cruz        2016    59 329609\n 5 La Pampa          2015    57 343373\n 6 La Pampa          2016    67 345361\n 7 La Rioja          2015    41 369369\n 8 La Rioja          2016     6 375000\n 9 Catamarca         2015    69 396552\n10 Catamarca         2016    51 401575\n# ℹ 38 more rows\n\n\nPara ordenar en forma descendente podemos utilizar desc() dentro de los argumentos de arrange():\n\ndatos |&gt;\n  arrange(desc(pob))\n\n# A tibble: 48 × 4\n   jurisdiccion   año casos      pob\n   &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n 1 Buenos Aires  2016   957 16789474\n 2 Buenos Aires  2015  1513 16626374\n 3 Cordoba       2016   368  3607843\n 4 Cordoba       2015   468  3572519\n 5 Santa Fe      2016   170  3400000\n 6 Santa Fe      2015   301  3382022\n 7 CABA          2015   901  3054237\n 8 CABA          2016   427  3050000\n 9 Mendoza       2016   254  1909774\n10 Mendoza       2015   316  1880952\n# ℹ 38 more rows\n\n\nPodemos combinar ordenamientos. Por ejemplo, en forma alfabética ascendente para jusrisdiccion y luego numérica descendente para casos:\n\ndatos |&gt;\n  arrange(jurisdiccion, desc(casos))\n\n# A tibble: 48 × 4\n   jurisdiccion   año casos      pob\n   &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n 1 Buenos Aires  2015  1513 16626374\n 2 Buenos Aires  2016   957 16789474\n 3 CABA          2015   901  3054237\n 4 CABA          2016   427  3050000\n 5 Catamarca     2015    69   396552\n 6 Catamarca     2016    51   401575\n 7 Chaco         2015    15  1153846\n 8 Chaco         2016     9  1125000\n 9 Chubut        2015   110   567010\n10 Chubut        2016    89   577922\n# ℹ 38 more rows\n\n\nFunción mutate()\n\nEsta función nos permite transformar variables dentro de un conjunto de datos. A menudo tendremos la necesidad de modificar variables existentes o crear nuevas variables a partir de las ya disponibles. La función mutate() nos ofrece una forma clara y eficiente de realizar este tipo de operaciones.\nPor ejemplo, podríamos querer calcular tasas crudas para cada jurisdicción y año, en función del número de casos y de la población total:\n\ndatos |&gt;\n  mutate(\n    tasa = casos/pob*100000\n  )\n\n# A tibble: 48 × 5\n   jurisdiccion   año casos      pob  tasa\n   &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;\n 1 Buenos Aires  2015  1513 16626374  9.10\n 2 Buenos Aires  2016   957 16789474  5.70\n 3 CABA          2015   901  3054237 29.5 \n 4 CABA          2016   427  3050000 14   \n 5 Catamarca     2015    69   396552 17.4 \n 6 Catamarca     2016    51   401575 12.7 \n 7 Chaco         2015    15  1153846  1.30\n 8 Chaco         2016     9  1125000  0.8 \n 9 Chubut        2015   110   567010 19.4 \n10 Chubut        2016    89   577922 15.4 \n# ℹ 38 more rows\n\n\nEn este caso, mutate() calcula la tasa cruda por 100.000 habitantes e incorpora una nueva variable (tasa) con los resultados correspondientes a cada observación.\nTambién se pueden construir múltiples variables en la misma expresión, solamente separadas por comas:\n\ndatos |&gt;\n  mutate(\n    tasaxcien_mil = casos/pob*100000,\n    tasaxdiez_mil = casos/pob*10000\n  )\n\n# A tibble: 48 × 6\n   jurisdiccion   año casos      pob tasaxcien_mil tasaxdiez_mil\n   &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;         &lt;dbl&gt;         &lt;dbl&gt;\n 1 Buenos Aires  2015  1513 16626374          9.10         0.910\n 2 Buenos Aires  2016   957 16789474          5.70         0.570\n 3 CABA          2015   901  3054237         29.5          2.95 \n 4 CABA          2016   427  3050000         14            1.4  \n 5 Catamarca     2015    69   396552         17.4          1.74 \n 6 Catamarca     2016    51   401575         12.7          1.27 \n 7 Chaco         2015    15  1153846          1.30         0.130\n 8 Chaco         2016     9  1125000          0.8          0.08 \n 9 Chubut        2015   110   567010         19.4          1.94 \n10 Chubut        2016    89   577922         15.4          1.54 \n# ℹ 38 more rows\n\n\nSi deseamos que estas nuevas variables se incorporen de forma permanente al conjunto de datos (y no solo se muestren en la consola), debemos utilizar el operador de asignación &lt;-:\n\ndatos &lt;- datos |&gt;\n  mutate(\n    tasaxcien_mil = casos/pob*100000,\n    tasaxdiez_mil = casos/pob*10000\n  )\n\nUn aspecto fundamental es que las funciones utilizadas dentro de mutate() deben estar vectorizadas: deben aceptar un vector de entrada y devolver otro vector del mismo tamaño como salida.\nExisten muchas funciones que se pueden utilizar dentro de mutate(). A continuación se presentan algunas útiles:\n\nOperadores aritméticos: +, -, *, /, ^.\nAritmética modular: %/% (división entera) y %% (resto), donde x == y * (x %/% y) + (x %% y). Esta herramienta resulta útil para dividir números enteros en porciones.\nFunciones matemáticas: log(), log2(), log10(), exp(), sqrt(), abs(), entre otras.\nValores acumulados: R ofrece funciones como cumsum(), cumprod(), cummin(), cummax() y dplyr incluye cummean() para promedios acumulados.\nClasificación o ranking: funciones como min_rank() permiten asignar rangos (1º, 2º, etc.). Por defecto, los valores más pequeños reciben rangos más bajos. Si se desea invertir el orden, puede utilizarse desc(x).\n\nFinalmente, si se utiliza en mutate() el mismo nombre de una variable que ya existe en la tabla, dicha variable será sobrescrita (por ejemplo, al cambiarle el tipo de character a factor). Si se desea crear una variable nueva, se debe utilizar un nombre que no esté previamente en el conjunto de datos.\nFunción summarise()\n\nLa función summarise() o summarize() se utiliza para calcular resúmenes estadísticos a partir de una o más variables de un conjunto de datos.\nPor ejemplo, podemos calcular el promedio y el total de casos:\n\ndatos |&gt;\n  summarise(\n    promedio_casos = mean(casos), \n    casos_totales = sum(casos)\n  )\n\n# A tibble: 1 × 2\n  promedio_casos casos_totales\n           &lt;dbl&gt;         &lt;dbl&gt;\n1           192.          9211\n\n\nSu uso es muy interesante cuando la combinamos con group_by() (función que detallaremos luego). Esta situación permite estratificar los resultados por grupos específicos.\nPor ejemplo, podemos agrupar el por año y simultáneamente aplicar el mismo summarise() anterior:\n\ndatos |&gt; \n  group_by(año) |&gt; \n  summarise(\n    promedio_casos = mean(casos), \n    casos_totales = sum(casos)\n  )\n\n# A tibble: 2 × 3\n    año promedio_casos casos_totales\n  &lt;dbl&gt;          &lt;dbl&gt;         &lt;dbl&gt;\n1  2015           224.          5369\n2  2016           160.          3842\n\n\nEl resultado es una tabla con dos filas, una para cada grupo (año 2015 y año 2016) con los valores promedio y casos totales respectivos.\nAlgunas de las funciones del R base que se pueden utilizar dentro de los argumentos de esta función son:\n\n\nmin(): mínimo\n\nmax(): máximo\n\nmean(): media\n\nmedian(): mediana\n\nvar(): varianza\n\nsd(): desvío\n\nsum(): sumatoria\n\nOtras funciones que se pueden incorporar las provee el mismo paquete dplyr, por ejemplo:\n\n\nfirst(): primer valor en el vector.\n\nlast(): último valor en el vector.\n\nn(): número de valores en el vector.\n\nn_distinct(): números de valores distintos en el vector.\nFunción group_by()\n\nComo mencionamos anteriormente, la función group_by() resulta especialmente útil cuando se utiliza en combinación con summarise(), dado que agrupa un conjunto de filas seleccionado según los valores de una o más columnas antes de aplicar funciones de resumen.\nAl aplicar group_by(), el conjunto de datos se estructura internamente en subgrupos definidos por las variables indicadas. Las funciones que se apliquen a continuación (por ejemplo, summarise() o mutate()) se ejecutarán dentro de cada grupo de forma independiente.\nPor ejemplo, podemos calcular las tasas crudas por 100.000 habitantes para cada combinación de jurisdicción y año:\n\ndatos |&gt;\n  group_by(jurisdiccion, año) |&gt; \n  summarise(tasa = casos/pob*100000)\n\n# A tibble: 48 × 3\n# Groups:   jurisdiccion [24]\n   jurisdiccion   año  tasa\n   &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt;\n 1 Buenos Aires  2015  9.10\n 2 Buenos Aires  2016  5.70\n 3 CABA          2015 29.5 \n 4 CABA          2016 14   \n 5 Catamarca     2015 17.4 \n 6 Catamarca     2016 12.7 \n 7 Chaco         2015  1.30\n 8 Chaco         2016  0.8 \n 9 Chubut        2015 19.4 \n10 Chubut        2016 15.4 \n# ℹ 38 more rows\n\n\nEn la mayoría de estos ejemplos, la salida es directa, es decir, no construimos nuevos objetos. Sin embargo, en muchas situaciones vamos a necesitar conservar los resultados obtenidos, asignándolos a un nuevo objeto.\nAdemás, si en algún momento aplicamos group_by() y luego queremos continuar trabajando con los datos sin agrupamientos, podemos utilizar la función ungroup(), que elimina la estructura de agrupamiento:\n\ndatos |&gt; \n  group_by(jurisdiccion, año) |&gt; \n  summarise(tasa = casos / pob * 100000) |&gt; \n  ungroup()\n\n# A tibble: 48 × 3\n   jurisdiccion   año  tasa\n   &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt;\n 1 Buenos Aires  2015  9.10\n 2 Buenos Aires  2016  5.70\n 3 CABA          2015 29.5 \n 4 CABA          2016 14   \n 5 Catamarca     2015 17.4 \n 6 Catamarca     2016 12.7 \n 7 Chaco         2015  1.30\n 8 Chaco         2016  0.8 \n 9 Chubut        2015 19.4 \n10 Chubut        2016 15.4 \n# ℹ 38 more rows\n\n\nEsto resulta útil cuando queremos realizar otras operaciones posteriores que no dependen de los grupos definidos previamente.\nCombinaciones\nEn los ejemplos anteriores vimos cómo se van integrando algunas de las funciones mediante el uso del operador de tubería %&gt;% o |&gt;. La idea detrás de esta “gramática de los datos” que propone el paquete dplyr es poder encadenar acciones de forma legible y lógica, construyendo oraciones más complejas paso a paso.\nVeamos un ejemplo que integra muchas de las funciones vistas hasta ahora:\n\nObtener una nueva tabla con las tasas crudas de casos notificados de VIH, por año y jurisdicción, mayores a 20 por 100.000 habitantes, ordenadas de mayor a menor.\n\n\ndatos |&gt;                                  # siempre partimos de los datos\n  group_by(año, jurisdiccion) |&gt;          # agrupamos\n  summarise(tasa = casos/pob*100000) |&gt;   # resumimos\n  filter(tasa &gt; 20) |&gt;                    # filtramos\n  arrange(desc(tasa))                     # ordenamos   \n\n# A tibble: 5 × 3\n# Groups:   año [2]\n    año jurisdiccion      tasa\n  &lt;dbl&gt; &lt;chr&gt;            &lt;dbl&gt;\n1  2015 CABA              29.5\n2  2015 Tierra del Fuego  23.6\n3  2015 Jujuy             22.0\n4  2016 Tierra del Fuego  21.7\n5  2015 Santa Cruz        20.3\n\n\nUna buena práctica para construir este tipo de código es escribir cada paso de la operación en una línea separada. Esto no solo mejora la legibilidad, sino que facilita la identificación de errores o la modificación de pasos específicos.\nEste ejemplo muestra claramente el poder y la claridad que se logran al combinar funciones como group_by(), summarise(), filter() y arrange() en una misma operación fluida y coherente.\nFunción count()\n\nEsta función permite contar rápidamente los valores únicos de una o más variables en un conjunto de datos. Es especialmente útil para crear tablas de frecuencias absolutas, que posteriormente pueden ser usadas para calcular frecuencias relativas.\nUn ejemplo básico de su uso es contar las observaciones por cada valor único de la variable jurisdiccion en el conjunto de datos:\n\ndatos |&gt;\n  count(jurisdiccion)\n\n# A tibble: 24 × 2\n   jurisdiccion     n\n   &lt;chr&gt;        &lt;int&gt;\n 1 Buenos Aires     2\n 2 CABA             2\n 3 Catamarca        2\n 4 Chaco            2\n 5 Chubut           2\n 6 Cordoba          2\n 7 Corrientes       2\n 8 Entre Rios       2\n 9 Formosa          2\n10 Jujuy            2\n# ℹ 14 more rows\n\n\nTiene un par de argumentos opcionales:\n\nname: Define el nombre de la columna que contendrá el conteo. Por defecto, esta columna se llama n.\nsort: Ordena la tabla de frecuencias de mayor a menor (por defecto, no realiza ninguna ordenación).\nwt: Permite incorporar una variable que funcione como ponderación (o factor de expansión) para el cálculo de la frecuencia.",
    "crumbs": [
      "Unidad 1",
      "Introducción a tidyverse"
    ]
  },
  {
    "objectID": "unidad_1/03_intro_tidyverse.html#gráficos-estadísticos-con-ggplot2",
    "href": "unidad_1/03_intro_tidyverse.html#gráficos-estadísticos-con-ggplot2",
    "title": "Introducción a tidyverse",
    "section": "Gráficos estadísticos con ggplot2\n",
    "text": "Gráficos estadísticos con ggplot2\n\n\n\n\n\nEl paquete ggplot2 (Wickham 2016) se autodefine como una librería para “crear elegantes visualizaciones de datos utilizando una gramática de gráficos”. Proporciona una forma intuitiva de construir gráficos basada en The Grammar of Graphics a través de un sistema basado en tres componentes básicos:\n\ndatos\ncoordenadas\nobjetos geométricos\n\nLa estructura para construir un gráfico es la siguiente:\n\n\n\n\nAnatomía de gráficos con ggplot2\n\nLa estructura básica para construir un gráfico con ggplot2 se organiza a partir de una gramática de gráficos, que se puede entender a través de sus componentes fundamentales:\n\n\n\n\n\n\ndata: el conjunto de datos que vamos a graficar, que debe contener toda la información necesaria para crear el gráfico.\n\naes(): el mapeo estético (aesthetic mapping) es donde se declaran las variables que se van a mapear en el gráfico (por ejemplo, qué variable va en el eje X, en el eje Y, o cómo se asignan los colores).\n\ngeoms: representaciones gráficas de los datos, como puntos, líneas, barras, cajas, entre otros. Son los “objetos geométricos” que realmente dibujan el gráfico.\n\nstats: Transformaciones estadísticas que se realizan sobre los datos, como el cálculo de medias, medias móviles o regresiones, que ayudan a hacer un resumen de los datos para visualizarlos mejor.\n\nscales: se utilizan para colorear o escalar los datos según distintas variables. Controlan los ejes y las leyendas.\n\ncoordinate systems: es el sistema de coordenadas para el mapeo del gráfico en un plano bidimensional.\n\nfacets: permiten dividir el conjunto de datos según factores y crear gráficos en paneles separados (viñetas), creando matrices gráficas.\n\ntheme: son conjuntos de características gráficas que permiten controlar la apariencia general de todos los elementos que no son datos, como el color del fondo, el tipo de fuente o los bordes.\n\nAntes de comenzar a mostrar cómo se usan estos componentes en un gráfico, leemos la base de datos de ejemplo “facultad.csv”, que contiene datos ficticios sobre ingresantes a una facultad (por ejemplo, sexo, edad, talla y peso). Usaremos este conjunto de datos para ilustrar los ejemplos gráficos:\n\n# Cargar datos\nfacultad &lt;- read_csv(\"datos/facultad.csv\")\n\n# Mostramos las 6 primeras observaciones\nhead(facultad) \n\n# A tibble: 6 × 18\n     hc sexo   edad ant_diabetes ant_tbc ant_cancer ant_obesidad ant_ecv ant_ht\n  &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt;        &lt;chr&gt;   &lt;chr&gt;      &lt;chr&gt;        &lt;chr&gt;   &lt;chr&gt; \n1 26880 M        17 NO           NO      NO         SI           NO      SI    \n2 26775 M        18 SI           NO      NO         NO           NO      NO    \n3 26877 M        18 SI           NO      SI         NO           NO      SI    \n4 26776 M        18 NO           NO      NO         SI           SI      NO    \n5 26718 M        18 NO           NO      NO         NO           NO      SI    \n6 26738 M        18 NO           NO      NO         NO           NO      SI    \n# ℹ 9 more variables: ant_col &lt;chr&gt;, fuma &lt;chr&gt;, edadini &lt;dbl&gt;, cantidad &lt;dbl&gt;,\n#   col &lt;dbl&gt;, peso &lt;dbl&gt;, talla &lt;dbl&gt;, sist &lt;dbl&gt;, diast &lt;dbl&gt;\n\n\nMapeo estético con aes() y capas geométricas (geom_)\nDecíamos que la función aes() hace referencia al contenido estético del gráfico. Es decir, le brinda indicaciones a ggplot2 sobre cómo dibujar los distintos elementos del gráfico: líneas, formas, colores y tamaños.\nEs importante notar que aes() crea una nueva capa vinculada a las variables que se desean mapear, y también agrega automáticamente leyendas cuando corresponde. Al incorporar aes() dentro del llamado a ggplot(), estamos compartiendo la información estética con todas las capas del gráfico. Si deseamos que esa información sólo esté en una de las capas, debemos usar aes() en la capa correspondiente.\nVeamos cómo funciona y cuáles son sus implicancias:\n\nfacultad |&gt;\n  # solo la capa estética aes()\n  ggplot(aes(x = talla, y = peso)) \n\n\n\n\n\n\n\nEste código genera un gráfico vacío que contiene únicamente los ejes especificados (peso y talla), pero aún no muestra los datos. Para visualizar los puntos, debemos agregar una capa geométrica usando geom_point() y enlazarla con el símbolo +:\n\nfacultad |&gt;\n  # mapeo estético\n  ggplot(aes(x = talla, y = peso))  + \n  \n  # agregamos la capa geométrica de puntos\n  geom_point()                    \n\n\n\n\n\n\n\nPodemos diferenciar los puntos según sexo incorporando la variable sexo como argumento del color dentro de aes():\n\nfacultad |&gt;\n  # mapeo estético\n  ggplot(aes(x = talla, y = peso, color = sexo))  + \n  \n  # agregamos la capa geométrica de puntos\n  geom_point()  \n\n\n\n\n\n\n\nTambién es posible superponer otras capas geométricas. Por ejemplo, podemos agregar rectas de regresión para cada grupo según sexo:\n\nfacultad |&gt;\n  # mapeo estético\n  ggplot(aes(x = talla, y = peso, color = sexo))  + \n  \n  # agregamos la capa geométrica de puntos\n  geom_point()  +\n  \n  # agregamos una segunda capa geométrica para la recta de regresión\n  geom_smooth(method = \"lm\")\n\n\n\n\n\n\n\nLa función geom_smooth() permite aplicar distintos métodos de suavizado o ajuste. En este caso usamos \"lm\" para mostrar la recta de regresión lineal entre talla y peso, junto con sus intervalos de confianza.\nA continuación, veremos las diferencias entre incluir aes() en ggplot() (aplicando el mapeo a todo el gráfico) o colocarlo solo dentro de alguna capa geométrica específica:\n\nfacultad |&gt;\n  # mapeo estético\n  ggplot(aes(x = talla, y = peso))  + \n  \n  # agregamos la capa geométrica de puntos coloreada por sexo\n  geom_point(aes(color = sexo))  +\n  \n  # agregamos una segunda capa geométrica para la recta de regresión\n  geom_smooth(method = \"lm\")\n\n\n\n\n\n\n\nEn este ejemplo, el color solo se especifica dentro de geom_point(), por lo que los puntos se dibujan diferenciados por sexo, pero no afecta a la capa de geom_smooth() produciendo solo una línea de regresión para el conjunto de puntos.\nEste comportamiento brinda gran flexibilidad en la construcción de gráficos, permitiendo definir qué capas deben responder a qué mapeos estéticos.\nAlgunas otras funciones de geom_ son:\n\ngeom_line(): gráfico de líneas.\ngeom_boxplot(): gráfico de caja y bigotes (boxplot).\ngeom_histogram(): histograma.\ngeom_density(): curva de densidad.\ngeom_bar(): gráfico de barras.\n\nTodas estas funciones pueden aplicarse sobre los mismos datos, y su uso depende del objetivo del análisis.\nA continuación, algunos ejemplos para visualizar la relación entre sexo y talla utilizando distintas capas geométricas:\n\n# Gráfico de puntos\nfacultad |&gt;\n  ggplot(aes(x = sexo, y = talla, color = sexo)) + \n  # capa geométrica de puntos\n  geom_point()\n\n\n\n\n\n\n# Boxplot\nfacultad |&gt;\n  ggplot(aes(x = sexo, y = talla, color = sexo)) + \n  # capa geométrica de boxplot\n  geom_boxplot()            \n\n\n\n\n\n\n# Entramado de puntos\nfacultad |&gt;\n  ggplot(aes(x = sexo, y = talla, color = sexo)) + \n  # capa geométrica jitter (entramado de puntos)\n  geom_jitter() \n\n\n\n\n\n\n# Gráfico de violín\nfacultad |&gt;\n  ggplot(aes(x = sexo, y = talla, color = sexo)) + \n  # capa geométrica de violin\n  geom_violin()\n\n\n\n\n\n\n\nObservemos que en el último gráfico se usó el argumento fill en lugar de color. Mientras color define el contorno de líneas, curvas o puntos, fill define el relleno de objetos geométricos, como los polígonos en gráficos de violín o barras.\nPersonalización de escalas con scale_\n\nEl sistema de escalas en ggplot2 permite ajustar múltiples aspectos visuales de un gráfico. Podemos modificar colores de contorno y relleno, invertir ejes, cambiar tamaños, tipos de línea, entre muchas otras opciones.\nTodas las funciones relacionadas con escalas comienzan con el prefijo scale_, seguido del atributo que queremos modificar (por ejemplo, scale_fill_ para cambiar el color de relleno).\nA continuación, mostramos algunos ejemplos aplicados sobre el conjunto de datos facultad:\n\nfacultad |&gt;\n  ggplot(aes(x = sexo, y = talla, fill = sexo)) + \n  \n  # capa geométrica boxplot\n  geom_boxplot() +\n  \n  # paleta de naranjas\n  scale_fill_brewer(palette = \"Oranges\")\n\n\n\n\n\n\n\nEn este ejemplo aplicamos una capa scale_fill_brewer() con la paleta de colores \"Oranges\" que se vincula con el argumento fill de aes() y definen los colores del boxplot.\nOtra alternativa es aplicar una escala de grises mediante scale_fill_grey():\n\nfacultad |&gt;\n  ggplot(aes(x = sexo, y = talla, fill = sexo)) + \n  \n  # capa geométrica boxplot\n  geom_boxplot() +\n  \n  # paleta en escala de grises\n  scale_fill_grey(start = 0.4, end = 0.8)\n\n\n\n\n\n\n\n\nRecomendamos trabajar con paletas de colores accesibles para personas con daltonismo y otras discapacidades visuales, como las incluidas en las dependencias RColorBrewer(Neuwirth 2022) y viridisLite, así como en paquetes específicos con paletas colorblind-friendly, como scico (Pedersen y Crameri 2023).\n\nTambién podemos modificar las escalas de los ejes. Por ejemplo, invertir el eje X con scale_x_reverse():\n\nfacultad |&gt;\n  ggplot(aes(x = talla, y = peso, fill = sexo)) + \n  \n  # capa geométrica de puntos\n  geom_point() +\n  \n  # invierte el eje x\n  scale_x_reverse()\n\n\n\n\n\n\n\nLa inclusión de scale_x_reverse() provoca la variable talla se muestre en orden descendente, de mayor a menor.\nPor último, podemos personalizar los cortes del eje Y utilizando scale_y_continuous(). Por ejemplo, en el boxplot pintado en escala de grises, definimos que el eje Y comienza en 130cm y termina en 200cm, con cortes cada 5cm y etiquetas cada 10cm:\n\nfacultad |&gt;\n  ggplot(aes(x = sexo, y = talla, fill = sexo)) + \n  \n  # capa geométrica boxplot\n  geom_boxplot() +\n  \n  # paleta en escala de grises\n  scale_fill_grey(start = 0.4, end = 0.8)   +\n  \n  # puntos de corte del eje Y\n  scale_y_continuous(limits = c(130, 200),       # límites\n                     breaks = seq(130, 200, 10)) # nro de etiquetas\n\n\n\n\n\n\n\nTransformaciones estadísticas con stat_\n\nAlgunos gráficos en ggplot2 no requieren transformaciones estadísticas, como los gráficos de dispersión. Sin embargo, otros tipos —como boxplots, histogramas o líneas de tendencia— sí aplican transformaciones estadísticas predeterminadas que pueden ser modificadas o personalizadas.\nEstas transformaciones pueden formar parte de las funciones geométricas, como ocurre en los histogramas, o agregarse como capas independientes mediante funciones stat_.\nPor ejemplo, en los histogramas podemos definir la cantidad de intervalos (o “clases”) a través del argumento bins, que forma parte de geom_histogram():\n\nfacultad |&gt;\n  ggplot(aes(edad)) +\n  \n  # capa geométrica histograma\n  geom_histogram(bins = nclass.Sturges(facultad$edad), \n                 fill = \"Blue\")\n\n\n\n\n\n\n\nEn este caso, utilizamos la regla de Sturges —a través de la función nclass.Sturges()— para determinar automáticamente la cantidad de clases para la variable edad.\nTambién podemos superponer al gráfico transformaciones estadísticas como capas adicionales. Por ejemplo, si deseamos agregar la media de talla en cada grupo de sexo sobre un boxplot, utilizamos stat_summary():\n\nfacultad |&gt;\n  ggplot(aes(x = sexo, y = talla, fill = sexo)) + \n  \n  # capa geométrica boxplot\n  geom_boxplot() +\n  \n  # paleta de tonos verdes\n  scale_fill_brewer(palette = \"Greens\") +\n  \n  # añade capa summary\n  stat_summary(fun = mean, \n               color = \"darkred\", \n               geom = \"point\", \n               shape = 18, \n               size = 3)\n\n\n\n\n\n\n\nLa función stat_summary() permite aplicar funciones estadísticas como mean, median, sd, entre otras, y representar el resultado con un objeto geométrico, en este caso geom = \"point\". En el gráfico, la media de talla se muestra con un punto rojo oscuro (color = \"darkred\"), de forma romboidal (shape = 18) y tamaño ampliado (size = 3).\nFacetado con facet_\n\nEl facetado permite dividir un gráfico en múltiples paneles o viñetas según los niveles de una o más variables categóricas. Esto resulta especialmente útil cuando se desea comparar patrones entre grupos sin sobrecargar un único gráfico con demasiada información.\nggplot2 ofrece dos funciones principales para realizar esta tarea:\n\nfacet_wrap(): separa los datos según una única variable categórica, generando una serie de paneles dispuestos de forma automática en filas y columnas.\nfacet_grid(): permite crear una matriz de paneles cruzando dos variables categóricas, una para las filas y otra para las columnas.\n\nRetomemos el gráfico de dispersión entre talla y peso, y generemos paneles separados para cada nivel de la variable sexo con facet_wrap():\n\nfacultad |&gt;\n  \n  ggplot(aes(x = talla, y = peso, color = sexo)) +\n  \n  # capa geométrica de puntos\n  geom_point() +\n  \n  # separa en paneles por sexo\n  facet_wrap(~ sexo)\n\n\n\n\n\n\n\nUsaremos facet_grid() para crear una matriz de histogramas producto del cruce de las variables fuma y sexo. Dentro de la cuadrícula graficaremos histogramas de la variable peso coloreados por sexo:\n\nfacultad |&gt;\n  ggplot(aes(y = peso, fill = sexo)) +\n  \n  # capa geométrica histograma\n  geom_histogram(bins = nclass.Sturges(facultad$peso)) +\n  \n  # paleta de colores\n  scale_fill_brewer(palette = \"Set1\") +\n  \n  # separa en paneles por sexo y fuma\n  facet_grid(sexo ~ fuma)\n\n\n\n\n\n\n\nComo se puede ver en estos ejemplos, estamos integrando varias de las funciones vistas: mapeo estético, capas geométricas, escalas, y ahora también el facetado.\nEl número de combinaciones posibles es enorme, dada la variedad de funciones y argumentos que ofrece ggplot2. Sin embargo, el objetivo de este material es comprender los principios fundamentales sobre los que se construye esta “gramática de los gráficos”, propuesta por los autores del paquete.\nSistema de coordenadas con coord_\n\nEn algunas ocasiones, puede ser útil modificar el sistema de coordenadas predeterminado del gráfico. Por ejemplo, para invertir los ejes y presentar un gráfico de barras en disposición horizontal:\n\nfacultad |&gt;\n  \n  ggplot(aes(sexo, fill = sexo)) +\n  \n  # capa geométrica barras\n  geom_bar() +\n  \n  # paleta de colores\n  scale_fill_brewer(palette = \"Set2\") +\n  \n  # invierte disposición de ejes\n  coord_flip()   \n\n\n\n\n\n\n\nTemas con theme()\n\nggplot2 incluye un conjunto de temas gráficos predefinidos que permiten modificar el aspecto general del gráfico. El tema por defecto es theme_gray(), pero puede cambiarse agregando una capa theme_*() dentro del gráfico.\nRepetimos el gráfico anterior, esta vez utilizando el tema blanco y negro (theme_bw()):\n\nfacultad |&gt;\n  \n  ggplot(aes(sexo, fill = sexo)) +\n  \n  # capa geométrica barras\n  geom_bar() +\n  \n  # paleta de colores\n  scale_fill_brewer(palette = \"Set2\") +\n  \n  # invierte disposición de ejes\n  coord_flip() +\n  \n  # tema en blanco y negro\n  theme_bw()\n\n\n\n\n\n\n\nTambién podemos aplicar un tema de fondo oscuro con theme_dark():\n\nfacultad |&gt;\n  \n  ggplot(aes(sexo, fill = sexo)) +\n  \n  # capa geométrica barras\n  geom_bar() +\n  \n  # paleta de colores\n  scale_fill_brewer(palette = \"Set2\") +\n  \n  # invierte disposición de ejes\n  coord_flip() +\n  \n  # tema con fondo oscuro\n  theme_dark()\n\n\n\n\n\n\n\nA continuación se muestra un cuadro con los principales temas disponibles en ggplot2 y sus características visuales:\n\n\n\n\nAdemás del aspecto general del gráfico, es posible agregar títulos, subtítulos y etiquetas de ejes con la función labs(). Por ejemplo:\n\nlabs(\n  x = \"Etiqueta X\", \n  y = \"Etiqueta Y\", \n  title = \"Título del gráfico\", \n  subtitle = \"Subtítulo del gráfico\"\n)\n\nTambién se pueden ajustar detalles del texto, como tipo de fuente o tamaño, mediante la función theme():\n\nfacultad |&gt;\n  ggplot(aes(sexo, fill = sexo)) +\n  # capa geométrica barras\n  geom_bar() +\n  # paleta de colores\n  scale_fill_brewer(palette = \"Set2\") +\n  # invierte disposición de ejes\n  coord_flip() +\n  # cambia etiquetas de los ejes\n  labs(y = \"Cantidad\", \n       title = \"Distribución de sexo\") +\n  # modifica el estilo de fuente del título\n  theme(plot.title = element_text(face = \"italic\", \n                                  size = 16)) \n\n\n\n\n\n\n\nPaquete esquisse\n\n\n\n\n\nEl paqueteesquisse (Meyer y Perrier 2025) es una extensión de ggplot2 que permite crear gráficos de manera interactiva, mediante una interfaz gráfica intuitiva basada en el sistema de arrastrar y soltar (drag & drop).\nCon esquisse es posible:\n\nExplorar visualmente los datos según su tipo.\nAsignar variables a diferentes estéticas del gráfico (ejes, color, tamaño, etc.).\nExportar los resultados en distintos formatos.\nRecuperar el código R que genera el gráfico, facilitando su reproducción y modificación.\n\nEl paquete se instala mediante el menú Packages de RStudio o ejecutando:\n\ninstall.packages(\"esquisse\")\n\nLuego se puede acceder a la aplicación por medio del acceso Addins:\n\n\n\n\no ejecutando en consola:\n\nesquisser()\n\nTambién se puede agregar el nombre de la tabla de datos dentro de los paréntesis\n\nesquisser(datos)\n\nPara más detalles, se puede consultar la viñeta del paquete disponible en CRAN o en su repositorio de GitHub.\nOtras extensiones de ggplot2\nUna de las grandes fortalezas de ggplot2 es su ecosistema de extensiones. Hasta el momento, existen más de 140 paquetes desarrollados para ampliar sus funcionalidades, muchos de ellos diseñados para facilitar tareas específicas o agregar nuevas capas, geometrías, temas, escalas o herramientas interactivas.\nAlgunas de las extensiones que utilizaremos durante el curso son:\n\npatchwork(Pedersen 2024): permite combinar múltiples gráficos de ggplot2 en una misma figura de forma sencilla y controlada.\nGGally(Schloerke et al. 2024): extiende ggplot2 con funciones para crear matrices de gráficos (como ggpairs()), útiles para análisis exploratorio multivariado.\nggpubr(Kassambara 2023): facilita la creación de gráficos listos para publicar, con funciones simplificadas para agregar títulos, etiquetas, comparaciones estadísticas y anotaciones.\nsee(Lüdecke et al. 2021): ofrece temas, paletas de colores y escalas compatibles con personas con daltonismo, además de geometrías personalizadas.\nsurvminer(Kassambara, Kosinski, y Biecek 2024): diseñado para la visualización de análisis de supervivencia (como curvas de Kaplan-Meier) utilizando ggplot2.\nggfortify(Tang, Horikoshi, y Li 2016): permite graficar objetos estadísticos como modelos lineales, PCA o series temporales sin necesidad de escribir código ggplot2 desde cero.\n\nEstas extensiones permiten ampliar la flexibilidad y expresividad gráfica de ggplot2 sin perder la estructura gramatical que lo caracteriza.",
    "crumbs": [
      "Unidad 1",
      "Introducción a tidyverse"
    ]
  },
  {
    "objectID": "unidad_1/01_intro_R.html",
    "href": "unidad_1/01_intro_R.html",
    "title": "Introducción a R",
    "section": "",
    "text": "Artwork por @allison_horst",
    "crumbs": [
      "Unidad 1",
      "Introducción a R"
    ]
  },
  {
    "objectID": "unidad_1/01_intro_R.html#qué-es-r",
    "href": "unidad_1/01_intro_R.html#qué-es-r",
    "title": "Introducción a R",
    "section": "¿Qué es R?",
    "text": "¿Qué es R?\nEl sitio oficial r-project.org define a R como “un entorno de software libre para gráficos y computación estadística. Se compila y se ejecuta en una amplia variedad de plataformas UNIX, Windows y MacOS.”\nProfundizando en su descripción, podemos decir que R es un lenguaje de programación interpretado, orientado a objetos, multiplataforma y de código abierto (open source) aplicado al manejo y análisis de datos estadísticos.\nA continuación, detallamos cada una de sus características:\nR es un lenguaje de programación estadístico\nSi bien posee un entorno y se puede utilizar como calculadora avanzada o para simulaciones, R es fundamentalmente un lenguaje de programación. Presenta una estructura y reglas de sintaxis propias, así como gran variedad de funciones desarrolladas con fines estadísticos.\nR es un lenguaje orientado a objetos\nR implementa conceptos de la programación orientada a objetos, lo cual le permite ofrecer simpleza y flexibilidad en el manejo de datos. En R, todo con lo que trabajamos —variables, funciones, datos, resultados— son objetos que pueden ser modificados o combinados con otros objetos.\nR es un lenguaje interpretado\nR no requiere compilación previa. Los scripts se ejecutan directamente mediante el intérprete del lenguaje, que devuelve resultados de forma inmediata.\nR es un lenguaje multiplataforma\nR puede instalarse y utilizarse en sistemas operativos Linux, Windows y MacOS. En todos ellos funciona de la misma manera, lo que garantiza que los scripts pueden correr en cualquier plataforma sin necesidad de modificaciones.\nR es software libre y de código abierto\nR se distribuye gratuitamente bajo licencia GNU - GPL (General Public License), lo que otorga a los usuarios la libertad de usar, estudiar, compartir y modificar el software. Esto ha favorecido el crecimiento de una comunidad global activa y colaborativa.",
    "crumbs": [
      "Unidad 1",
      "Introducción a R"
    ]
  },
  {
    "objectID": "unidad_1/01_intro_R.html#breve-historia-del-lenguaje",
    "href": "unidad_1/01_intro_R.html#breve-historia-del-lenguaje",
    "title": "Introducción a R",
    "section": "Breve historia del lenguaje",
    "text": "Breve historia del lenguaje\nR tiene su origen en el lenguaje S, desarrollado en los años 70 en los laboratorios Bell de AT&T (actualmente Lucent Technologies). Posteriormente, S dio lugar a una versión comercial llamada S-Plus, distribuida por Insightful Corporation.\nEn 1995, los profesores de estadística Ross Ihaka y Robert Gentleman, de la Universidad de Auckland (Nueva Zelanda) iniciaron el “Proyecto R”, con la intención de desarrollar un programa estadístico inspirado en el lenguaje S pero de dominio público.\nAunque R es considerado un dialecto de S, existen diferencias importantes en el diseño de ambos lenguajes.\nEl software está desarrollado principalmente en lenguaje C++, con algunas rutinas en Fortran. El nombre “R” hace referencia a las iniciales de sus creadores: Ross y Robert. Actualmente, R es mantenido por un grupo internacional de desarrolladores voluntarios conocido como el Core Development Team.",
    "crumbs": [
      "Unidad 1",
      "Introducción a R"
    ]
  },
  {
    "objectID": "unidad_1/01_intro_R.html#scripts-de-r",
    "href": "unidad_1/01_intro_R.html#scripts-de-r",
    "title": "Introducción a R",
    "section": "Scripts de R",
    "text": "Scripts de R\nUn script es un archivo de texto plano que contiene una secuencia de instrucciones para ser ejecutadas por el intérprete de R.\nEl término script puede traducirse como guión, archivo de órdenes, archivo de procesamiento por lotes o archivo de sintaxis.\nPuede crearse con cualquier editor de texto o con entornos especializados, y permite ser leído, modificado, guardado y ejecutado de forma completa o línea por línea.\nUna de sus principales ventajas es su reutilización: los scripts pueden adaptarse fácilmente a distintos análisis o contextos, lo que facilita la replicabilidad del trabajo.",
    "crumbs": [
      "Unidad 1",
      "Introducción a R"
    ]
  },
  {
    "objectID": "unidad_1/01_intro_R.html#características-generales-del-lenguaje",
    "href": "unidad_1/01_intro_R.html#características-generales-del-lenguaje",
    "title": "Introducción a R",
    "section": "Características generales del lenguaje",
    "text": "Características generales del lenguaje\nR posee una sintaxis textual precisa. Como en otros lenguajes de programación, la escritura debe ser exacta: distingue entre mayúsculas y minúsculas (case sensitive), y cada línea escrita en la consola comienza con el símbolo &gt; (prompt1).",
    "crumbs": [
      "Unidad 1",
      "Introducción a R"
    ]
  },
  {
    "objectID": "unidad_1/01_intro_R.html#documentación-del-código",
    "href": "unidad_1/01_intro_R.html#documentación-del-código",
    "title": "Introducción a R",
    "section": "Documentación del código",
    "text": "Documentación del código\nLa documentación es una tarea fundamental en cualquier lenguaje de programación, ya que nos permite entender el propósito del script, facilita su mantenimiento y posibilita su reutilización tanto por quien lo creó como por otras personas.\nEn R, la documentación de los scripts se realiza a través de comentarios, indicados con el símbolo #. Todo lo que sigue a ese símbolo es ignorado por el intérprete cuando se ejecute el código:\n\n# esto es una línea de comentario y el intérprete no la ejecuta\n\nAsí que a la hora de documentar es preferible abusar de estos comentarios que no utilizarlos.",
    "crumbs": [
      "Unidad 1",
      "Introducción a R"
    ]
  },
  {
    "objectID": "unidad_1/01_intro_R.html#funciones",
    "href": "unidad_1/01_intro_R.html#funciones",
    "title": "Introducción a R",
    "section": "Funciones",
    "text": "Funciones\nLas órdenes elementales en R se denominan comandos o funciones. Algunas se conocen como “integradas” ya que están incluidas en el núcleo del lenguaje (R base) y otras provienen de paquetes adicionales.\nLa mayoría de las ofrecidas en los paquetes o librerías están elaboradas con R, dado que todos los usuarios podemos crear nuevas funciones con el mismo lenguaje.\nToda función tiene un nombre y puede recibir argumentos (obligatorios u opcionales), que se colocan entre paréntesis y separados por comas. Incluso algunas funciones que no tienen asociado argumentos necesitan, en su sintaxis, a los paréntesis ().\nSiempre una función devuelve un resultado, un valor o realiza una acción:\n\nnombre_de_la_función(arg_1, arg_2, arg_n)\n\nComo el interprete de R no permite errores en la sintaxis de las expresiones, debemos atender a los siguientes puntos a la hora de escribirlas:\nLa sintaxis habitual de una función es la siguiente:\n\nfuncion(arg1, arg2, arg3,...)\n\nLos títulos de los argumentos pueden escribirse y mediante un igual agregar el valor correspondiente:\n\nfuncion(arg1 = 32, arg2 = 5, arg3 = 65,...)\n\nSe puede omitir el título del argumento y escribir directamente el valor, pero en este caso, hay que respetar el orden definido por la función:\n\nfuncion(32, 5, 65,...)\n\nLos valores numéricos, lógicos, especiales y objetos van escritos en forma directa y cuando escribimos caracteres (texto) van necesariamente encerrados entre comillas:\n\nfuncion(arg1 = 3, arg2 = NA, arg3 = TRUE, arg4 = \"less\", arg5 = x, ...)",
    "crumbs": [
      "Unidad 1",
      "Introducción a R"
    ]
  },
  {
    "objectID": "unidad_1/01_intro_R.html#paquetes-o-librerías",
    "href": "unidad_1/01_intro_R.html#paquetes-o-librerías",
    "title": "Introducción a R",
    "section": "Paquetes o librerías",
    "text": "Paquetes o librerías\nLos paquetes, también conocidos como librerías, son conjuntos de funciones, datos y documentación organizados organizadas en torno a una temática específica, que permiten ampliar las capacidades del sistema base de R.\nAl instalar R, se incorpora un núcleo básico que queda activo automáticamente en cada sesión (base, datasets, graphics, grDevices, methods, stats y utils). Junto a este núcleo, se incluye un conjunto de paquetes recomendados que forman parte de la distribución oficial y pueden activarse manualmente cuando se los necesita. No obstante, la verdadera potencia de R reside en la posibilidad de incorporar nuevos paquetes desarrollados por la comunidad, lo que permite extender continuamente sus funcionalidades.\n\n\n\n\nEn la actualidad, existen más de 17.000 paquetes disponibles para una amplia variedad de aplicaciones, número que crece mes a mes gracias a la naturaleza open source del lenguaje. Cualquier persona puede desarrollar y compartir sus propios paquetes, aunque no todos llegan a publicarse en el repositorio oficial CRAN (Comprehensive R Archive Network), que actúa como principal fuente de distribución.\nLos paquetes pueden descargarse directamente desde CRAN o instalarse a partir de archivos comprimidos locales (como .zip o .tar.gz). Una vez instalados, se los puede activar en cualquier análisis.\nDependencias\nEn muchos casos, al utilizar un paquete, este necesita de otros para funcionar correctamente. Esta relación se conoce como dependencia, y es una característica central en el ecosistema de R.\nLa mayoría de las funciones incluidas en los paquetes están desarrolladas en el propio lenguaje R y, durante su construcción, es habitual que recurran a funciones ya existentes en otros paquetes. Por ejemplo, una función nueva puede hacer uso de una función auxiliar que no pertenece al sistema base, sino a otro paquete externo.\nCuando intentamos ejecutar una función que depende de otras no disponibles en nuestra instalación, R no podrá encontrar esas funciones y nos devolverá un mensaje de error indicando que no reconoce el nombre solicitado. Este tipo de errores suele alertar sobre funciones “desconocidas” o “no encontradas”, lo que indica que falta instalar o activar alguno de los paquetes requeridos.\nPara evitar estos inconvenientes, R intenta resolver automáticamente las dependencias cuando instalamos un paquete desde el repositorio oficial CRAN. Si el paquete necesita otros para funcionar, el sistema detecta esta relación y se encarga de instalar también aquellos paquetes auxiliares. De todos modos, si alguno no se instala correctamente o no se activa en la sesión, será necesario hacerlo de forma manual.",
    "crumbs": [
      "Unidad 1",
      "Introducción a R"
    ]
  },
  {
    "objectID": "unidad_1/01_intro_R.html#errores-y-advertencias",
    "href": "unidad_1/01_intro_R.html#errores-y-advertencias",
    "title": "Introducción a R",
    "section": "Errores y advertencias",
    "text": "Errores y advertencias\nEl lenguaje R es muy preciso en su sintaxis. Cometer errores al escribir funciones u objetos es común durante el aprendizaje, y el intérprete de R devuelve mensajes de error cuando detecta algo incorrecto.\nUno de los aspectos clave a tener en cuenta es que R distingue entre mayúsculas y minúsculas (case sensitive), por lo que no es lo mismo escribir a que A.\nExisten tres grupos de mensajes de error:\n\nErrores de sintaxis\nError de objeto no encontrado\nOtros errores\n\nErrores de sintaxis\nOcurre cuando R no puede interpretar una línea de código porque algo está mal escrito. Los errores de sintaxis más frecuentes incluyen:\n\nParéntesis, comas, comillas, corchetes o llaves mal colocados o ausentes.\nArgumentos mal separados o incorrectamente definidos.\n\nPor ejemplo la función rep() repite valores una cantidad de veces. Tiene dos argumentos, x donde se coloca el valor a repetir y times donde se define la cantidad de veces:\n\nrep(x = 3, times = 4) # repetimos 4 veces 3 con rep()\n\n[1] 3 3 3 3\n\n\nSi nos olvidamos de cerrar el paréntesis:\n\nrep(x = 3, times = 4\n\nError in parse(text = input): &lt;text&gt;:2:0: unexpected end of input\n1: rep(x = 3, times = 4\n   ^\n\n\nSi omitimos la coma entre argumentos:\n\nrep(x = 3 times = 4)\n\nError in parse(text = input): &lt;text&gt;:1:11: unexpected symbol\n1: rep(x = 3 times\n              ^\n\n\nSi usamos un nombre de argumento no válido:\n\nrep(y = 3, times = 4)\n\nError in rep(y = 3, times = 4): attempt to replicate an object of type 'symbol'\n\n\nSi escribimos mal el nombre de la función:\n\nrop(x = 3, times = 4)\n\nError in rop(x = 3, times = 4): could not find function \"rop\"\n\n\nEste último error se asemeja a un error de objeto no encontrado, aunque tiene origen en un problema de sintaxis.\nLos mensajes de error en general y sobre todo al principio pueden parecer extraños y difíciles de entender, pero con un poco de práctica podemos inferir donde está el problema.\nError de objeto no encontrado\nEste tipo de error se produce cuando R no reconoce un objeto utilizado en el código. Las causas más frecuentes incluyen:\n\nEl nombre del objeto está mal escrito (por ejemplo, errores de sintaxis o uso incorrecto de mayúsculas/minúsculas).\nEl objeto pertenece a un paquete o archivo que no fue cargado.\nFaltan comillas al declarar caracteres.\nOtras causas similares.\n\nVolvamos al ejemplo anterior, ahora repitiendo un valor tipo character:\n\nrep(x = \"A\", times = 4) # repetimos 4 veces \"A\" con rep()\n\n[1] \"A\" \"A\" \"A\" \"A\"\n\n\nSi olvidamos las comillas:\n\nrep(x = A, times = 4) # repetimos 4 veces A con rep()\n\nError: object 'A' not found\n\n\nOtros errores\nSon aquellos que se generan por causas distintas a errores de sintaxis o de objeto no encontrado. Por ejemplo:\n\n\"x\" + 10\n\nError in \"x\" + 10: non-numeric argument to binary operator\n\n\nEl código anterior genera un mensaje de error porque intenta sumar objetos de tipos incompatibles entre sí.\nVeamos otro ejemplo:\n\nt.test(1)\n\nError in t.test.default(1): not enough 'x' observations\n\n\nEn este caso, el error se debe a que no hay suficientes observaciones para realizar una prueba \\(t\\) de Student.\nOtro caso frecuente ocurre cuando se intenta acceder a una posición inexistente en un vector:\n\nx &lt;- c(1, 2, 3)\nx[[5]]\n\nError in x[[5]]: subscript out of bounds\n\n\nEste mensaje de error indica que el subíndice está fuera de los límites del objeto (subscript out of bounds).\nAdvertencias\nLas advertencia no son tan serias como un error, o al menos no lo parece, ya que permiten el código se ejecute igual. Pero puede ocurrir que ignorar una advertencia llegue a ser algo muy serio, si esto implica que la salida de la función es equivocada.\nIgnorar advertencias puede llevar a conclusiones erróneas, por lo que es recomendable prestarles atención y entender su causa.\nPor ejemplo:\n\nlog(-1)\n\nWarning in log(-1): NaNs produced\n\n\n[1] NaN\n\n\nla función genera una advertencia porque el logaritmo de un número negativo no está definido en los reales, y R devuelve un valor NaN (Not a Number).\nResumiendo…",
    "crumbs": [
      "Unidad 1",
      "Introducción a R"
    ]
  },
  {
    "objectID": "unidad_1/01_intro_R.html#creación-de-objetos",
    "href": "unidad_1/01_intro_R.html#creación-de-objetos",
    "title": "Introducción a R",
    "section": "Creación de objetos",
    "text": "Creación de objetos\nTodas las declaraciones donde se crean objetos usan el símbolo de asignación &lt;-:\n\nnombre_objeto &lt;- valor\n\nVeámoslo en un ejemplo:\n\na &lt;- 1\n\nEn este caso asignamos el valor 1 al objeto a. El objeto a es un vector de una posición (un solo valor). Si llamamos al objeto, el intérprete devuelve el valor asignado previamente:\n\na\n\n[1] 1\n\n\nObservemos que, además de devolvernos el valor, aparece delante un número entre corchetes [1]. Este número es la ubicación o índice del comienzo del objeto. En este caso, como el vector tiene una sola posición, indica que el primer valor mostrado empieza en la posición 1.",
    "crumbs": [
      "Unidad 1",
      "Introducción a R"
    ]
  },
  {
    "objectID": "unidad_1/01_intro_R.html#estructuras-de-datos",
    "href": "unidad_1/01_intro_R.html#estructuras-de-datos",
    "title": "Introducción a R",
    "section": "Estructuras de datos",
    "text": "Estructuras de datos\nLos objetos contenedores de datos más simples pertenecen a cinco clases atómicas, que son:\n\ninteger (números enteros)\nnumeric (números reales)\ncomplex (números complejos)\ncharacter (cadenas de caracteres)\nlogical (valores lógicos: TRUE o FALSE)\n\n\n\n\n\nSin embargo, cada una de estas clases de datos no se encuentran de manera aislada, sino encapsuladas dentro de la clase de objeto más básica de R, a la que se denomina vector.\nEn RStudio, el sistema de colores ayuda a distinguir los tipos de valores:\n\n# Número\n1111\n\n[1] 1111\n\n# Caracter\n\"esto es una cadena de texto\"\n\n[1] \"esto es una cadena de texto\"\n\n# Lógico\nTRUE\n\n[1] TRUE\n\n# Objeto\na &lt;- 1\n\nVectores\nUn vector es un conjunto de valores (números o símbolos), del mismo tipo ordenados de la forma (elemento 1, elemento 2, … , elemento \\(n\\)), donde \\(n\\) es la longitud o tamaño del vector.\nLos atributos principales son:\n\nTipo: puede ser integer, numeric, character, complex o logical.\nLongitud: cantidad de elementos que contiene el objeto,\n\nEl vector más simple contiene un solo dato, por ejemplo un vector de longitud 1 y tipo numeric:\n\nvec1 &lt;- 1\nvec1\n\n[1] 1\n\n\nOtro vector más grande por ejemplo podría ser (1, 5, 2). En este caso también es del tipo numeric pero tiene una longitud de 3 elementos:\n\nvec2 &lt;- c(1, 5, 2)\nvec2\n\n[1] 1 5 2\n\n\nPara concatenar elementos usamos la función c(), dentro de la cual van los valores separados por comas. El orden de los elementos importa, en nuestro ejemplo la primera posición la ocupa el 1, la segunda el 5 y la tercera el 2. Si tuvieramos otro vector (5, 1, 2), no sería lo mismo porque los valores están ordenados de forma diferente.\nPara conocer la longitud del vector usamos:\n\nlength(vec2)\n\n[1] 3\n\n\nNos informa que vec2 tiene 3 elementos.\nPara conocer el tipo de dato ejecutamos:\n\nclass(vec2)\n\n[1] \"numeric\"\n\n\nPodemos ver que los datos almacenados en este segundo ejemplo cumplen con la definición en lo que respecta al tipo de dato, ya que cada elemento es del mismo tipo (numeric).\nVeamos un ejemplo de asignación de otro tipo de dato atómico, como es el character:\n\nvec3 &lt;- \"Hola\"\nvec3\n\n[1] \"Hola\"\n\n\nSiempre que escribamos contenido de tipo character debemos hacerlo entre comillas. En este caso generamos el vector vec3 con el contenido \"Hola\", que, a pesar de ser una palabra compuesta de varios caracteres, dentro del vector vec3 esta ocupa una sola posición:\n\nlength(vec3)\n\n[1] 1\n\n\nRespecto al tipo de dato si usamos la función class() tendremos:\n\nclass(vec3)\n\n[1] \"character\"\n\n\nFactores\nUn factor es un objeto especialmente diseñado para contener datos categóricos y se asocia particularmente con las variables cualitativas.\nEn su estructura interna está compuesto por dos vectores:\n\nUn vector de índices enteros.\nUn vector de categorías (niveles) de tipo character, a los que hace referencia el primer vector.\n\nExisten de dos tipos: factores nominales y factores ordinales. En el caso del segundo se establece un orden en los niveles.\nNormalmente, obtenemos un tipo factor de convertir un vector u otro tipo de objeto con caracteres, pero para mostrar un ejemplo lo realizamos con la función factor():\n\nfactor1 &lt;- factor(x = c(\"a\", \"b\", \"a\", \"c\", \"b\", \"a\"), \n                  levels = c(\"a\", \"b\", \"c\"))\nfactor1\n\n[1] a b a c b a\nLevels: a b c\n\n\nCreamos el objeto factor1 con 6 elementos caracteres y tres niveles sin orden.\nAdemás de la practicidad de trabajar con factores, muchas funciones de R requieren que las variables categóricas estén en formato factor para funcionar correctamente.\nDataframe\nUn dataframe es un objeto diseñado para contener conjuntos de datos y representa una estructura bidimensional similar a una tabla, con filas y columnas. Cada columna puede almacenar elementos de distintos tipos (por ejemplo, numéricos, de texto o lógicos), siempre que todos tengan la misma longitud.\nLas columnas suelen tener nombres únicos, lo que permite referenciarlas fácilmente como si fueran variables individuales dentro del conjunto de datos.\nEste es el tipo de objeto que se utiliza habitualmente para almacenar información importada desde archivos externos (por ejemplo, archivos de texto separados por comas o planillas de Excel), y con el que más frecuentemente trabajamos en los análisis.\nDesde el punto de vista estructural, un dataframe está compuesto por una serie de vectores de igual longitud dispuestos verticalmente, uno al lado del otro, conformando las columnas de la tabla.\nVeamos un ejemplo:\n\n# Historia clínica\nHC &lt;- c(\"F324\", \"G21\", \"G34\", \"F231\")\n\n# Edad\nedad &lt;- c(34, 32, 34, 54)\n\n# Sexo\nsexo &lt;- c(\"M\", \"V\", \"V\", \"M\")\n\n# dataframe\ndf1 &lt;- data.frame(HC, edad, sexo)\n\ndf1\n\n    HC edad sexo\n1 F324   34    M\n2  G21   32    V\n3  G34   34    V\n4 F231   54    M\n\n\nCreamos tres vectores con datos de supuestos individuos, su historia clinica, la edad y el sexo. Luego mediante la función data.frame() “unimos” esos vectores en forma vertical para formar un dataframe de 3 variables y 4 observaciones.\n\n\n\n\n\n\nExisten otras estructuras de datos que aparecen en la siguiente figura. Las más habituales en nuestro trabajo son los vectores y los dataframes. Los factores serán necesarios cuando tengamos que especificar distintos órdenes de niveles.",
    "crumbs": [
      "Unidad 1",
      "Introducción a R"
    ]
  },
  {
    "objectID": "unidad_1/01_intro_R.html#operadores",
    "href": "unidad_1/01_intro_R.html#operadores",
    "title": "Introducción a R",
    "section": "Operadores",
    "text": "Operadores\nAdemás de funciones, el lenguaje R cuenta con operadores de uso relativamente intuitivo, que permiten realizar operaciones de diferentes tipos con los objetos que contienen datos.\nOperadores aritméticos\n\n\n\n\nOperador\nDescripción\n\n\n\n+\nSuma\n\n\n-\nResta\n\n\n*\nMultiplicación\n\n\n/\nDivisión\n\n\n^\nPotencia\n\n\n%%\nMódulo\n\n\n%/%\nDivisión de enteros\n\n\n\n\n\nLos operadores aritméticos se utilizan como si el lenguaje fuese una calculadora:\n\n# Suma\n2 + 5\n\n[1] 7\n\n# Resta\n3 - 2\n\n[1] 1\n\n# Multiplicación\n9 * 3\n\n[1] 27\n\n# División\n10 / 2\n\n[1] 5\n\n# Potencia\n5 ^ 2\n\n[1] 25\n\n\nTambién se pueden hacer operaciones con los objetos que almacenan valores numéricos:\n\na &lt;- 3\n\nb &lt;- 6\n\n(a + b) * b\n\n[1] 54\n\n\nY funciona con objetos con más de un elemento, aplicando aritmética vectorial, donde las operaciones se realizan elemento a elemento:\n\na &lt;- c(1, 2, 3)\n\na * 3\n\n[1] 3 6 9\n\n\nO bien, con operaciones entre los objetos, donde se realiza entre los elementos de la misma posición:\n\na &lt;- c(1, 2, 3)\n\na * a\n\n[1] 1 4 9\n\n\nOperadores relacionales\n\n\n\n\nOperador\nDescripción\n\n\n\n&lt;\nMenor que\n\n\n&gt;\nMayor que\n\n\n&lt;=\nMenor o igual que\n\n\n&gt;=\nMayor o igual que\n\n\n==\nIgual que\n\n\n!=\nNo igual que\n\n\n\n\n\nHabitualmente estos operadores se utilizan asiduamente en expresiones para indicar relaciones entre valores.\nPodemos ver su funcionamiento en el ejemplo siguiente:\n\na &lt;- c(3, 8, 2)\n\na == c(3, 4, 5)\n\n[1]  TRUE FALSE FALSE\n\n\nEl lenguaje evalúa las comparaciones que hace el operador relacional igual (en este caso) y en aquellos valores que coinciden devuelve TRUE y en los que no hay coincidencia devuelve FALSE.\nLo mismo sucede con los otros operadores relacionales:\n\na &lt;- c(4, 8, 10)\n\na &gt; 8\n\n[1] FALSE FALSE  TRUE\n\na &lt; 8\n\n[1]  TRUE FALSE FALSE\n\na != 8\n\n[1]  TRUE FALSE  TRUE\n\n\nOperadores lógicos\n\n\n\n\nOperador\nDescripción\n\n\n\n!\nNOT\n\n\n&\nAND booleano\n\n\n&&\nAND booleano para vectores de longitud 1\n\n\n|\nOR booleano\n\n\n||\nOR booleano para vectores de longitud 1\n\n\n\n\n\nCuando queremos conectar algunas de las expresiones relacionales hacemos uso de estos operadores lógicos típicos (AND, OR, NOT).\nPara ejemplificar podemos hacer:\n\na &lt;- c(1:8)\n\na\n\n[1] 1 2 3 4 5 6 7 8\n\n(a &gt; 3) & (a &lt; 7)\n\n[1] FALSE FALSE FALSE  TRUE  TRUE  TRUE FALSE FALSE\n\n\nEn el caso anterior usamos el operador & como conector AND de dos expresiones relacionales donde el lenguaje devuelve TRUE en el rango mayor a 3 y menor a 7 (valores 4,5 y 6).",
    "crumbs": [
      "Unidad 1",
      "Introducción a R"
    ]
  },
  {
    "objectID": "unidad_1/01_intro_R.html#valores-especiales",
    "href": "unidad_1/01_intro_R.html#valores-especiales",
    "title": "Introducción a R",
    "section": "Valores especiales",
    "text": "Valores especiales\nExisten algunos valores especiales para datos con expresiones reservadas en R, entre ellos encontramos los valores NA, NaN, Inf y NULL.\n\n\n\n\nOperador\nSignificado\nDescripción\n\n\n\nNA\nNot available\nEs la forma de expresar a los valores perdidos o faltantes (missing values)\n\n\nNaN\nNot a number\nUtilizado para resultados de operaciones que devuelven error numérico\n\n\nInf\nInfinity\nValor infinito (positivo)\n\n\n-Inf\nInfinity\nValor infinito (negativo)\n\n\nNULL\nNull\nValor nulo\n\n\n\n\n\nEl más relevante de estos valores especiales es el NA que sirve para indicar que no hay valor en esa posición o elemento de un objeto.",
    "crumbs": [
      "Unidad 1",
      "Introducción a R"
    ]
  },
  {
    "objectID": "unidad_1/01_intro_R.html#secuencias-regulares",
    "href": "unidad_1/01_intro_R.html#secuencias-regulares",
    "title": "Introducción a R",
    "section": "Secuencias regulares",
    "text": "Secuencias regulares\nAdemás de concatenar elementos con la función c(), existen tres formas comunes de generar secuencias regulares.\nLa primera es mediante un operador secuencial (:), que genera una secuencia de enteros entre dos valores, ya sea en forma ascendente o descendente:\n\n1:10\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\n10:1\n\n [1] 10  9  8  7  6  5  4  3  2  1\n\n\nLa segunda forma es mediante la función seq(), cuyos argumentos principales son from, to y by. Esta función permite mayor flexibilidad:\n\nseq(from = 1, to = 20, by = 2)\n\n [1]  1  3  5  7  9 11 13 15 17 19\n\n\nEl ejemplo anterior genera una secuencia de números que comienza en 1 y llega hasta 20, avanzando de dos en dos.\nAlgunos otros ejemplos de la misma función:\n\nseq(from = 0.1, to = 0.9, by = 0.1)\n\n[1] 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9\n\nseq(from = -5, to = 5, by = 1)\n\n [1] -5 -4 -3 -2 -1  0  1  2  3  4  5\n\nseq(from = 300, to = 0, by = -50)\n\n[1] 300 250 200 150 100  50   0\n\n\nLa tercera opción es la función rep(), que permite duplicar valores. Su forma más básica es rep(x, times = n), donde se repite el valor x la cantidad de veces indicada por n.\nAlgunos ejemplos:\n\nrep(x = 2, times = 5)\n\n[1] 2 2 2 2 2\n\n# combinada con el operador :\nrep(1:4, 5)  \n\n [1] 1 2 3 4 1 2 3 4 1 2 3 4 1 2 3 4 1 2 3 4\n\n# combinada con la función c()\nrep(c(4.5, 6.8, 7.2), 2) \n\n[1] 4.5 6.8 7.2 4.5 6.8 7.2",
    "crumbs": [
      "Unidad 1",
      "Introducción a R"
    ]
  },
  {
    "objectID": "unidad_1/01_intro_R.html#índices",
    "href": "unidad_1/01_intro_R.html#índices",
    "title": "Introducción a R",
    "section": "Índices",
    "text": "Índices\nR implementa una forma eficiente y flexible de acceder selectivamente a elementos de un objeto basado en una indexación interna.\nPara acceder a un índice se utiliza la notación de corchetes. Por ejemplo, si tenemos un vector x con varios elementos y queremos acceder al segundo, usamos x[2].\nEsta forma de indexar los elementos de los distintos objetos nos permite realizar muchas operaciones desde el simple llamado, hasta seleccionar, eliminar o modificar valores.\nVeamos algunos ejemplos:\n\n# generamos un vector x con 5 letras\nx &lt;- c(\"a\",\"b\",\"c\",\"d\",\"e\")\n\n# llamamos a la primer posición y nos devuelve su valor \nx[1]\n\n[1] \"a\"\n\n# llamamos a la tercer posición y nos devuelve su valor\nx[3]    \n\n[1] \"c\"\n\n# llamamos a las posiciones 1 y 3 juntas mediante c()\nx[c(1,3)]   \n\n[1] \"a\" \"c\"\n\n# llamamos a las posiciones menos la 1 y la 4\nx[-c(1, 4)] \n\n[1] \"b\" \"c\" \"e\"\n\n# creamos otro vector y con los valores 1,2 y 5\ny &lt;- c(1, 2, 5) \n\n# utilizamos el vector y como índice    \nx[y]        \n\n[1] \"a\" \"b\" \"e\"\n\n# asignamos el valor “h” a la posición 2 del vector x\nx[2] &lt;- \"h\" \n\nx\n\n[1] \"a\" \"h\" \"c\" \"d\" \"e\"\n\n# creamos el vector z eliminando la posición 5 de x\nz &lt;- x[-5]  \n\nz\n\n[1] \"a\" \"h\" \"c\" \"d\"\n\n\nEstos ejemplos muestran las múltiples posibilidades que ofrece el uso de índices para manipular objetos en R, lo que hace al lenguaje muy poderoso y versátil.\nCuando se aplican índices a estructuras bidimensionales, como matrices o dataframes, la notación general es:\n\nnombre[índice de fila, índice de columna]",
    "crumbs": [
      "Unidad 1",
      "Introducción a R"
    ]
  },
  {
    "objectID": "unidad_1/01_intro_R.html#gestión-de-factores",
    "href": "unidad_1/01_intro_R.html#gestión-de-factores",
    "title": "Introducción a R",
    "section": "Gestión de factores",
    "text": "Gestión de factores\nAl presentar las distintas estructuras de datos mencionamos que los factores suelen generarse a partir de vectores u otros objetos de tipo carácter.\nPara entender cómo funcionan, partamos de un vector con datos categóricos:\n\nrespuesta &lt;- c(\"Si\", \"No\", \"No\", \"Si\", \"Si\", \"Si\", \"No\")\n\nCreamos un vector llamado respuesta con 7 elementos de tipo carácter, en el que se repiten las categorías \"Si\" y \"No\".\nPodemos confirmar que se trata de un vector y que su contenido es de tipo carácter:\n\n# preguntamos si sexo es un vector\nis.vector(respuesta) \n\n[1] TRUE\n\n# visualizamos el tipo de dato de sexo\nclass(respuesta)  \n\n[1] \"character\"\n\n\nPara crear un factor a partir de este vector debemos utilizar la función factor():\n\nrespuesta &lt;- factor(respuesta)\n\nrespuesta\n\n[1] Si No No Si Si Si No\nLevels: No Si\n\n\nEn la salida observamos los siete elementos y, debajo, una línea con los niveles del factor, indicados por Levels. Estos niveles son identificados automáticamente.\nPodemos verificar cómo cambió el tipo de objeto:\n\n# preguntamos si respuesta es un vector\nis.vector(respuesta) \n\n[1] FALSE\n\n# preguntamos si respuesta es un factor\nis.factor(respuesta) \n\n[1] TRUE\n\n# visualizamos el tipo de dato de respuesta\nclass(respuesta) \n\n[1] \"factor\"\n\n\nAunque visualmente vemos palabras, internamente R trata al factor como un tipo especial basado en números. ¿Por qué sucede esto? Veámoslo con más detalle:\n\nstr(respuesta)\n\n Factor w/ 2 levels \"No\",\"Si\": 2 1 1 2 2 2 1\n\n\nLa función str() devuelve la estructura interna del objeto. En este caso, muestra que respuesta tiene dos niveles y que cada elemento del vector es representado por un número (1 o 2), donde 1 corresponde a \"No\" y 2 a \"Si\".\nEsto significa que la estructura de los factores está compuesta por dos vectores: uno numérico que funciona como índice de enteros, que sustituye al vector de caracteres original, y el otro es un vector de caracteres, que contiene los niveles o categorías, a los que hace referencia el primer vector.\nPara ver solo los niveles o categorías del factor podemos usar:\n\nlevels(respuesta)\n\n[1] \"No\" \"Si\"\n\n\nEn los factores nominales donde no importa el orden, la función factor() implementa el orden alfabético para determinar a qué índice numérico pertenece cada categoría. Es claro en el ejemplo que a No le asigna el 1 y a Si el 2.\nPero si nos encontramos frente a una variable cualitativa ordinal vamos a necesitar indicarle a la función cual es el orden de las categorías.\nVamos al siguiente ejemplo:\n\nsalud &lt;- c(4, 3, 1, 3, 2, 2, 3, 3, 1)\nsalud\n\n[1] 4 3 1 3 2 2 3 3 1\n\n\nTenemos en el vector salud algunos códigos numéricos que representan nivel de salud de personas registradas por una encuesta donde 1 significa mala salud, 2 regular, 3 buena y 4 muy buena.\nProcedemos a crear el factor nivsalud a partir de este vector:\n\nnivsalud &lt;- factor(salud,\n                   label = c(\"Mala\", \"Regular\", \"Buena\", \"Muy buena\"),\n                   levels = 1:4)\n\nnivsalud\n\n[1] Muy buena Buena     Mala      Buena     Regular   Regular   Buena    \n[8] Buena     Mala     \nLevels: Mala Regular Buena Muy buena\n\n\nAquí utilizamos dos argumentos adicionales:\n\nlabels: define las etiquetas que queremos mostrar para cada categoría.\nlevels: indica el orden de los niveles originales (en este caso, 1 a 4).\n\nAquí es necesario definir estos argumentos porque, a diferencia del factor respuesta, el vector salud contiene números en lugar de las palabras correspondientes a las categorías.\nHasta aquí hemos creado un factor pero si miramos sus niveles no encontraremos señales que sigan un orden específico:\n\nlevels(nivsalud)\n\n[1] \"Mala\"      \"Regular\"   \"Buena\"     \"Muy buena\"\n\n\nSin embargo, aún no hemos definido un orden explícito entre las categorías. Para hacerlo, agregamos el argumento ordered = TRUE:\n\nnivsalud &lt;- factor(salud,\n                   label = c(\"Mala\", \"Regular\", \"Buena\", \"Muy buena\"),\n                   levels = 1:4, \n                   ordered = TRUE)\n\nnivsalud\n\n[1] Muy buena Buena     Mala      Buena     Regular   Regular   Buena    \n[8] Buena     Mala     \nLevels: Mala &lt; Regular &lt; Buena &lt; Muy buena\n\n\nAl incorporar este argumento, estamos diciendo que los niveles tienen orden natural. Esto puede observarse en la salida donde se muestra que \"Mala\" &lt; \"Regular\" &lt; \"Buena\" &lt; \"Muy buena\".",
    "crumbs": [
      "Unidad 1",
      "Introducción a R"
    ]
  },
  {
    "objectID": "unidad_1/01_intro_R.html#gestión-de-dataframes",
    "href": "unidad_1/01_intro_R.html#gestión-de-dataframes",
    "title": "Introducción a R",
    "section": "Gestión de dataframes\n",
    "text": "Gestión de dataframes\n\nEn R, la estructura utilizada para almacenar datos provenientes de fuentes externas (como archivos .csv, planillas de cálculo, bases SQL, etc.) es el dataframe.\nA modo de ejemplo, vamos a construir un dataframe llamado datos con 4 variables y 5 observaciones. Las variables serán:\n\nid: identificador numérico entero y correlativo.\nedad: edad en años.\nsexo: codificado como \"V\" para varón y \"M\" para mujer.\ntrabaja: variable lógica, donde TRUE (T) representa que la persona trabaja y FALSE (F) que no lo hace.\n\n\n# construimos el vector id\nid &lt;- 1:5\n\n# construimos el vector edad\nedad &lt;- c(23, 43, 12, 65, 37)\n\n# construimos el vector sexo\nsexo &lt;- c(\"V\", \"M\", \"M\", \"V\", \"M\")\n\n# construimos el vector trabaja\ntrabaja &lt;- c(T, T, F, F, T)\n\n# construimos el dataframe datos\ndatos &lt;- data.frame(id, edad, sexo, trabaja)\n\nAunque para el ejemplo construimos un dataframe manualmente, lo habitual en la práctica es leer archivos externos que se importan directamente como dataframes.\nAlgunas de las funciones generales que podemos aplicar son:\n\n# pedimos el número de columnas (variables)\nncol(datos)\n\n[1] 4\n\n# pedimos el número de filas (registros u observaciones)\nnrow(datos)\n\n[1] 5\n\n# pedimos las dimensiones del dataframe (observaciones,variables)\ndim(datos)\n\n[1] 5 4\n\n\nTambién podemos visualizar como se compone el objeto datos aplicando str() que devuelve la estructura interna de cualquier objeto en R.\n\nstr(datos)\n\n'data.frame':   5 obs. of  4 variables:\n $ id     : int  1 2 3 4 5\n $ edad   : num  23 43 12 65 37\n $ sexo   : chr  \"V\" \"M\" \"M\" \"V\" ...\n $ trabaja: logi  TRUE TRUE FALSE FALSE TRUE\n\n\nEsta salida nos informa que:\n\ndatos es un dataframe con 5 observaciones y 4 variables.\nid es un entero (int).\nedad es numérica (num).\nsexo es de tipo carácter (chr).\ntrabaja es lógica (logi).\n\nAdemás, str() muestra los primeros valores de cada variable, que en este caso son todos, ya que el dataframe tiene solo 5 registros.\nCuando necesitemos llamar al contenido de alguna columna o variable del dataframe, utilizamos la siguiente notación:\n\nnombre_del_dataframe$nombre_de_la_variable\n\nPor ejemplo, si queremos mostrar el contenido de la variable sexo del objeto datos hacemos:\n\ndatos$sexo\n\n[1] \"V\" \"M\" \"M\" \"V\" \"M\"\n\n\nEsto devuelve todos los valores de la variable sexo.\nTambién podemos acceder a los elementos del dataframe usando indexación por filas y columnas:\n\n# pedimos la tercer variable (sexo)\ndatos[,3]\n\n[1] \"V\" \"M\" \"M\" \"V\" \"M\"\n\n\nObservemos que las dos salidas son idénticas, dado que muestran todas las observaciones de la variable sexo, aunque la solicitud sea de manera diferente.\nAlgunos otros ejemplos de uso de índices:\n\n# observación 1 de la variable 2\ndatos[1,2]\n\n[1] 23\n\n# observación 4 de todas las variables\ndatos[4,]  \n\n  id edad sexo trabaja\n4  4   65    V   FALSE\n\n# observación 1,2 y 3 de la variable 3\ndatos[1:3,3] \n\n[1] \"V\" \"M\" \"M\"\n\n# observación 5 de las variables 1 y 4\ndatos[5, c(1,4)] \n\n  id trabaja\n5  5    TRUE\n\n\nPor último, vamos podemos mostrar y gestionar los nombres de las variables con la función names():\n\nnames(datos)\n\n[1] \"id\"      \"edad\"    \"sexo\"    \"trabaja\"",
    "crumbs": [
      "Unidad 1",
      "Introducción a R"
    ]
  },
  {
    "objectID": "unidad_1/01_intro_R.html#fórmulas",
    "href": "unidad_1/01_intro_R.html#fórmulas",
    "title": "Introducción a R",
    "section": "Fórmulas",
    "text": "Fórmulas\nEn R, las fórmulas se utilizan principalmente para describir modelos estadísticos, y las vamos a emplear a lo largo de toda la cursada.\nLas fórmulas se escriben utilizando operadores como ~, +, *, entre otros. Se usan para especificar modelos como regresiones, ANOVA, pruebas de hipótesis y, en algunos casos, también para generar ciertos tipos de gráficos.\nFormula genérica\nAquí la formula está dada por el operador ~, a la izquierda está la variable de respuesta o dependiente, a la derecha la o las variables explicativas o independientes. El esquema general es el siguiente:\n\nvariable_dependiente ~ variable independiente\n\nEl símbolo ~ (llamado virgulilla) puede interpretarse como “es modelada por” o “en función de”.\nPor ejemplo, en una regresión lineal se utiliza la función base lm() y el argumento principal de esta función es una formula:\n\nregresion_lineal &lt;- lm(variable_respuesta ~ variable_explicativa,\n                       data = datos)\n\nAdemás de la fórmula, la función lm() incluye el argumento data =, que es fundamental: allí se le indica a R el dataframe en el que debe buscar las variables involucradas en el modelo.\nLa siguiente tabla muestra los usos de los los operadores más comunes dentro de una formula:\n\n\n\n\nOperador\nEjemplo\nDescripción\n\n\n\n+\n+x\nIncluye la variable x\n\n\n-\n-x\nExcluye la variable x\n\n\n:\nx : z\nIncluye la interacción de la variable x con z\n\n\n*\nx * z\nIncluye ambas variables y la interacción entre ellas",
    "crumbs": [
      "Unidad 1",
      "Introducción a R"
    ]
  },
  {
    "objectID": "unidad_1/01_intro_R.html#footnotes",
    "href": "unidad_1/01_intro_R.html#footnotes",
    "title": "Introducción a R",
    "section": "Notas",
    "text": "Notas\n\nSímbolo que aparece en la pantalla de la computadora indicando que el sistema está esperando información del usuario o que el sistema está listo para recibir instrucciones del usuario.↩︎",
    "crumbs": [
      "Unidad 1",
      "Introducción a R"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Curso Epidemiología - Nivel Avanzado",
    "section": "",
    "text": "¡Les damos la bienvenida al curso de Epidemiología Nivel Avanzado!\n\nEs un gusto recibirles en esta nueva edición del curso, diseñado para profundizar en el análisis de datos provenientes de distintos diseños de estudio mediante el uso de modelos estadísticos.\nA lo largo del curso, abordaremos herramientas analíticas avanzadas que nos permitirán modelar adecuadamente la relación entre exposiciones y desenlaces en estudios transversales, de casos y controles, de cohortes y experimentales. Nos enfocaremos en la aplicación práctica de estos modelos, en la interpretación de resultados y en los supuestos que los sustentan, promoviendo una mirada crítica sobre su uso en la investigación epidemiológica.\nEsperamos que este espacio sea una experiencia formativa enriquecedora y un nuevo impulso en su desarrollo como investigadoras e investigadores en salud.\n\n\nEl equipo docente\n\nChristian Ballejo \nAndrea Silva \nTamara Ricardo \nMicaela Gauto \nRamiro Dana Smith \nJulia Maxwell \n\n\n\n\n Volver arribaReutilizaciónCC BY-NC 4.0"
  },
  {
    "objectID": "unidad_1/02_intro_RStudio.html",
    "href": "unidad_1/02_intro_RStudio.html",
    "title": "Primeros pasos y generalidades",
    "section": "",
    "text": "Artwork por @allison_horst",
    "crumbs": [
      "Unidad 1",
      "Primeros pasos y generalidades"
    ]
  },
  {
    "objectID": "unidad_1/02_intro_RStudio.html#introducción-a-rstudio",
    "href": "unidad_1/02_intro_RStudio.html#introducción-a-rstudio",
    "title": "Primeros pasos y generalidades",
    "section": "Introducción a RStudio",
    "text": "Introducción a RStudio\nRStudio Desktop (2025, Posit Software) es un entorno de desarrollo integrado (IDE, por sus siglas en inglés) diseñado específicamente para trabajar con el lenguaje R.\nEs una herramienta multiplataforma y de código abierto que facilita la programación, el análisis de datos y la elaboración de informes científicos. Ofrece una integración fluida con otros componentes del ecosistema de R, como R Markdown, Quarto, control de versiones (por ejemplo, Git) y la gestión de proyectos.\nRStudio presenta una interfaz unificada compuesta por distintos paneles, lo que permite organizar el trabajo de forma clara y eficiente:\n\n\n\n\n\nEditor de scripts (Source): permite crear y editar scripts de R, así como documentos R Markdown o Quarto.\nConsola de R (Console): muestra la salida del código ejecutado y ejecuta código de R de forma inmediata.\nEntorno (Environment): muestra los objetos creados durante la sesión, como vectores, dataframes o funciones.\nPanel de resultados (Output): presenta los gráficos, tablas, visualizaciones en HTML y también incluye un explorador de archivos, visor de paquetes instalados y panel de ayuda.\n\nPara garantizar la reproducibilidad de los resultados, es recomendable evitar el guardado automático del historial de objetos entre sesiones, ya que puede generar confusión. Para desactivar esta opción, ir a Tools &gt; Global Options y desmarcar las casillas de las opciones Workspace y History como se muestra en la siguiente imagen:",
    "crumbs": [
      "Unidad 1",
      "Primeros pasos y generalidades"
    ]
  },
  {
    "objectID": "unidad_1/02_intro_RStudio.html#proyectos-de-rstudio",
    "href": "unidad_1/02_intro_RStudio.html#proyectos-de-rstudio",
    "title": "Primeros pasos y generalidades",
    "section": "Proyectos de RStudio",
    "text": "Proyectos de RStudio\nLos proyectos de RStudio permiten organizar de forma estructurada todo el material asociado a un análisis: scripts, informes, bases de datos, imágenes, etc. Cada proyecto se vincula a una carpeta específica del sistema de archivos, y RStudio la utiliza como directorio de trabajo por defecto. Esta carpeta puede estar ubicada en cualquier parte del sistema de almacenamiento que deseemos (disco rígido, pendrive, disco externo, etc).\nTrabajar con proyectos facilita la importación de datos y evita errores relacionados con rutas relativas o absolutas.\nCrear un proyecto\nPara crear un nuevo proyecto, se puede utilizar el menú File &gt; New Project.... También accedemos a generar un proyecto nuevo a partir de pulsar el acceso directo New Project... ubicado en la esquina superior derecha de la interfaz:\n\n\n\n\nEn cualquiera de los dos casos aparecerá un cuadro de diálogo que presenta tres opciones para crear el nuevo proyecto de RStudio:\n\n\n\n\n\nNew Directory: crea una nueva carpeta para el proyecto a la que deberemos asignarle un nombre, es la opción más habitual. Todos los archivos de configuración aparecerán asociados a esta nueva carpeta.\nExisting Directory: vincula el proyecto a una carpeta ya existente que contenga archivos previos con los que deseamos trabajar.\nVersion Control: permite clonar un repositorio (Git o SVN). Esta opción no se utilizará durante el curso.\n\nUna vez que creamos un nuevo proyecto con la opción New Directory, aparecerá una pantalla con una lista de tipos de proyectos que se pueden crear en RStudio:\n\n\n\n\nDurante este curso utilizaremos siempre la primera opción (New Project). Las demás opciones están pensadas para usos más específicos, como el desarrollo de paquetes de R, sitios web o documentos con Quarto o R Markdown.\nAl seleccionar New Project, se abrirá una nueva ventana con los siguientes campos:\n\nDirectory name: aquí debemos escribir el nombre del nuevo directorio (carpeta) que también será el nombre del proyecto. Por ejemplo, podemos llamarlo Practicas_R.\nCreate project as subdirectory of: este campo permite definir en qué ubicación del sistema de archivos se guardará el proyecto. Podemos hacer clic en el botón Browse… para abrir el explorador de archivos y seleccionar la carpeta contenedora. En nuestro ejemplo, lo ubicaremos dentro de Mis Documentos.\n\n\n\n\n\nUna vez completados estos campos, hacemos clic en Create Project.\nLos proyectos en RStudio tienen entornos independientes, lo que significa que si cerramos un proyecto o cambiamos a otro, la configuración de cada uno se mantendrá inalterable sin interferir con los demás.\nEsto incluye los scripts abiertos, el directorio de trabajo, las pestañas que dejamos activas y otros elementos del entorno que puedan ser necesarios para continuar con un análisis. Este sistema permite mantener organizados los distintos trabajos que llevamos adelante.\nEchemos un vistazo a lo que RStudio realizó:\n\nEn primer lugar el panel Files (pantalla inferior derecha) apunta a la nueva carpeta Practicas_R y dentro de ella vemos un nuevo archivo el nombre del proyecto y la extensión .Rproj. Este archivo contiene todas las configuraciones para su proyecto.\nEl otro cambio se observa en la parte superior derecha, que muestra el nombre del proyecto. Si hacemos click en él, se desplegará el menú de proyectos. Desde aquí se puede abrir y cerrar proyectos, navegar rápidamente a proyectos que se han abierto recientemente y configurar las opciones de RStudio para cada uno de ellos.\n\n\n\n\n\nAbrir un proyecto existente\nCuando el proyecto ya existe, sea porque lo creamos nosotros o porque alguien nos pasó una carpeta con un proyecto de RStudio creado vamos a visualizar dentro de esa carpeta un archivo con extensión .Rproj.\nLa forma más veloz para abrir el proyecto es ejecutar este archivo (debería abrir una sesión de RStudio con el proyecto activo). La otra forma es desde el menú superior derecho de RStudio en la opción Open project… y luego buscando en nuestro directorio el mismo archivo .Rproj.\n\n\n\n\n\n\nNota\n\n\n\nEl menú de proyectos del área superior derecha va guardando como elementos recientes los proyectos que se van abriendo y también es una forma rápida de acceder a ellos pulsando sobre estos atajos.",
    "crumbs": [
      "Unidad 1",
      "Primeros pasos y generalidades"
    ]
  },
  {
    "objectID": "unidad_1/02_intro_RStudio.html#scripts-en-rstudio",
    "href": "unidad_1/02_intro_RStudio.html#scripts-en-rstudio",
    "title": "Primeros pasos y generalidades",
    "section": "Scripts en RStudio",
    "text": "Scripts en RStudio\nComo vimos anteriormente, un script es un archivo de código que contiene una secuencia de instrucciones escritas en R. Estos scripts pueden ser reutilizados, modificados y compartidos, lo que los convierte en una herramienta fundamental para garantizar la reproducibilidad del trabajo.\nCrear un nuevo script\nTenemos varias formas de crear un script nuevo:\n\nDesde el menú superior: File &gt; New File &gt; R Script\nCon el atajo de teclado: Ctrl + Shift + N\nDesde la barra de herramientas: presionando el ícono \nEjecutar scripts\nLa forma habitual de ejecutar el contenido de un script es línea por línea, usando alguna de las siguientes opciones:\n\nPresionando el botón  del editor de código de RStudio\nMediante el atajo de teclado Ctrl + Enter\n\nPara ejecutar una línea, simplemente ubicamos el cursor en cualquier parte de ella y presionamos el comando correspondiente. Luego de ejecutarse, el cursor avanzará automáticamente a la siguiente línea de código.\nMientras ejecutamos cada línea debemos ir observando la salida en la consola (panel inferior izquierdo) y también los cambios que se dan en el panel Environment (panel superior derecho) donde aparecerán los objetos que vayamos creando y manipulando.\nEdición de scripts\nModificar o agregar líneas al script puede hacerse directamente en el editor. Cada vez que realizamos un cambio, es necesario volver a ejecutar la línea o bloque modificado para que los cambios se reflejen en el entorno de trabajo.\nPodemos probar y modificar tantas veces como sea necesario. Sin embargo, debemos tener presente que cada manipulación en los objetos se mantiene hasta que se vuelvan a cambiar y a veces, cuando los objetos están vinculados con otras líneas de código posteriores tenemos que tener cuidado que se mantenga la consistencia del script.\nPor ejemplo: si definimos un vector numérico para realizar cálculos matemáticos, pero luego lo sobrescribimos con un valor de tipo caracter, los cálculos posteriores producirán un error y RStudio nos informará de esto en la consola.\nPor eso, es clave observar el contenido de los objetos en el panel Environment, lo que nos ayuda a evitar errores y operaciones incoherentes.\nGuardado de scripts\nCualquier agregado o modificación que hayamos realizado al script y nos interese mantener nos obligará a guardar el archivo de código editado.\nExisten distintas formas de guardar un script:\n\nDesde el menú superior: File &gt; Save\nCon el atajo de teclado: Ctrl + S\nPresionando el ícono del disquete azul 💾\n\nSi en cambio quisiera guardarlo como otro archivo para mantener el script original, podemos guardarlo con diferente nombre o en otra ubicación mediante File &gt; Save As...\nAbrir scripts existentes\nLos scripts que construyamos o nos compartan siempre tendrán extensión .R y, por lo general, se encontrarán dentro de un proyecto.\nPara abrir estos archivos .R podemos:\n\nDesde el menú superior: File &gt; Open file...\nCon el atajo de teclado: Ctrl + O\nHaciendo click sobre el archivo desde el panel Files\nPresionando el botón de la carpeta amarilla 📂\n\nEsto abrirá el script en una nueva pestaña dentro del editor de código.\n¿Cómo trabajaremos en este curso?\nEn general, utilizaremos scripts dentro de proyectos de RStudio. La secuencia recomendada será:\n\nDescargar desde el Aula Virtual un archivo comprimido conteniendo la carpeta, el proyecto, los scripts y archivos de datos.\nDescomprimir el archivo en la ubicación que deseamos (recomendamos crear una carpeta destinada al curso).\nAbrir la carpeta y ejecutar el archivo de proyecto .Rproj.\nUna vez abierto RStudio con el proyecto activo, ubicamos los scripts desde el panel Files.\nEjecutar cada línea del script, leyendo la documentación del código y observando la salida en la consola y los cambios en el entorno",
    "crumbs": [
      "Unidad 1",
      "Primeros pasos y generalidades"
    ]
  },
  {
    "objectID": "unidad_1/02_intro_RStudio.html#herramientas-de-rstudio",
    "href": "unidad_1/02_intro_RStudio.html#herramientas-de-rstudio",
    "title": "Primeros pasos y generalidades",
    "section": "Herramientas de RStudio",
    "text": "Herramientas de RStudio\nAlgunas de las herramientas fundamentales de RStudio son el asistente de código, la ayuda en línea y el historial de comandos.\nAsistente de código\nAl escribir en el editor o la consola, la tecla Tab activa el autocompletado de funciones, nombres de objetos y argumentos, agilizando la escritura y reduciendo errores de sintaxis. En versiones recientes, el asistente también permite la previsualización de colores en los gráficos, resaltar los paréntesis de cierre en funciones anidadas con distintos colores y gestionar automáticamente la indentación del código.\n\n\n\n\nMuchas de estas opciones se pueden configurar desde el menú Code y desde Tools &gt; Global Options &gt; Code(pestañas Editing y Display).\n\n\n\n\nAyuda en línea\nAl posicionar el cursor sobre el nombre de una función en el editor y presionar F1, se accede directamente a la documentación correspondiente en el panel Help (habitualmente ubicado en la esquina inferior derecha).\n\n\n\n\nHistorial de comandos\nEn la consola, al usar las teclas de flecha arriba/abajo, se puede navegar por los comandos ejecutados durante la sesión actual. Además, el panel History (parte superior derecha) almacena los comandos de todas las sesiones previas, permitiendo reutilizarlos con un clic en To Console (Enter) o To Source (Shift + Enter), según se desee insertarlos en la consola o en el script activo.",
    "crumbs": [
      "Unidad 1",
      "Primeros pasos y generalidades"
    ]
  },
  {
    "objectID": "unidad_1/02_intro_RStudio.html#atajos-de-teclado-windows",
    "href": "unidad_1/02_intro_RStudio.html#atajos-de-teclado-windows",
    "title": "Primeros pasos y generalidades",
    "section": "Atajos de teclado (Windows)",
    "text": "Atajos de teclado (Windows)\n\n\n\n\nMenú\nDescripción\n\n\n\nArchivo (File)\n\n\n\nCtrl + Shift + N\nCrea un nuevo script\n\n\nCtrl + O\nAbre un script guardado\n\n\nCtrl + S\nGuarda el script activo\n\n\nCtrl + W\nCierra el script activo\n\n\nCtrl + Q\nSale del programa RStudio\n\n\nEdición (Edit)\n\n\n\nCtrl + F\nAbre la ventana de búsqueda (para buscar palabras dentro de un script)\n\n\nCtrl + L\nLimpia la consola\n\n\nCódigo (Code)\n\n\n\nCtrl + Enter\nEjecuta la línea de código donde está situado el cursor\n\n\nCtrl + Alt + R\nEjecuta todo el código del script activo\n\n\nCtrl + Shift + N\nInserta nueva sección de código\n\n\nCtrl + Shift + R\nInserta nueva sección de comentarios de texto\n\n\nSesión (Session)\n\n\n\nCtrl + Shift + H\nAbre la ventana para establecer directorio de trabajo\n\n\nCtrl + Shift + F10\nReinicia la sesión de R\n\n\nHerramientas (Tools)\n\n\n\nAlt + Shift + K\nAbre la lista de ayuda de atajos de teclado",
    "crumbs": [
      "Unidad 1",
      "Primeros pasos y generalidades"
    ]
  },
  {
    "objectID": "unidad_1/02_intro_RStudio.html#paquetes",
    "href": "unidad_1/02_intro_RStudio.html#paquetes",
    "title": "Primeros pasos y generalidades",
    "section": "Paquetes",
    "text": "Paquetes\nExisten dos formas principales de descargar paquetes: directamente desde RStudio o desde el sitio web de CRAN, descargándolos como archivos comprimidos. Si el equipo cuenta con conexión a Internet, lo más práctico es realizar la descarga directamente desde RStudio. En cambio, si no se dispone de acceso permanente a la red, es posible descargar los paquetes desde otro equipo y luego transferirlos como archivos .zip o .tar.gz al equipo donde se encuentra instalado R.\nCuando accedemos al sitio web de CRAN y buscamos un paquete específico, encontraremos información útil como una breve descripción del paquete, el número de versión, la fecha de publicación, el nombre del autor, documentación asociada y enlaces de descarga específicos para cada sistema operativo.\nDado que la mayoría de las computadoras hoy en día cuentan con acceso a Internet, en este curso nos enfocaremos en la instalación y activación de paquetes directamente desde RStudio.\nDentro del entorno de RStudio, la gestión de paquetes se realiza desde la pestaña Packages, ubicada en el panel inferior derecho. Esta interfaz facilita tareas como la instalación, actualización y activación de paquetes, y cada acción que realizamos desde la interfaz gráfica se traduce internamente en la ejecución de funciones del lenguaje R que pueden observarse en la consola.\nPara instalar un paquete nuevo, simplemente pulsamos el botón Install dentro de la pestaña Packages, lo cual abrirá una ventana emergente.\n\n\n\n\nAllí podemos escribir el nombre del paquete que deseamos instalar. Para asegurarnos de que también se descarguen e instalen automáticamente los paquetes de los que depende, es importante tildar la opción Install dependencies.\n\n\n\n\nComo alternativa, también podemos realizar la instalación desde el editor de scripts mediante el siguiente comando:\n\ninstall.packages(\"nombre_del_paquete\", \n                 dependencies = TRUE)\n\n\n\n\n\n\n\nPara facilitar la instalación de los paquetes requeridos durante el curso, recomendamos descargar el archivo “Paquetes y datos para el curso”, descomprimirlo y ejecutar el script que se encuentra en su interior.",
    "crumbs": [
      "Unidad 1",
      "Primeros pasos y generalidades"
    ]
  },
  {
    "objectID": "unidad_1/02_intro_RStudio.html#lectura-de-archivos-de-datos",
    "href": "unidad_1/02_intro_RStudio.html#lectura-de-archivos-de-datos",
    "title": "Primeros pasos y generalidades",
    "section": "Lectura de archivos de datos",
    "text": "Lectura de archivos de datos\nR permite importar tablas de datos desde diversos formatos, tanto utilizando funciones de R base como funciones provistas por paquetes específicos.\nEl formato más común es el texto plano (ASCII), donde los valores están organizados en columnas separadas por caracteres delimitadores. Los separadores más habituales incluyen:\n\nComa (,)\nPunto y coma (;)\nTabulación (\\t)\nBarra vertical (|)\n\nEstos archivos suelen tener una cabecera (header) en la primera fila con los nombres de las variables, y cada columna debe contener datos del mismo tipo (números, texto, lógicos, etc.).\nPara importar correctamente un archivo es importante conocer su estructura:\n\nSi incluye o no cabecera.\nQué carácter se usa como separador.\nEl tipo de codificación (UTF-8, Latin1, etc.).\n\nDado que son archivos de texto, pueden visualizarse con editores simples como el Bloc de Notas o desde RStudio, lo que facilita su inspección previa.\nPara cargar los datos desde un archivo de texto plano usamos el código:\n\ndatos &lt;- read.xxx(\"mis_datos.xxx\")\n\n(Se debe reemplazar read.xxx() por la función correspondiente: read.table(), read.csv(), read_delim(), read_excel(), etc., según la extensión del archivo).\nR también permite cargar bases de datos incluidas en paquetes instalados mediante:\n\ndata(nombre_datos)\n\ndatos &lt;- nombre_datos",
    "crumbs": [
      "Unidad 1",
      "Primeros pasos y generalidades"
    ]
  },
  {
    "objectID": "unidad_1/02_intro_RStudio.html#buenas-prácticas",
    "href": "unidad_1/02_intro_RStudio.html#buenas-prácticas",
    "title": "Primeros pasos y generalidades",
    "section": "Buenas prácticas",
    "text": "Buenas prácticas\nAdoptar buenas prácticas desde el inicio mejora la reproducibilidad, facilita el trabajo colaborativo y reduce errores. Algunas recomendaciones clave son:\n\nTrabajar siempre dentro de un proyecto de RStudio (.Rproj). Esto permite organizar los archivos, mantener rutas relativas consistentes y acceder a funcionalidades específicas como control de versiones o panel de archivos integrados.\nIncluir al comienzo de cada script las líneas de activación de paquetes necesarios, utilizando la función library().\nCargar los datos una vez activados los paquetes, para garantizar que todas las funciones requeridas estén disponibles.\nDocumentar el código mediante comentarios iniciados con #. Esto permite entender qué hace cada bloque de código, facilitando futuras modificaciones o revisiones.\nUsar espacios e indentación adecuada para mejorar la legibilidad. Esto es especialmente importante en estructuras anidadas (como condicionales, bucles o funciones).",
    "crumbs": [
      "Unidad 1",
      "Primeros pasos y generalidades"
    ]
  },
  {
    "objectID": "unidad_1/04_analisis_exploratorio.html#introducción",
    "href": "unidad_1/04_analisis_exploratorio.html#introducción",
    "title": "Análisis exploratorio de datos",
    "section": "Introducción",
    "text": "Introducción\nEl análisis exploratorio de datos (conocido como EDA, su sigla en inglés) es un enfoque de análisis fundamental para resumir y visualizar las características importantes de un conjunto de datos.\nJohn Tukey, estadístico estadounidense, fue uno de los principales impulsores de este enfoque. En 1977 publicó el libro Exploratory Data Analysis, donde, entre otras contribuciones, introdujo el gráfico boxplot (diagrama de caja y bigotes).\nEn términos sencillos, antes de avanzar hacia el análisis formal o la construcción de modelos estadísticos, resulta esencial explorar, conocer y describir las variables presentes en nuestra tabla de datos.\nEntre los principales objetivos perseguidos por EDA se encuentran:\n\nConocer la estructura de la tabla de datos y sus tipos de variable.\nDetectar observaciones incompletas (valores missing o NA).\nExplorar la distribución de las variables de interés a partir de:\n\nEstadísticos descriptivos\nRepresentaciones gráficas\n\n\nDetectar valores atípicos (outliers).\n\nAclaración\nEn este documento utilizaremos funciones del lenguaje R basadas en la filosofía tidyverse, junto con otros paquetes diseñados para tareas específicas. Esto no implica que no se puedan emplear funciones del R base; sin embargo, el ecosistema tidyverse facilita la comprensión y legibilidad del código.\nPresentaremos estas diferentes funciones de distintos paquetes que pueden servir en cada etapa de un EDA. Los paquetes con los que trabajaremos son:\n\ntidyverse\nskimr\ndlookr\njanitor\n\n\n\n\n\n\n\nNota: Algunos paquetes, como dlookr, pueden generar falsos positivos en la detección del antivirus durante el proceso de instalación. Sugerimos desactivar momentáneamente el antivirus para evitar inconvenientes.\n\n\n\nUna vez instalados, podemos activar los paquetes con el siguiente código:\n\n# Carga de paquetes\nlibrary(skimr)\nlibrary(janitor)\nlibrary(dlookr)\nlibrary(tidyverse)\n\nSe recomienda cargar tidyverse al final de la lista para evitar conflictos con funciones que puedan solaparse entre paquetes.\n\nEs importante destacar que no existe un único camino y/o función para realizar un análisis exploratorio. Esta selección de herramientas puede adaptarse según las preferencias y necesidades de cada usuario. Por lo tanto, quienes ya tengan familiaridad con otras funciones o paquetes pueden continuar utilizándolos sin inconvenientes.\n\nPara ilustrar los pasos del análisis exploratorio, utilizaremos un archivo con datos ficticios llamado “datos2.txt”, que contiene variables de distintos tipos.",
    "crumbs": [
      "Unidad 1",
      "Análisis exploratorio de datos"
    ]
  },
  {
    "objectID": "unidad_1/04_analisis_exploratorio.html#conocer-la-estructura-de-la-tabla-de-datos-y-sus-tipos-de-variable",
    "href": "unidad_1/04_analisis_exploratorio.html#conocer-la-estructura-de-la-tabla-de-datos-y-sus-tipos-de-variable",
    "title": "Análisis exploratorio de datos",
    "section": "Conocer la estructura de la tabla de datos y sus tipos de variable",
    "text": "Conocer la estructura de la tabla de datos y sus tipos de variable\nEl primer paso en la exploración de un conjunto de datos es conocer su estructura y tamaño:\n\nEl tamaño se refiere a la cantidad de observaciones (filas) y de variables (columnas).\nLa estructura incluye cómo están organizadas las variables, qué tipo de datos contiene cada una y qué categorías o valores pueden tomar.\n\nComenzaremos por cargar los datos de ejemplo con la función read_csv2() de tidyverse:\n\ndatos &lt;- read_csv2(\"datos/datos2.txt\")\n\nUna vez cargados los datos, la función glimpse() permite obtener una visión general de la tabla:\n\nglimpse(datos)\n\nRows: 74\nColumns: 7\n$ id      &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18,…\n$ sexo    &lt;chr&gt; \"M\", \"M\", \"M\", \"M\", \"M\", \"M\", \"M\", \"M\", NA, \"F\", \"F\", \"M\", \"F\"…\n$ edad    &lt;dbl&gt; 76, 68, 50, 49, 51, 68, 70, 64, 60, 57, 83, 76, 27, 34, 17, 45…\n$ peso    &lt;dbl&gt; 71, 71, 79, 71, 87, 75, 80, 83, 69, 73, 60, 70, 648, 718, 61, …\n$ talla   &lt;dbl&gt; 167, 164, 164, 164, 1675, 170, 166, 160, 160, 155, 155, 167, 1…\n$ trabaja &lt;lgl&gt; FALSE, FALSE, FALSE, TRUE, TRUE, FALSE, NA, TRUE, TRUE, TRUE, …\n$ fecha   &lt;date&gt; 2020-10-20, 2020-10-20, 2020-10-20, 2020-11-05, 2020-11-05, 2…\n\n\nEsta función nos informa que la tabla contiene, por ejemplo, 74 observaciones y 7 variables, mostrando el tipo de dato de cada una y los primeros valores que aparecen.\nEntre los tipos de datos más comunes que podemos encontrar se incluyen:\n\nint (integer): números enteros.\ndbl (double): números reales.\nlgl (logical): valores lógicos (TRUE, FALSE).\nchr (character): texto o cadenas de caracteres.\nDate: fechas.\nfct (factor): variables categóricas con niveles.\ndttm (date-time): fechas y horas.\n\nEsta primera revisión de la estructura suele complementarse con el diccionario de datos, un recurso fundamental que describe el significado, tipo, unidad y codificación de cada variable. Este diccionario puede acompañar tanto a bases de datos generadas en investigaciones propias (fuentes primarias) como a datos provenientes de fuentes secundarias.\nEs importante tener en cuenta que el tipo de dato en R no siempre coincide con la naturaleza estadística de la variable. Por ejemplo:\n\nUna variable codificada como dbl puede representar una medida cuantitativa continua, como la edad o el peso.\nPero también puede representar una variable cualitativa codificada con números. Por ejemplo, si una variable que registra respuestas “Sí” y “No” fue codificada como 1 y 0, su tipo de dato será numérico (dbl o int), aunque conceptualmente sea una variable categórica.\n\nPor esta razón, además de inspeccionar el tipo de datos en R, es importante revisar el significado y el uso previsto de cada variable dentro del contexto del análisis.",
    "crumbs": [
      "Unidad 1",
      "Análisis exploratorio de datos"
    ]
  },
  {
    "objectID": "unidad_1/04_analisis_exploratorio.html#detectar-observaciones-incompletas",
    "href": "unidad_1/04_analisis_exploratorio.html#detectar-observaciones-incompletas",
    "title": "Análisis exploratorio de datos",
    "section": "Detectar observaciones incompletas",
    "text": "Detectar observaciones incompletas\nLos valores perdidos o faltantes (conocidos como missing en inglés), representados en R por el valor especial NA, constituyen un desafío importante en el análisis de datos. Su presencia puede afectar la calidad del análisis y condicionar las decisiones estadísticas posteriores.\nExisten numerosos enfoques para el tratamiento de valores faltantes, incluyendo técnicas de imputación y modelado específico. Sin embargo, en este curso nos enfocaremos exclusivamente en cómo detectar, contabilizar y, en algunos casos, excluir valores faltantes utilizando funciones del lenguaje R.\nUna forma sencilla de detectar valores faltantes es mediante la función count() del paquete dplyr. Al aplicarla a una variable, la salida incluye una fila adicional que informa cuántos valores NA hay:\n\ndatos |&gt; \n  count(trabaja)\n\n# A tibble: 3 × 2\n  trabaja     n\n  &lt;lgl&gt;   &lt;int&gt;\n1 FALSE      26\n2 TRUE       39\n3 NA          9\n\n\nUna alternativa más completa es la función find_na() del paquete dlookr (Ryu 2024):\n\nfind_na(datos, rate = T)\n\n     id    sexo    edad    peso   talla trabaja   fecha \n  0.000   4.054   0.000   0.000   0.000  12.162   0.000 \n\n\nEsta función se puede aplicar al conjunto de datos completo y devuelve, para cada variable, la cantidad y el porcentaje de valores NA. Por ejemplo, podríamos observar que la variable sexo tiene alrededor de un 4 % de valores faltantes, y la variable trabaja, algo más del 12 %.\nEstos porcentajes pueden ayudarnos a decidir si una variable debe incluirse en un análisis o si es conveniente excluir ciertas observaciones con datos incompletos, siempre que los NA sean el resultado de una ausencia real de información.\nEl mismo paquete trae una función gráfica llamada plot_na_pareto(), que genera un gráfico de barras ordenados por frecuencia de valores faltantes:\n\nplot_na_pareto(datos, only_na = T)\n\n\n\n\n\n\n\nFinalmente, para un diagnóstico más integral de la calidad de las variables, puede utilizarse la función diagnose():\n\ndiagnose(datos)\n\n# A tibble: 7 × 6\n  variables types     missing_count missing_percent unique_count unique_rate\n  &lt;chr&gt;     &lt;chr&gt;             &lt;int&gt;           &lt;dbl&gt;        &lt;int&gt;       &lt;dbl&gt;\n1 id        numeric               0            0              74      1     \n2 sexo      character             3            4.05            3      0.0405\n3 edad      numeric               0            0              45      0.608 \n4 peso      numeric               0            0              56      0.757 \n5 talla     numeric               0            0              38      0.514 \n6 trabaja   logical               9           12.2             3      0.0405\n7 fecha     Date                  0            0              11      0.149 \n\n\nEsta función ofrece un resumen detallado que incluye el tipo de variable, la cantidad de valores faltantes, la proporción de valores únicos, entre otros indicadores de utilidad para la exploración inicial.",
    "crumbs": [
      "Unidad 1",
      "Análisis exploratorio de datos"
    ]
  },
  {
    "objectID": "unidad_1/04_analisis_exploratorio.html#conocer-la-distribución-de-las-variables-de-interés",
    "href": "unidad_1/04_analisis_exploratorio.html#conocer-la-distribución-de-las-variables-de-interés",
    "title": "Análisis exploratorio de datos",
    "section": "Conocer la distribución de las variables de interés",
    "text": "Conocer la distribución de las variables de interés\nResumir variables cuantitativas\nLa instalación básica de R tiene incorporadas múltiples funciones estadísticas que permiten calcular medidas resumen para variables cuantitativas. Estas funciones pueden integrarse a la función summarise() de tidyverse.\nMedidas de tendencia central\nLas medidas de tendencia central forman parte del grupo de medidas de posición o localización, pero su objetivo principal es resumir la información en torno a un valor que representa el “centro” de la distribución. Es decir, un valor respecto al cual tienden a agruparse los demás valores.\nPodemos obtener la media y la mediana de nuestros datos con el siguiente código:\n\ndatos |&gt;\n  summarise(\n    # Media\n    media = mean(edad),\n    # Mediana\n    mediana = median(edad)\n  )\n\n# A tibble: 1 × 2\n  media mediana\n  &lt;dbl&gt;   &lt;dbl&gt;\n1  48.1    52.5\n\n\nEn cambio, R base no incluye una función específica para calcular la moda. Para obtenerla, debemos escribir una función propia o utilizar algún paquete adicional que la implemente (por ejemplo, modeest::mlv()).\nMedidas de posición\nLas medidas de posición dividen los datos en grupos con igual número de observaciones. Entre las más utilizadas se encuentran los cuartiles y percentiles.\nLa función quantile() del paquete base stats permite calcular cuartiles u otros percentiles. Por ejemplo, para calcular los cuartiles Q1 y Q3, indicamos en el argumento probs los valores 0.25 y 0.75:\n\ndatos |&gt;\n  summarise(\n    # Primer cuartil\n    cuartil1 = quantile(edad, probs = 0.25),\n    # Tercer cuartil\n    cuartil3 = quantile(edad, probs = 0.75)\n  )\n\n# A tibble: 1 × 2\n  cuartil1 cuartil3\n     &lt;dbl&gt;    &lt;dbl&gt;\n1       28       64\n\n\nPara obtener el mínimo y máximo de estos valores numéricos usamos el siguiente código:\n\ndatos |&gt;\n  summarise(\n    # Mínimo\n    minimo = min(edad),\n    # Máximo\n    maximo = max(edad)\n  )\n\n# A tibble: 1 × 2\n  minimo maximo\n   &lt;dbl&gt;  &lt;dbl&gt;\n1     13     86\n\n\nMedidas de dispersión\nLas medidas de dispersión nos permiten conocer cuán dispersos o variables son los valores dentro del conjunto de datos.\nEntre las más clásicas se encuentran la varianza y el desvío estándar, que se calculan fácilmente con las funciones var() y sd():\n\ndatos |&gt;\n  summarise(\n    # Varianza\n    varianza = var(edad),\n    # Desvío estándar\n    desvio = sd(edad)\n  )\n\n# A tibble: 1 × 2\n  varianza desvio\n     &lt;dbl&gt;  &lt;dbl&gt;\n1     405.   20.1\n\n\nTambién puede ser útil calcular el rango, que se obtiene como la diferencia entre el valor máximo y el mínimo, y el rango intercuartílico (RIC), mediante IQR():\n\ndatos |&gt;\n  summarise(\n    # Rango\n    rango = max(edad) - min(edad),\n    # Rango intercuartílico\n    ric = IQR(edad)\n  )\n\n# A tibble: 1 × 2\n  rango   ric\n  &lt;dbl&gt; &lt;dbl&gt;\n1    73    36\n\n\nEl paquete dlookr ofrece la función describe() para generar un resumen completo de las variables numéricas:\n\ndescribe(datos, -id)\n\n# A tibble: 3 × 26\n  described_variables     n    na  mean    sd se_mean   IQR skewness kurtosis\n  &lt;chr&gt;               &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 edad                   74     0  48.1  20.1    2.34  36     -0.211    -1.11\n2 peso                   74     0 358.  323.    37.6  626.     0.451    -1.41\n3 talla                  74     0 363.  505.    58.7   12.8    2.19      2.90\n# ℹ 17 more variables: p00 &lt;dbl&gt;, p01 &lt;dbl&gt;, p05 &lt;dbl&gt;, p10 &lt;dbl&gt;, p20 &lt;dbl&gt;,\n#   p25 &lt;dbl&gt;, p30 &lt;dbl&gt;, p40 &lt;dbl&gt;, p50 &lt;dbl&gt;, p60 &lt;dbl&gt;, p70 &lt;dbl&gt;,\n#   p75 &lt;dbl&gt;, p80 &lt;dbl&gt;, p90 &lt;dbl&gt;, p95 &lt;dbl&gt;, p99 &lt;dbl&gt;, p100 &lt;dbl&gt;\n\n\nEsta función puede aplicarse directamente sobre todo el conjunto de datos. Si bien selecciona automáticamente las variables numéricas, en este caso estamos excluyendo explícitamente la variable id, ya que un identificador no tiene interés estadístico.\nEl resumen que devuelve incluye:\n\nna: cantidad de observaciones con datos y con NA.\nmean: media aritmética.\nsd: desvío estándar de la media.\nse_mean: error estándar de la media.\nIQR: rango intercuartílico.\nMedidas de forma como la simetría (skewness) y la curtosis (kurtosis).\nPercentiles, incluyendo la mediana (P50) y los cuartiles (P25 y P75).\nResumir variables cualitativas\nLas variables cualitativas o categóricas pueden encontrarse en R bajo los tipos de dato character o factor. En ocasiones será necesario convertirlas a factor, ya que este tipo permite aplicar ciertos procedimientos específicos para variables categóricas.\nFrecuencias\nPodemos resumir individualmente variables cualitativas mediante las frecuencias absolutas y relativas de sus categorías. La función count() de dplyr nos muestra el conteo absoluto:\n\ndatos |&gt; \n  count(sexo)\n\n# A tibble: 3 × 2\n  sexo      n\n  &lt;chr&gt; &lt;int&gt;\n1 F        27\n2 M        44\n3 &lt;NA&gt;      3\n\n\nEn la salida se incluirán, además de las categorías presentes, las observaciones con valores faltantes (NA).\nLa inclusión o no de los valores faltantes dependerá del propósito del análisis. Para excluirlos, podemos utilizar drop_na():\n\ndatos |&gt; \n  count(sexo) |&gt; \n  # Saltea los valores NA\n  drop_na()  \n\n# A tibble: 2 × 2\n  sexo      n\n  &lt;chr&gt; &lt;int&gt;\n1 F        27\n2 M        44\n\n\nPara obtener frecuencias relativas en porcentaje:\n\ndatos |&gt;  \n  count(sexo) |&gt;  \n  # Saltea los valores NA\n  drop_na() |&gt; \n  # Transforma a porcentajes\n  mutate(porc = 100 * n / sum(n))\n\n# A tibble: 2 × 3\n  sexo      n  porc\n  &lt;chr&gt; &lt;int&gt; &lt;dbl&gt;\n1 F        27  38.0\n2 M        44  62.0\n\n\nRedondeamos el valor del porcentaje con round():\n\ndatos |&gt;  \n  count(sexo) |&gt;  \n  # Saltea los valores NA\n  drop_na() |&gt; \n  # Transforma a porcentajes y redondea decimales\n  mutate(\n    porc = 100 * n / sum(n),\n    porc = round(porc, digits = 2)\n  )\n\n# A tibble: 2 × 3\n  sexo      n  porc\n  &lt;chr&gt; &lt;int&gt; &lt;dbl&gt;\n1 F        27  38.0\n2 M        44  62.0\n\n\nEl paquete janitor (Firke 2024) ofrece una alternativa más completa mediante la función tabyl():\n\ndatos |&gt;\n  tabyl(sexo)\n\n sexo  n    percent valid_percent\n    F 27 0.36486486     0.3802817\n    M 44 0.59459459     0.6197183\n &lt;NA&gt;  3 0.04054054            NA\n\n\nEsta función muestra tanto frecuencias absolutas como relativas, incluyendo y excluyendo los valores NA (porcentaje sobre el total de valores válidos).\nPodemos mejorar la presentación combinando otras funciones del paquete:\n\ndatos |&gt;  \n  # Excluímos valores NA\n  tabyl(sexo, show_na = F) |&gt; \n  # Añadimos totales por fila\n  adorn_totals(where = \"row\") |&gt;  \n  # Redondea porcentajes a 2 decimales\n  adorn_pct_formatting(digits = 2) \n\n  sexo  n percent\n     F 27  38.03%\n     M 44  61.97%\n Total 71 100.00%\n\n\nTablas de contingencia\nLa forma más adecuada de describir la relación entre dos variables cualitativas es a través de una tabla de contingencia, en la cual:\n\nLas filas representan las categorías de una variable.\nLas columnas representan las categorías de otra variable.\nLas celdas muestran el número de observaciones correspondientes a cada combinación de categorías.\n\nLa función tabyl() también permite crear este tipo de tablas. A continuación, un ejemplo entre sexo y trabaja (aunque trabaja sea lógica, puede tratarse como categórica):\n\ndatos |&gt;  \n  tabyl(sexo, trabaja) \n\n sexo FALSE TRUE NA_\n    F     8   15   4\n    M    17   22   5\n &lt;NA&gt;     1    2   0\n\n\nRecordemos que el orden dentro de los paréntesis de la función es igual al de los índices, el primer argumento es la variable que aparecerá en las filas y el segundo la variable de las columnas. Por ese motivo, en la tabla de contingencia absoluta tenemos sexo en las filas y trabaja en las columnas.\nSe puede mejorar la tabla excluyendo los valores NA y agregando totales por fila:\n\ndatos |&gt;  \n  # Excluímos valores NA\n  tabyl(sexo, trabaja, show_na = F) |&gt; \n  # Añadimos totales por fila\n  adorn_totals(where = \"row\")\n\n  sexo FALSE TRUE\n     F     8   15\n     M    17   22\n Total    25   37\n\n\nPara calcular frecuencias relativas porcentuales por columna usamos el siguiente código:\n\ndatos |&gt;  \n  # Excluímos valores NA\n  tabyl(sexo, trabaja, show_na = F) |&gt; \n  # Añadimos totales\n  adorn_totals(where = \"row\") |&gt; \n  # Añadimos porcentajes por columna\n  adorn_percentages(denominator = \"col\") |&gt; \n  # Redondea porcentajes a 2 decimales\n  adorn_pct_formatting(digits = 2) \n\n  sexo   FALSE    TRUE\n     F  32.00%  40.54%\n     M  68.00%  59.46%\n Total 100.00% 100.00%\n\n\nCalculamos frecuencias relativas porcentuales por fila:\n\ndatos |&gt;  \n  # Excluímos valores NA\n  tabyl(sexo, trabaja, show_na = F) |&gt; \n  # Añadimos totales por columna\n  adorn_totals(where = \"col\") |&gt; \n  # Añadimos porcentajes por fila\n  adorn_percentages(denominator = \"row\") |&gt; \n  # Redondea porcentajes a 2 decimales\n  adorn_pct_formatting(digits = 2)\n\n sexo  FALSE   TRUE   Total\n    F 34.78% 65.22% 100.00%\n    M 43.59% 56.41% 100.00%\n\n\nCambiando el argumento denominator por \"all\" se calculan frecuencias relativas al total:\n\ndatos |&gt;  \n  # Excluímos valores NA\n  tabyl(sexo, trabaja, show_na = F) |&gt; \n  # Añadimos totales por columna\n  adorn_totals(where = \"col\") |&gt; \n  # Añadimos porcentajes al total\n  adorn_percentages(denominator = \"all\") |&gt; \n  # Redondea porcentajes a 2 decimales\n  adorn_pct_formatting(digits = 2)\n\n sexo  FALSE   TRUE  Total\n    F 12.90% 24.19% 37.10%\n    M 27.42% 35.48% 62.90%\n\n\nExplorar variables mediante gráficos\nUno de los aportes más importantes de John Tukey al análisis de datos es la incorporación de los gráficos como herramienta exploratoria. A través de representaciones visuales podemos detectar rápidamente patrones, anomalías, valores extremos, asimetrías o relaciones entre variables.\nEn R, los gráficos más útiles para explorar la distribución univariada de las variables son:\n\nPara variables cualitativas: gráficos de barras\nPara variables cuantitativas: histogramas, gráficos de densidad, boxplots y violin plots\n\nCuando queremos explorar la relación entre dos o más variables, los tipos de gráficos más comunes incluyen:\n\nDiagramas de dispersión (puntos)\nGráficos de líneas\nGráficos de mosaico para variables categóricas cruzadas\n\n\n\nEl lenguaje R soporta una serie de sistemas gráficos asociados a paquetes como graphics, lattice, ggplot2, etc. que sirven de base incluso para otros paquetes con funciones más específicas. Actualmente el estándar gráfico en R es ggplot2.\nEn el documento dedicado a tidyverse ya explicamos cómo funciona ggplot2. Aquí nos concentraremos únicamente en aplicar distintos elementos geométricos (geoms) para representar las variables según su tipo.\nBarras (univariado)\nEl gráfico de barras permite visualizar la frecuencia de las categorías de una variable cualitativa:\n\ndatos |&gt; \n  # Omitimos los NA de sexo\n  drop_na(sexo) |&gt; \n  # Generamos histograma\n  ggplot(aes(x = sexo, fill = sexo)) + \n  geom_bar() + \n  scale_fill_manual(values = c(\"palevioletred4\", \"orange\")) +\n  theme_minimal()\n\n\n\n\n\n\n\nBarras (bivariado)\nCuando cruzamos dos variables categóricas, podemos representar la relación entre ambas modificando el argumento position de geom_bar().\nEl argumento position = \"stack\" nos muestra los valores absolutos acumulados:\n\ndatos |&gt; \n  # Omitimos los NA de sexo y trabaja\n  drop_na(sexo, trabaja) |&gt; \n  # Generamos gráfico de barras\n  ggplot(aes(x = sexo, fill = trabaja)) + \n  geom_bar(position = \"stack\") + \n  scale_fill_brewer(palette = \"Set1\") +\n  theme_minimal()\n\n\n\n\n\n\n\nPor otro lado, el argumento position = \"dodge\" muestra las barras lado a lado, permitiendo comparar proporciones entre grupos:\n\ndatos |&gt; \n  # Omitimos los NA de sexo y trabaja\n  drop_na(sexo, trabaja) |&gt; \n  # Generamos gráfico de barras\n  ggplot(aes(x = sexo, fill = trabaja)) + \n  geom_bar(position = \"dodge\") + \n  scale_fill_brewer(palette = \"Set1\") +\n  theme_minimal()\n\n\n\n\n\n\n\nFinalmente, position = \"fill\" convierte las alturas en proporciones sobre el total por grupo:\n\ndatos |&gt; \n  # Omitimos los NA de sexo y trabaja\n  drop_na(sexo, trabaja) |&gt; \n  # Generamos gráfico de barras\n  ggplot(aes(x = sexo, fill = trabaja)) + \n  geom_bar(position = \"fill\") + \n  scale_fill_brewer(palette = \"Set1\") +\n  theme_minimal()\n\n\n\n\n\n\n\nHistograma\nRepresenta la frecuencia de valores en intervalos definidos. Útil para observar la forma general de la distribución:\n\ndatos |&gt; \n  # Genera histograma\n  ggplot(aes(x = edad)) +\n  geom_histogram(binwidth = 10,\n                 fill = \"royalblue1\",\n                 color = \"white\"\n                )\n\n\n\n\n\n\n\nDensidad\nEs una estimación suave de la distribución de frecuencias:\n\ndatos |&gt; \n  ggplot(aes(x = edad)) + \n  geom_density(fill = \"thistle1\")  \n\n\n\n\n\n\n\nBoxplot\nMuestra el rango intercuartílico, la mediana y los valores atípicos. Ideal para detectar asimetrías y outliers:\n\ndatos |&gt; \n  # Genera boxplot\n  ggplot(aes(x = edad)) + \n  geom_boxplot(fill = \"seagreen4\")  \n\n\n\n\n\n\n\nViolinplot\nCombina el boxplot con una curva de densidad reflejada. Permite visualizar tanto la forma de la distribución como los cuantiles:\n\ndatos |&gt; \n  # Omitimos los NA de sexo\n  drop_na(sexo) |&gt; \n  # Genera violinplot\n  ggplot(aes(x = edad, y = sexo, fill = sexo)) + \n  geom_violin() +\n  scale_fill_brewer(palette = \"Set2\") +\n  theme_light()\n\n\n\n\n\n\n\nQ-Q Plot\nLos gráficos Q-Q (cuantil-cuantil) permiten evaluar visualmente si una variable sigue una distribución teórica, como la normal. Suelen usarse como método gráfico para analizar “normalidad”, es decir cuanto se asemeja la distribución de la variable a la distribución normal o gaussiana.\nLa función plot_normality() de dlookr muestra un diagnóstico gráfico de normalidad de una variable usando histogramas y Q-Q plot. Además muestra otros histogramas con conversiones de datos (logarítmico y raíz cuadrada por defecto, pero también “Box-Cox” y otras):\n\n# Sobre la variable edad\ndatos |&gt; \n  plot_normality(edad)\n\n\n\n\n\n\n# Sobre la variable peso\ndatos |&gt; \n  plot_normality(peso)\n\n\n\n\n\n\n\nPodemos decir que la variable peso se ajusta mejor a una distribución normal, ya que los puntos del Q-Q plot se alinean más cercanamente a la diagonal teórica.\n\nNota: Este análisis gráfico de normalidad suele complementarse con pruebas estadísticas específicas, que abordaremos en la próxima unidad.",
    "crumbs": [
      "Unidad 1",
      "Análisis exploratorio de datos"
    ]
  },
  {
    "objectID": "unidad_1/04_analisis_exploratorio.html#detección-de-valores-atípicos",
    "href": "unidad_1/04_analisis_exploratorio.html#detección-de-valores-atípicos",
    "title": "Análisis exploratorio de datos",
    "section": "Detección de valores atípicos",
    "text": "Detección de valores atípicos\nUn valor atípico (outlier) es una observación que se encuentra numéricamente alejada del resto de los datos. Su presencia puede tener diferentes causas, y su tratamiento dependerá del contexto:\n\nErrores de carga o procedimiento: deben corregirse si se detectan.\nValores extremos plausibles: pueden ser válidos, pero conviene evaluarlos en detalle.\nEventos extraordinarios o causas desconocidas: si no se pueden justificar, suelen excluirse del análisis.\n\nEstos valores pueden afectar sensiblemente ciertos estadísticos como la media, distorsionando su interpretación.\nUna forma gráfica común de detectar valores atípicos es mediante los boxplots. Los puntos situados fuera de los “bigotes” representan posibles outliers.\nA continuación, se presenta un ejemplo con la variable peso, donde se observa un valor extremo en el límite superior de la distribución (punto rojo):\n\ndatos |&gt; \n  ggplot(aes(x = peso)) + \n  geom_boxplot(fill = \"darkkhaki\", \n               outlier.color = \"red\"\n              )  \n\n\n\n\n\n\n\nEste valor coincide con el máximo observado:\n\nmax(datos$peso)\n\n[1] 1105\n\n\nEl paquete dlookr incluye la función diagnose_outlier() para la detección automatizada de valores atípicos en todas las variables numéricas de un conjunto de datos:\n\ndiagnose_outlier(datos)\n\n# A tibble: 4 × 6\n  variables outliers_cnt outliers_ratio outliers_mean with_mean without_mean\n  &lt;chr&gt;            &lt;int&gt;          &lt;dbl&gt;         &lt;dbl&gt;     &lt;dbl&gt;        &lt;dbl&gt;\n1 id                   0            0             NaN      37.5         37.5\n2 edad                 0            0             NaN      48.1         48.1\n3 peso                 0            0             NaN     358.         358. \n4 talla               10           13.5          1630     363.         165. \n\n\nEsta función devuelve una tabla que incluye, para cada variable: cantidad y proporción de outliers detectados, media de la variable incluyendo los outliers, media de la variable excluyendo los outliers. En función de estos dos estadísticos se puede comparar el efecto de los valores atípicos en la media.\nEl paquete skimr (Waring et al. 2022) permite obtener un resumen estadístico compacto y amigable de un conjunto de datos mediante la función skim():\n\nskim(datos)\n\n\nData summary\n\n\nName\ndatos\n\n\nNumber of rows\n74\n\n\nNumber of columns\n7\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n1\n\n\nDate\n1\n\n\nlogical\n1\n\n\nnumeric\n4\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\nsexo\n3\n0.96\n1\n1\n0\n2\n0\n\n\nVariable type: Date\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nmedian\nn_unique\n\n\nfecha\n0\n1\n2020-10-20\n2020-12-15\n2020-11-11\n11\n\n\nVariable type: logical\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\ncount\n\n\ntrabaja\n9\n0.88\n0.6\nTRU: 39, FAL: 26\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\nid\n0\n1\n37.50\n21.51\n1\n19.25\n37.5\n55.75\n74\n▇▇▇▇▇\n\n\nedad\n0\n1\n48.07\n20.12\n13\n28.00\n52.5\n64.00\n86\n▇▃▇▇▃\n\n\npeso\n0\n1\n358.05\n323.31\n42\n75.00\n91.5\n700.50\n1105\n▇▁▂▃▁\n\n\ntalla\n0\n1\n363.09\n504.93\n148\n161.00\n166.0\n173.75\n1745\n▇▁▁▁▁\n\n\n\n\n\nAdemás, puede integrarse fácilmente con la gramática tidyverse. Por ejemplo, podemos explorar estadísticas descriptivas de variables numéricas agrupadas por sexo:\n\ndatos |&gt; \n  # Excluye NAs de sexo\n  drop_na(sexo) |&gt;\n  # Agrupa por sexo\n  group_by(sexo) |&gt;\n  # Solo variables numéricas - id\n  select(where(is.numeric), -id) |&gt; \n  # Explora outliers\n  skim()\n\n\nData summary\n\n\nName\nselect(…)\n\n\nNumber of rows\n71\n\n\nNumber of columns\n4\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nsexo\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nsexo\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\nedad\nF\n0\n1\n41.89\n19.64\n15\n26.00\n39.0\n54.50\n83\n▇▃▅▃▂\n\n\nedad\nM\n0\n1\n51.59\n19.56\n13\n40.75\n55.5\n64.75\n86\n▅▂▅▇▂\n\n\npeso\nF\n0\n1\n426.78\n300.85\n42\n70.50\n516.0\n677.00\n856\n▇▁▃▃▆\n\n\npeso\nM\n0\n1\n306.95\n331.27\n64\n78.00\n86.5\n647.00\n1105\n▇▁▁▂▁\n\n\ntalla\nF\n0\n1\n421.74\n562.57\n148\n155.50\n161.0\n167.50\n1625\n▇▁▁▁▂\n\n\ntalla\nM\n0\n1\n340.77\n485.79\n158\n164.00\n168.5\n177.25\n1745\n▇▁▁▁▁\n\n\n\n\n\nEn este ejemplo, mostramos resultados de variables numéricas menos de id agrupados por sexo (sin considerar valores NA en las categorías de sexo).",
    "crumbs": [
      "Unidad 1",
      "Análisis exploratorio de datos"
    ]
  },
  {
    "objectID": "unidad_2/01_est_epidemiologicos.html",
    "href": "unidad_2/01_est_epidemiologicos.html",
    "title": "Estudios epidemiológicos",
    "section": "",
    "text": "Un estudio epidemiológico es un proceso que se desarrolla en varias fases: desde la definición del problema de investigación, la selección de la estrategia o diseño del estudio, hasta la planificación de las actividades. En esencia, el diseño constituye la estrategia metodológica elegida para alcanzar los objetivos planteados."
  },
  {
    "objectID": "unidad_2/01_est_epidemiologicos.html#clasificación-de-estudios-epidemiológicos",
    "href": "unidad_2/01_est_epidemiologicos.html#clasificación-de-estudios-epidemiológicos",
    "title": "Estudios epidemiológicos",
    "section": "Clasificación de estudios epidemiológicos",
    "text": "Clasificación de estudios epidemiológicos\nExisten distintos criterios para clasificar los estudios epidemiológicos. La siguiente tabla (Tabla 1) resume esta clasificación, que probablemente ya hayan abordado en cursos previos:\n\n\n\nTabla 1: Clasificación de estudios epidemiológicos.\n\n\n\n\n\n\n\n\n\n\n\n\nEstudios descriptivos\n\n\nEstudios analíticos\n\n\n\nPoblacionales\nIndividuales\nObservacionales\nExperimentales\n\n\n\n\nAnálisis de situación\nSerie de casos\nEstudios de casos y controles\nEnsayos clínicos\n\n\nEstudios Ecológicos\nReporte de casos\nEstudios de cohortes\nEnsayos comunitarios y/o de campo\n\n\n\nEstudios transversales\n\n\n\n\n\n\n\n\n\n\nToda investigación debe partir de una revisión actualizada del conocimiento sobre el tema a abordar. Esto implica delimitar el problema de investigación de forma precisa, definiendo objetivos, propósito, justificación e hipótesis. Para ello, es fundamental conocer las características y ventajas de cada tipo de estudio epidemiológico, evaluar su adecuación a los objetivos planteados y considerar los recursos disponibles en términos de tiempo, personal y presupuesto.\nClasificación GRADE\nExisten propuestas que jerarquizan la calidad de la evidencia según el tipo de diseño, bajo el supuesto de que algunos están más expuestos a sesgos que otros. Esta jerarquización permite orientar la toma de decisiones clínicas y de salud pública.\nEl análisis riguroso de la evidencia facilita establecer grados de recomendación para intervenciones diagnósticas, terapéuticas o preventivas. Uno de los sistemas más utilizados para clasificar la calidad de la evidencia y la fuerza de las recomendaciones es GRADE (Grading of Recommendations Assessment, Development and Evaluation), que permite sintetizar la calidad de la evidencia según el tipo de diseño:\n\nEnsayos clínicos aleatorizados (EC): calidad alta\nEstudios observacionales: calidad baja\nOtras fuentes: calidad muy baja\n\nLa siguiente tabla (Tabla 2) muestra la clasificación de GRADE, que combina calidad metodológica, balance entre beneficios y riesgos, y las implicancias de cada recomendación:\n\n\n\nTabla 2: Clasificación GRADE de estudios epidemiológicos.\n\n\n\n\nGrado\nRecomendación\nCalidad\nBeneficio vs. Riesgo y cargas\nCalidad metodológica que apoya la evidencia\nImplicancias\n\n\n\n1.A\nFuerte\nAlta\nSuperan ampliamente los riesgos y cargas (o viceversa)\nEC sin importantes limitaciones o evidencia abrumadora de estudios observacionales.\nPuede aplicarse a la mayoría de los pacientes, en la mayoría de las circunstancias sin reservas.\n\n\n1.B\nFuerte\nModerada\nSuperan ampliamente los riesgos y cargas (o viceversa)\nEC con importantes limitaciones (resultados inconsistentes, defectos metodológicos indirectos o imprecisos) o pruebas excepcionalmente fuertes a partir de estudios observacionales.\nPuede aplicarse a la mayoría de los pacientes, en la mayoría de las circunstancias sin reservas.\n\n\n1.C\nFuerte\nBaja o muy baja\nSuperan ampliamente los riesgos y cargas (o viceversa)\nEstudios observacionales o series de casos.\nPuede cambiar cuando se disponga de mayor evidencia de calidad.\n\n\n2.A\nDébil\nAlta\nEstrechamente equilibrados\nEC sin importantes limitaciones o evidencia abrumadora de estudios observacionales.\nLa mejor acción puede variar dependiendo de las circunstancias de los pacientes o de los valores de la sociedad.\n\n\n2.B\nDébil\nModerada\nEstrechamente equilibrados\nEC con importantes limitaciones (resultados inconsistentes, defectos metodológicos indirectos o imprecisos) o pruebas excepcionalmente fuertes a partir de estudios observacionales.\nLa mejor acción puede variar dependiendo de las circunstancias de los pacientes o de los valores de la sociedad.\n\n\n2.C\nDébil\nBaja o muy baja\nIncertidumbre en las estimaciones de beneficios, riesgos y carga\nEstudios observacionales o series de casos.\nOtras alternativas pueden ser igual de razonables."
  },
  {
    "objectID": "unidad_2/01_est_epidemiologicos.html#planificación-y-análisis-de-los-estudios-epidemiológicos",
    "href": "unidad_2/01_est_epidemiologicos.html#planificación-y-análisis-de-los-estudios-epidemiológicos",
    "title": "Estudios epidemiológicos",
    "section": "Planificación y análisis de los estudios epidemiológicos",
    "text": "Planificación y análisis de los estudios epidemiológicos\nUna vez definidos los objetivos y el diseño, es fundamental planificar detalladamente el estudio: seleccionar la población, estimar el tamaño muestral, establecer criterios de inclusión y exclusión, definir las variables, las fuentes de datos y los métodos de recolección, procesamiento y análisis.\nMientras que la definición del problema y la elección del diseño ya han sido abordadas en instancias previas, en este curso nos centraremos en la fase analítica, haciendo especial énfasis en el tratamiento de los datos derivados de distintos tipos de estudios.\nDado que una gran proporción de la investigación en salud es observacional, comenzaremos por los análisis correspondientes a estudios transversales, de casos y controles, y de cohortes. En todos los casos, la comunicación de los resultados debe ser clara y estructurada, para facilitar la comprensión del proceso, desde la planificación hasta las conclusiones.\nLa formulación de recomendaciones en torno a la comunicación de la investigación puede contribuir a mejorar su calidad.\nDeclaración STROBE\nLa Declaración STROBE (Strengthening the Reporting of Observational Studies in Epidemiology) (Elm et al. 2008) es una guía internacional destinada a estandarizar y mejorar la calidad del reporte de estudios observacionales, incluyendo estudios transversales, de casos y controles, y de cohortes.\nSe trata de una iniciativa colaborativa que reúne a epidemiólogos, metodólogos, estadísticos, investigadores y editores de revistas científicas, comprometidos con la realización y difusión de investigaciones observacionales rigurosas.\nSTROBE incluye una lista de 22 ítems, organizados en secciones del manuscrito (título, resumen, introducción, métodos, resultados, discusión y otra información relevante), que especifican lo que debe informarse para asegurar la transparencia y reproducibilidad de los hallazgos. Esta guía también es una herramienta útil para investigadores, ya que indica claramente los elementos esenciales que deben estar presentes en un artículo científico basado en estudios observacionales. El respaldo de STROBE en revistas biomédicas es creciente, siendo requisito editorial en gran parte de ellas.\nPuede descargarse desde el sitio oficial de STROBE:\n➡️Guía completa y checklist\n➡️ Versión en español\nLa declaración STROBE ha sido adaptada para contextos específicos mediante extensiones temáticas:\nSTROME-ID\nSTROME-ID (Strengthening the Reporting of Molecular Epidemiology for Infectious Diseases), tiene como objetivo establecer recomendaciones que respalden un adecuado informe científico de estudios en epidemiología molecular de enfermedades infecciosas. Incentiva un reporte adecuado que considere amenazas específicas a la validez, como errores de clasificación, contaminación cruzada, o sesgos derivados de los métodos de laboratorio y análisis genético.\n➡️Descargar STROME-ID\nSTROBE-VET\nSTROBE-VET (STROBE - Veterinary Extension), orienta el reporte de estudios observacionales en medicina veterinaria, salud pública, zoonosis y seguridad alimentaria. Facilita la evaluación crítica y reproducibilidad en investigaciones con poblaciones animales.\n➡️ Descargar STROBE-VET\nRECORD\nRECORD (Reporting of studies Conducted using Observational Routinely-collected health Data) fue creada para estudios que utilizan datos rutinarios, como registros electrónicos de salud, bases de datos administrativas o registros poblacionales.\n➡️ Descargar RECORD\nDeclaración CONSORT\nEn cuanto a estudios experimentales, la Declaración CONSORT (Consolidated Standards of Reporting Trials) (Schulz et al. 2010) es una guía internacional destinada a mejorar la calidad del reporte de los ensayos clínicos aleatorizados (ECA). Fue desarrollada en 1996 y revisada por primera vez en 2001, con actualizaciones posteriores (la última versión es de 2010).\nMuchas revistas biomédicas la han adoptado como requisito editorial, lo que ha contribuido significativamente a la mejora en la calidad, transparencia y reproducibilidad de los informes sobre ECA.\nCONSORT establece un conjunto mínimo de 25 ítems organizados en secciones del artículo científico (título, resumen, introducción, métodos, resultados, discusión y otros), complementados por un diagrama de flujo que muestra el número de participantes en cada fase del estudio (asignación, intervención, seguimiento y análisis). Esta herramienta ofrece un formato estándar para que los autores preparen informes completos y transparentes de los resultados de los ensayos, facilitando su evaluación e interpretación crítica.\nPuede descargarse desde el sitio oficial de CONSORT:\n➡️ https://www.consort-spirit.org/\n\n\n\n\n\n\nEn este curso nos centraremos en los aspectos metodológicos y en el análisis de datos según el diseño del estudio. Para ello, utilizaremos el lenguaje R, un entorno especializado en análisis estadístico que permitirá explorar diferentes enfoques adaptados a cada tipo de diseño epidemiológico. Partiremos de cada diseño y discutiremos posibles caminos de análisis, reconociendo que dichos caminos no siempre son únicos y que ciertos elementos del análisis pueden ser comunes a varios diseños.\n\n\n\n\nHernández-Ávila (2011)"
  },
  {
    "objectID": "unidad_2/03_inferencia.html",
    "href": "unidad_2/03_inferencia.html",
    "title": "Inferencia estadística",
    "section": "",
    "text": "La estadística inferencial es la rama de la estadística que permite formular conclusiones sobre una población a partir del análisis de una muestra. Se apoya en el cálculo de probabilidades, que proporciona el marco teórico para modelar fenómenos aleatorios y generalizar los resultados muestrales a toda la población. Dado que las inferencias basadas en muestras están sujetas a incertidumbre, es fundamental expresarlas siempre en términos probabilísticos.\nLas dos actividades principales en este proceso son:\n\n\nEstimación de parámetros: consiste en calcular, a partir de los datos muestrales, valores que aproximen parámetros desconocidos de la población.\n\nEjemplo: Estimar la prevalencia de una enfermedad X en la población.\n\n\n\nPruebas de hipótesis: implica evaluar con base estadística afirmaciones acerca de uno o más parámetros.Ejemplos:\n\n¿La prevalencia de la enfermedad X en Argentina es menor que en Uruguay?\n¿La prevalencia de la enfermedad X en Argentina en 2010 fue menor que en el año 2000?\n\n\n\nAl trabajar con muestras, entra en juego el concepto de teoría del muestreo, que, si bien no abordaremos en profundidad aquí, es clave para comprender cómo se relacionan los valores observados en la muestra con los de la población.\nEsta teoría estudia la relación entre la distribución de una variable en la población y el comportamiento de dicha variable en muestras aleatorias extraídas de ella. A las medidas obtenidas a partir de la muestra se las denomina estadísticos muestrales o simplemente estadísticos, mientras que sus contrapartes en la población se denominan parámetros.\nPor ejemplo, supongamos que queremos conocer el valor medio de colesterol total de la población de Mar del Plata y tomamos una muestra de tamaño \\(n\\).\n\nLa media poblacional del colesterol total se representa con la letra griega \\(\\mu\\) y corresponde al parámetro.\nLa media muestral, que se obtiene a partir de los datos de la muestra, se representa como \\(\\bar{x}\\) y es un estimador o estadístico muestral.\n\n\n\n\n\nLa distribución muestral de un estadístico es la distribución de todos los valores posibles que ese estadístico puede tomar al calcularse en muestras aleatorias del mismo tamaño extraídas de una misma población. Este concepto es central en la inferencia estadística, ya que permite cuantificar la incertidumbre asociada a las estimaciones.\nPara comprender mejor qué es una distribución muestral y cómo se construye, pueden ver la siguiente videoclase:\n➡️ Distribuciones Muestrales\nComo construir una distribución muestral puede resultar muy laborioso cuando la población es grande, y directamente imposible si es infinita, se suelen utilizar aproximaciones basadas en la toma de un gran número de muestras aleatorias del mismo tamaño."
  },
  {
    "objectID": "unidad_2/03_inferencia.html#introducción",
    "href": "unidad_2/03_inferencia.html#introducción",
    "title": "Inferencia estadística",
    "section": "",
    "text": "La estadística inferencial es la rama de la estadística que permite formular conclusiones sobre una población a partir del análisis de una muestra. Se apoya en el cálculo de probabilidades, que proporciona el marco teórico para modelar fenómenos aleatorios y generalizar los resultados muestrales a toda la población. Dado que las inferencias basadas en muestras están sujetas a incertidumbre, es fundamental expresarlas siempre en términos probabilísticos.\nLas dos actividades principales en este proceso son:\n\n\nEstimación de parámetros: consiste en calcular, a partir de los datos muestrales, valores que aproximen parámetros desconocidos de la población.\n\nEjemplo: Estimar la prevalencia de una enfermedad X en la población.\n\n\n\nPruebas de hipótesis: implica evaluar con base estadística afirmaciones acerca de uno o más parámetros.Ejemplos:\n\n¿La prevalencia de la enfermedad X en Argentina es menor que en Uruguay?\n¿La prevalencia de la enfermedad X en Argentina en 2010 fue menor que en el año 2000?\n\n\n\nAl trabajar con muestras, entra en juego el concepto de teoría del muestreo, que, si bien no abordaremos en profundidad aquí, es clave para comprender cómo se relacionan los valores observados en la muestra con los de la población.\nEsta teoría estudia la relación entre la distribución de una variable en la población y el comportamiento de dicha variable en muestras aleatorias extraídas de ella. A las medidas obtenidas a partir de la muestra se las denomina estadísticos muestrales o simplemente estadísticos, mientras que sus contrapartes en la población se denominan parámetros.\nPor ejemplo, supongamos que queremos conocer el valor medio de colesterol total de la población de Mar del Plata y tomamos una muestra de tamaño \\(n\\).\n\nLa media poblacional del colesterol total se representa con la letra griega \\(\\mu\\) y corresponde al parámetro.\nLa media muestral, que se obtiene a partir de los datos de la muestra, se representa como \\(\\bar{x}\\) y es un estimador o estadístico muestral.\n\n\n\n\n\nLa distribución muestral de un estadístico es la distribución de todos los valores posibles que ese estadístico puede tomar al calcularse en muestras aleatorias del mismo tamaño extraídas de una misma población. Este concepto es central en la inferencia estadística, ya que permite cuantificar la incertidumbre asociada a las estimaciones.\nPara comprender mejor qué es una distribución muestral y cómo se construye, pueden ver la siguiente videoclase:\n➡️ Distribuciones Muestrales\nComo construir una distribución muestral puede resultar muy laborioso cuando la población es grande, y directamente imposible si es infinita, se suelen utilizar aproximaciones basadas en la toma de un gran número de muestras aleatorias del mismo tamaño."
  },
  {
    "objectID": "unidad_2/03_inferencia.html#intervalos-de-confianza-ic",
    "href": "unidad_2/03_inferencia.html#intervalos-de-confianza-ic",
    "title": "Inferencia estadística",
    "section": "Intervalos de Confianza (IC)",
    "text": "Intervalos de Confianza (IC)\nUna forma eficaz de abordar la inferencia estadística es a través de los intervalos de confianza (IC), ya que, aunque son procedimientos inferenciales, están estrechamente vinculados con la estadística descriptiva.\nSupongamos que queremos estimar la media de colesterol de la población de Mar del Plata. Sería inviable medir el colesterol de cada habitante, por lo que optamos por tomar una muestra de, por ejemplo, 100, 200 o 300 individuos (más adelante veremos cómo determinar el tamaño adecuado de la muestra). Debemos recordar que diferentes muestras producirán en general medias diferentes. Existe, por tanto, un grado de incertidumbre asociado. Si hiciéramos una estimación puntual, obtendríamos un solo valor, pero sin información sobre su variabilidad. No sabríamos qué tan cerca o lejos está nuestra estimación (\\(\\bar{x}\\)) de la verdadera media poblacional (\\(\\mu\\)).\nEl intervalo de confianza proporciona un rango de valores dentro del cual se espera que se encuentre el valor verdadero del parámetro poblacional, con un cierto nivel de confianza. A diferencia de la estimación puntual que proporciona un único valor numérico, el intervalo consta de dos valores entre los cuales se supone está contenido el parámetro estimado. Entonces, el intervalo de confianza puede expresarse como:\n\\[ IC = estimador~puntual \\pm (coeficiente~de~confiabilidad) * (error~ estandar) \\]\ndonde:\n\n\nEstimador puntual:\n\nPara la media poblacional (\\(\\mu\\)), se toma la media muestral(\\(\\bar{x}\\)).\nPara una proporción de la población (\\(p\\)), se toma la proporción muestral (\\(\\hat{p}\\)).\n\n\nCoeficiente de confiabilidad: Se relaciona con el nivel de confianza deseado (por ejemplo, 90%, 95% o 99%), y se expresa como \\(1 - \\alpha\\), es decir la probabilidad de que el parámetro se encuentre dentro del IC. Recordemos que el nivel de significancia (\\(\\alpha\\)) es la probabilidad de que el parámetro no se halle dentro del IC y es un valor generalmente pequeño (por ejemplo, 0.1, 0.05 o 0.01) expresado como probabilidad o porcentaje (por ejemplo, 10%, 5% o 1%).\n\nError estándar (SE): Representa la variabilidad de la distribución muestral. Por ejemplo, para la media el error estándar se calcula como la raíz cuadrada de la varianza de la distribución muestral:\n\\[\nSE = \\frac{\\sigma}{\\sqrt{n}}\n\\]\nDonde \\(\\sigma\\) es la desviación estándar poblacional y \\(n\\) el tamaño de la muestra. Si se estima un IC para una proporción, el error estándar es:\n\\[\nSE = \\sqrt{\\frac{\\hat{p}(1 - \\hat{p})}{n}}\n\\]\n\n\nEl proceso se fundamenta en el Teorema del Límite Central (TCL), que establece que, para muestras suficientemente grandes, la distribución de \\(\\bar{x}\\) es aproximadamente normal, con media \\(\\mu\\) y varianza \\(\\sigma^2/n\\). Así, la variable tipificada:\n\\[\nZ = \\frac{\\bar{x}-\\mu}{\\sigma}\n\\]\nsigue una distribución normal estándar (media 0 y desviación estándar 1), lo que permite calcular probabilidades y construir el IC.\nEn cualquier distribución normal:\n\nEntre \\(\\mu \\pm \\sigma\\) se encuentra el 68% de los datos.\nEntre \\(\\mu \\pm 2\\sigma\\) se encuentra el 95%.\nEntre \\(\\mu \\pm 3\\sigma\\) se encuentra el 99%.\n\nEl siguiente gráfico ilustra lo explicado anteriormente:\n\n\n\n\n\n\n\n\nSabemos que, independientemente de la localización de los valores, aproximadamente el 95% de los valores posibles de \\(\\bar{x}\\) en la distribución muestral estarán a menos de dos desviaciones estándar de la media \\(\\mu\\). Es decir, el intervalo \\(\\mu \\pm 2\\sigma\\) contendrá el 95% de los valores posibles de \\(\\bar{x}\\).\nSupongamos que formamos intervalos a partir de todos los posibles valores de \\(\\bar{x}\\), calculados a partir de todas las muestras posibles de tamaño \\(n\\) tomadas de la población de interés. Esto generará una gran cantidad de intervalos de la forma \\(\\mu \\pm  2\\sigma\\), todos con la misma amplitud, centrados en torno a una \\(\\mu\\) desconocida.\nAproximadamente el 95% de estos intervalos tendrán sus centros dentro del intervalo \\(\\mu \\pm  2\\sigma\\). Cada uno de estos intervalos, que se encuentran dentro de \\(\\mu \\pm  2\\sigma\\), puede contener el valor verdadero de \\(\\mu\\).\n\n\n\n\n\n\n\n\nFinalmente, y basándonos en las propiedades de la distribución Normal, se puede deducir la expresión del IC:\n\\[\nP(Z_{\\alpha/2} &lt; Z_{1-\\alpha/2)} = 1 - \\alpha\n\\] Reemplazando \\(Z\\):\n\\[\nP(Z_{\\alpha/2} &lt; \\frac{\\bar{x} - \\mu}{\\frac{\\sigma}{\\sqrt{n}}} &lt; Z_{(1-\\alpha/2)}) = 1-\\alpha   \n\\]\nReordenando, la expresión del IC para la media queda:\n\\[\n\\bar{x} - Z_{\\alpha/2}\\sqrt{\\frac{\\sigma^2}{n}} &lt; \\mu &lt; \\bar{x} + Z_{\\alpha/2}\\frac{\\sigma^2}{n}   \n\\]\n¿Cómo se interpreta un IC?\nSi hubiésemos tomado múltiples muestras del mismo tamaño de la población, en al menos \\(100 * (1 − \\alpha)\\%\\) de las ocasiones el intervalo calculado contendría el parámetro poblacional real. Es decir, un IC al 95% implica que, a largo plazo, el 95% de los intervalos obtenidos a partir de muestras repetidas incluirán el valor verdadero del parámetro.\nEl producto del coeficiente de confiabilidad y el error estándar se denomina precisión de la estimación y es el componente responsable de la amplitud del IC. Recordemos que la fórmula general para construir un intervalo de confianza era:\n\\[ IC = estimador~puntual \\pm (coeficiente~de~confiabilidad) * (error~ estandar) \\]\nPara el caso de la media:\n\nAumento de la confiabilidad: Si se incrementa el nivel de confianza, el coeficiente (por ejemplo, pasando de 1.96 a un valor mayor) aumenta, lo que a su vez incrementa la amplitud del IC.\n\nReducción del error estándar: Si se fija la confiabilidad (por ejemplo, al 95%), para disminuir la amplitud del IC es necesario reducir el error estándar. Dado que el error estándar de la media es:\n\\[ SE = \\frac{\\sigma}{\\sqrt{n}} \\]\ny considerando que \\(\\sigma\\) es constante, la única forma de disminuir el error estándar es aumentando el tamaño muestral (\\(n\\)).\n\n\nSurge entonces la pregunta: ¿qué tan grande debe ser \\(n\\)?\nLa respuesta dependerá de \\(\\sigma\\), del nivel de significación (\\(\\alpha\\)) y de la amplitud deseada para el IC. La relación es:\n\\[ Amplitud = Z \\frac{\\sigma}{\\sqrt{n}} \\Longrightarrow n = \\frac{Z^2\\sigma^2}{Amplitud^2} \\quad (Z = 1.96~si~\\alpha = 0.05) \\]\n(En la práctica, \\(\\sigma\\) generalmente no se conoce, así que se usa su estimación muestral).\nLa expresión del error estándar varía según el parámetro a estimar. Hemos visto el caso de la media; si lo que se desea es calcular un IC para una proporción, recordemos que, para muestras grandes, la distribución de las proporciones de la muestra es aproximadamente normal de acuerdo con el TCL. En este caso:\n\nLa media de la distribución es la proporción real \\(p\\)\nLa varianza es \\(p(1-p)/n\\), lo que nos lleva a que el error estándar es:\n\n\\[\nSE = \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}      \n\\]\ny el IC para la proporción se expresa como:\n\\[ \\hat{p} - Z_{1-\\alpha/2}\\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}} &lt; p &lt; \\hat{p} + Z_{1-\\alpha/2}\\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}  \\]\nDado que un intervalo de confianza implica una declaración probabilística, su cálculo se fundamenta en las distribuciones muestrales de los estimadores y en el correspondiente error estándar. Aunque las fórmulas pueden parecer complejas, los paquetes estadísticos (como R) realizan estos cálculos automáticamente. Lo fundamental es comprender en qué depende la amplitud del IC (nivel de confianza, error estándar y tamaño muestral) y cómo cada uno de estos componentes influye en la precisión de la estimación.\nPara profundizar y visualizar simulaciones sobre estos conceptos, pueden explorar recursos interactivos como:\n➡️ Viendo la teoría: Una introducción visual a probabilidad y estadística\nAdemás, existe un ejemplo práctico de cálculo de IC para la media utilizando la distribución \\(t\\) de Student en el siguiente enlace:\n➡️ Intervalos de confianza con R"
  },
  {
    "objectID": "unidad_2/03_inferencia.html#test-de-hipótesis",
    "href": "unidad_2/03_inferencia.html#test-de-hipótesis",
    "title": "Inferencia estadística",
    "section": "Test de hipótesis",
    "text": "Test de hipótesis\nAunque los estudios de corte transversal no se diseñan originalmente con grupos de comparación, en aquellos de carácter más analítico es frecuente establecer comparaciones. Por ejemplo, pueden surgir preguntas como:\n\n¿La prevalencia de la enfermedad es mayor en mujeres, en determinados grupos etarios o en una provincia específica?\n\nCon esto en mente, revisaremos las herramientas que permiten comparar grupos mediante test o contrastes de hipótesis. El propósito de estos test es ofrecer al investigador una herramienta para tomar decisiones sobre la población a partir de la información obtenida en una muestra.\nAntes de adentrarnos en la parte estadística, es importante distinguir entre dos tipos de hipótesis:\n\nHipótesis de investigación: Es la conjetura o suposición que motiva la investigación.\nHipótesis estadística: Es aquella que puede ser evaluada mediante técnicas estadísticas apropiadas.\n\nEn este texto nos centraremos en aclarar aspectos relativos a las hipótesis estadísticas, asumiendo que las hipótesis de investigación ya han sido discutidas previamente por los investigadores. Describiremos brevemente el razonamiento subyacente a estos test.\nLos contrastes de hipótesis parten de una hipótesis nula, la cual afirma que los dos grupos comparados son iguales o, en otras palabras, que las diferencias observadas se deben únicamente al azar. Como ya se ha mencionado, la variabilidad intrínseca de cualquier muestra impide que la diferencia entre grupos sea exactamente cero.\nEl método estadístico nos permite cuantificar la diferencia entre grupos asumiendo que, si repitiésemos el experimento infinitas veces obtenemos todas las posibles muestras del tamaño indicado a partir de nuestras poblaciones (distribución muestral), las diferencias entre grupos “iguales” se distribuirían conforme a una curva teórica. Basándonos en las propiedades de esta distribución, podemos determinar un valor límite que comprende, por ejemplo, el 95% o el 99% de las diferencias esperadas. Si la diferencia observada entre las muestras supera este valor límite, se considera excesiva para ser atribuible al azar y, por tanto, se rechaza la hipótesis nula. Por el contrario, si la diferencia cae dentro del área del 95%, se concluye que la diferencia encontrada podría atribuirse al azar, y no habría evidencia que nos permita rechazar la hipótesis nula. En estos casos decimos que los grupos “no son diferentes” pero no “son iguales”, ya que la variabilidad inherente impide probar una igualdad exacta.\nLos contrastes de hipótesis se realizan generalmente bajo las siguientes condiciones:\n\nSe asume a priori que la ley de distribución de la población es conocida.\nSe extrae una muestra aleatoria de dicha población.\n\nEl conjunto de estas técnicas de inferencia se denomina técnicas paramétricas. Sin embargo, existen otros métodos, denominados técnicas no paramétricas o contrastes de distribuciones libres, que no requieren estimar parámetros ni suponer una ley de probabilidad específica para la población. Algunos de estos métodos serán desarrollados más adelante en este módulo.\nEs importante señalar que los contrastes de hipótesis se utilizan no solo en estudios transversales, sino con mayor frecuencia en diseños en los que a priori existen grupos de comparación, como en estudios de casos y controles, cohortes, ensayos clínicos, entre otros. Desarrollaremos aquí la teoría que los sustenta y los utilizaremos a lo largo de todo el curso.\nEstructura del test de hipótesis\nLos componentes básicos de cualquier test de contraste de hipótesis son:\n\nHipótesis nula (\\(H_0\\)): Afirma que no existe diferencia entre los grupos que se comparan, es decir, que las variaciones observadas se deben únicamente al azar.\nHipótesis alternativa (\\(H_1\\)): Es la conjetura o suposición que plantea el investigador, estableciendo que sí existe una diferencia entre los grupos. Generalmente es complementaria de la \\(H_0\\).\nEstadístico de prueba: Es el valor calculado a partir de los datos muestrales que se utiliza para tomar la decisión sobre la \\(H_0\\). Cada tipo de problema tiene un estadístico adecuado, cuya magnitud, al compararse con su distribución muestral (por ejemplo, la distribución normal estándar en el caso del estadístico \\(Z\\)), permite determinar si las diferencias observadas son atribuibles al azar.\n\nValor crítico o Región crítica: La región crítica se establece en función del nivel de significación (\\(\\alpha\\)) y consiste en el conjunto de valores extremos del estadístico de prueba que, de ser alcanzados, llevarían a rechazar la \\(H_0\\). Todos los posibles valores del estadístico se ubican en el eje horizontal de la gráfica de su distribución, y la región crítica delimita aquellos valores que son muy poco probables bajo la hipótesis nula.\n\n\n\n\n\n\n\n\nLa regla de decisión es la siguiente:\n\nSi el valor del estadístico de prueba calculado a partir de la muestra cae en la región crítica, se rechaza la \\(H_0\\)​ y se concluye que las diferencias observadas son estadísticamente significativas.\nSi el valor no cae en la región crítica, no se rechaza la \\(H_0\\)​; esto indica que las diferencias entre lo observado y lo esperado pueden explicarse por el azar. Es decir, no son estadísticamente significativas.\n\n\nNivel de significación (\\(\\alpha\\)): Es la probabilidad de cometer un error tipo I, es decir, rechazar \\(H_0\\) cuando es verdadera. Este valor, elegido por el investigador (comúnmente 5% o 1%), determina el límite entre la región de no rechazo y la región crítica.\n\nValor p: Es una medida de qué tan probable son los resultados de la muestra, considerando que \\(H_0\\) sea verdadera.\n\nUn valor \\(p\\) muy pequeño indica que es muy poco probable obtener los resultados observados para el estadístico muestral si \\(H_0\\) fuese cierta, por lo que debemos rechazarla.\nEsto significa que si el valor \\(p \\leq \\alpha\\), es posible rechazar \\(H_0\\); mientras que si \\(p &gt; \\alpha\\), no es posible rechazar la hipótesis nula.\n\nPor ejemplo, si en un test de contraste de dos proporciones se obtiene un estadístico \\(Z= 3,034\\) y un valor \\(p = 0.0024\\), esto significa que la probabilidad de obtener un valor de \\(Z\\) de 3.034 o mayor, suponiendo que \\(H_0\\) es cierta, es del 0.24%. Dado que este valor es mucho menor que un nivel de significación del 5% (o incluso del 1%), se rechaza \\(H_0\\) y se concluye que la diferencia observada es estadísticamente significativa. En otras palabras, si bajo un supuesto dado, la probabilidad de un suceso observado particular es excepcionalmente pequeña, concluimos que el supuesto probablemente es incorrecto (regla del suceso infrecuente).\n\nTipos de contrastes\nLos contrastes de hipótesis se clasifican según la forma de la hipótesis alternativa (\\(H_1\\)). Esta clasificación determina si la prueba es unilateral de cola izquierda o derecha) o bilateral (de dos colas). Siendo las colas de la distribución las regiones extremas limitadas por los valores críticos.\nTest de cola izquierda\nLa hipótesis alternativa plantea que la media del primer grupo es significativamente menor que la del segundo:\n\\[\nH_1: \\mu_1 &lt; \\mu_2\n\\]\nLa región crítica se encuentra en el extremo izquierdo de la distribución. Todo el área crítica tiene un tamaño \\(\\alpha\\) con un valor crítico de \\(-1,645\\).\n\n\n\n\n\n\n\n\nTest de cola derecha\nLa hipótesis alternativa establece que la media del primer grupo es significativamente mayor que la del segundo:\n\\[ H_1: \\mu_1 &gt; \\mu_2 \\]\nLa región crítica se concentra en el extremo derecho de la distribución y toda el área crítica tiene un tamaño \\(\\alpha\\) con un valor crítico de \\(1,645\\).\n\n\n\n\n\n\n\n\nPruebas bilaterales\nLa hipótesis alternativa afirma que existen diferencias entre los grupos, sin especificar la dirección:\n\\[\nH_1: \\mu_1 \\neq \\mu_2\n\\]\nLa región crítica se divide entre ambos extremos de la distribución, con valores críticos de \\(\\pm 1,96\\). El nivel de significación total (\\(\\alpha\\)) se reparte en partes iguales entre las dos colas (\\(\\alpha/2\\) en cada una), lo que implica un 2,5% de probabilidad en cada cola si \\(H_0\\) es verdadera.\n\n\n\n\n\n\n\n\nErrores\nAl utilizar el razonamiento de los contrastes de hipótesis, existen dos tipos principales de errores que podemos cometer:\n\nError tipo I (\\(\\alpha\\)): Ocurre cuando el investigador rechaza la hipótesis nula (\\(H_0\\)) siendo esta verdadera en la población y se concluye erróneamente que existe una diferencia cuando en realidad no la hay. Se suele eligir un valor pequeño de \\(\\alpha\\) (0.01, 0.05 y 0.10) para hacer que la probabilidad de rechazo de \\(H_0\\) sea pequeña.\nError tipo II (\\(\\beta\\)): Ocurre cuando el investigador no rechaza la \\(H_0\\) siendo esta falsa en la población, es decir, se falla en detectar una diferencia real. Generalmente \\(\\beta\\) es mayor que \\(\\alpha\\), pero su valor real se desconoce en la práctica.\n\nEs importante notar que, una vez que se realiza el procedimiento de prueba, no es posible saber con certeza si se ha cometido alguno de estos errores, ya que se desconoce el verdadero estado de la realidad. Sin embargo, al fijar un \\(\\alpha\\) pequeño, se busca asegurar que, en caso de rechazar la \\(H_0\\), la probabilidad de haber cometido un error Tipo I sea baja.\nEn resumen, al interpretar los resultados de un test de hipótesis:\n\nSi se rechaza la \\(H_0\\): se asume que la probabilidad de haber cometido un error Tipo I es baja (debido al valor pequeño de \\(\\alpha\\)).\nSi no se rechaza la \\(H_0\\): se desconoce el riesgo real de un error Tipo II, pero es importante tener en cuenta que, en muchas situaciones, este riesgo es mayor que el de un error Tipo I.\n\nLa tabla que se presenta a continuación resume las posibles situaciones a las que nos enfrentamos con los test de hipótesis:\n\n\n\n\n\nNo rechazar H0\nRechazar H0\n\n\n\nH0 es cierta\nCorrecto (1-α)\nError tipo I (α)\n\n\nH0 es falsa\nError tipo II (β)\nCorrecto (1-β)\n\n\n\n\n\nPara quienes están familiarizados con el ámbito del diagnóstico, existe una clara analogía entre los falsos positivos y falsos negativos en las pruebas diagnósticas y, respectivamente, el error Tipo I y el error Tipo II en los contrastes de hipótesis.\nHemos discutido el significado de \\(\\alpha\\) (error Tipo I); ahora veamos qué implica \\(\\beta\\). Recordemos que el error Tipo II es análogo a los falsos negativos de las pruebas diagnósticas: es la probabilidad de no detectar una diferencia cuando, en realidad, ésta existe. En otras palabras, \\(\\beta\\) es la probabilidad de no rechazar la hipótesis nula (\\(H_0\\)​) siendo esta falsa.\nA diferencia de \\(\\alpha\\) que se fija en un único valor y es determinado por el investigador, \\(\\beta\\) varía según el valor real del parámetro en estudio. Por ejemplo, si consideramos la hipótesis nula \\(H_0: \\mu_1 - \\mu_2 = 0\\), habrá un valor de \\(\\beta\\) para cada posible diferencia entre \\(\\mu_1\\)​ y \\(\\mu_2\\) cuando el valor real no sea cero. La probabilidad de detectar correctamente una diferencia real - es decir, de obtener un resultado estadísticamente significativo cuando la diferencia existe- se denomina potencia estadística y se expresa como \\(1-\\beta\\).\nLos contrastes de hipótesis no son exclusivos de los estudios transversales; por el contrario, su uso es más común en estudios analíticos que involucran grupos de comparación, tales como estudios de casos y controles, cohortes o ensayos clínicos. Por ejemplo, en un ensayo clínico que evalúa dos tratamientos, la potencia estadística refleja la capacidad del estudio para identificar un efecto real del tratamiento.\nEs deseable que la potencia del estudio sea lo mayor posible, ya que esto incrementa la probabilidad de detectar diferencias verdaderas. Sin embargo, no es posible minimizar ambos errores simultáneamente ya que al disminuir \\(\\alpha\\) (es decir, al ser más exigentes para rechazar la \\(H_0\\)​), \\(\\beta\\) tiende a aumentar, y viceversa. En los contrastes, la hipótesis privilegiada es \\(H_0\\) que solo será rechazada cuando la evidencia de su falsedad supere el umbral del \\(1-\\alpha\\). Esto significa que, a menos que la evidencia en contra de \\(H_0\\) sea muy significativa, se opta por no rechazarla. Lo ideal a la hora de definir un test es encontrar un compromiso entre \\(\\beta\\) y \\(\\alpha\\).\n\n\n\n\n\n\n\n\nMientras que para tests bilaterales:\n\n\n\n\n\n\n\n\nFinalmente, podemos decir que la potencia estadística ofrece un segundo mecanismo de seguridad en un contraste de hipótesis. Es como contar con una protección adicional en la toma de decisiones: si solo dispusiéramos del nivel de significación (\\(\\alpha\\)), tendríamos menos garantías. Al incorporar la potencia (\\(1-\\beta\\)) agregamos un segundo control. Por ello, en un contraste no basta con tener un valor p pequeño; también se necesita una potencia alta, que en la práctica suele fijarse en un 80% (es decir, \\(1 - \\beta = 0.8\\)).\nPor otro lado, cuando existe una diferencia real —o un efecto real de una terapia, o una verdadera diferencia entre dos fármacos—, la magnitud de ese efecto influye en la facilidad para detectarlo. Los efectos grandes son más fáciles de identificar que los pequeños. Para estimar la potencia de una prueba, debemos especificar el efecto mínimo que valga la pena identificar.\nLa potencia de un test estadístico depende de tres factores que actúan de manera interrelacionada:\n\nEl riesgo de error que se tolerará al rechazar la hipótesis de ausencia de efecto o diferencia (\\(\\alpha\\)).\nLa dimensión de la diferencia que se desea identificar en relación con la variabilidad en las poblaciones.\nEl tamaño de la muestra.\n\nDel mismo modo que en un problema de estimación se necesita una idea de la magnitud a estimar y del error aceptable para definir el tamaño de la muestra, en un contraste de hipótesis se requiere conocer el tamaño del efecto que se quiere detectar. Así, el tamaño muestral se determina en función del nivel de confianza y de la potencia de la prueba, además de otros aspectos relacionados con el diseño y la prueba estadística elegida.\nEn epidemiología, una de las situaciones más frecuentes al diseñar un estudio es el cálculo del tamaño muestral para un nivel de confianza del 95% y una potencia del 80%, que, como se mencionó, es un nivel de potencia alto y que además permite manejar un \\(\\alpha\\) relativamente bajo. El lenguaje R cuenta con diversas funciones para calcular y visualizar la relación entre tamaño muestral, potencia, tamaño del efecto y nivel de confianza, facilitando el diseño de estudios con un adecuado balance entre la probabilidad de detectar diferencias reales y la de controlar errores estadísticos.\nPor ejemplo, la función pwr.t.test() del paquete pwr (Champely 2020), calcula la potencia para pruebas \\(t\\) de Student de medias (para una muestra, dos muestras y muestras pareadas), basado en el tamaño de la muestra, el nivel de confianza y el tamaño de efecto:\n\npwr.t.test(n, d, sig.level = 0.05, power, type, alternative)\n\nDonde:\n\nn: Número de observaciones para cada grupo.\n\nd: tamaño de efecto (\\(d\\) de Cohen) - diferencia (estandarizada) entre grupos.\n\nNota: La \\(d\\) de Cohen representa las desviaciones estándar que separan dos o más grupos. Por ejemplo: \\(d_{Cohen} = 0.5\\) representa que la diferencia entre grupo experimental y muestral es de media desviación estándar. Cohen sugirió (provisoriamente) que 0.2 es un tamaño de efecto pequeño, 0.5 es mediano y 0.8 es grande,\n\n\nsig.level: nivel de significación (probabilidad del error de tipo I).\npower: potencia del test (1 menos la probabilidad del error tipo II).\ntype: tipo de test (\"two.sample\", \"one.sample\", \"paired\").\nalternative: palabra que especifica la hipotesis alternativa, debe ser \"two.sided\" (predeterminado), \"greater\" or \"less\".\n\nLa función se ejecuta incorporando todos los argumentos obligatorios (d,n, power y sig.level) menos el que se quiere calcular. En ese caso se iguala a NULL o se omite.\nSupongamos que queremos conocer el tamaño de la muestra para detectar diferencias en la media de la hemoglobina glicosilada entre dos grupos de pacientes con tratamientos de control de la diabetes distintos. Aceptamos un nivel de efecto convencional de una pequeña desviación (\\(d_{Cohen} = 0.2\\)), una potencia del 80% y una significación habitual de 0,05.\nCargamos el paquete requerido:\n\nlibrary(pwr)\n\nCalculamos el tamaño de muestra:\n\npwr.t.test(d = 0.2, \n           power = 0.8,\n           sig.level = 0.05, \n           type = \"two.sample\", \n           alternative = \"two.sided\")\n\n\n     Two-sample t test power calculation \n\n              n = 393.4057\n              d = 0.2\n      sig.level = 0.05\n          power = 0.8\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n\n\nTambién podemos graficar la salida:\n\npwr.t.test(d = 0.2, \n           power = 0.8,\n           sig.level = 0.05, \n           type = \"two.sample\", \n           alternative = \"two.sided\") |&gt; \n  plot()\n\n\n\n\n\n\n\nObservamos que, si por ejemplo tomásemos una muestra de 300 individuos por grupo, la potencia del estudio con ese tamaño de efecto y nivel de significación del 0,05 sería aproximadamente de 68%.\n¿Qué test se debe aplicar en cada caso?\nHemos discutido la mecánica general de los test de hipótesis. Ahora, nos centraremos en orientarnos sobre qué test aplicar en cada situación. Aunque existe un desarrollo teórico detrás de cada caso, aquí nos quedaremos con ciertas reglas prácticas.\nPara sistematizar los contrastes de hipótesis, es útil responder dos preguntas fundamentales:\n\n¿Qué tipo de variable dependiente tengo?\nLa variable resultado puede ser cuantitativa, cualitativa, de tiempo hasta un evento, etc.\n¿Qué se está comparando?\nEsto se traduce en evaluar el tipo de experimento: ¿se comparan dos grupos o más? ¿Son muestras independientes o relacionadas (por ejemplo, medidas en el mismo grupo antes y después de una intervención)?\n\nVariable resultado cuantitativa\nCuando la variable de interés es cuantitativa, la comparación de grupos generalmente se traduce en comparar las medias de dichos grupos. En este contexto, se deben responder una tercera pregunta y es si dicha variable se distribuye normalmente, porque si no fuera así, debemos recurrir a los contrastes no paramétricos.\n\nEn la siguiente sección detallaremos las pruebas de normalidad y heterogeneidad de varianzas y su aplicación en R.\n\nPor ejemplo, para comparar dos grupos se plantearía:\n\n\n\\(H_0: \\mu_1 = \\mu_2\\) (o su equivalente \\(\\mu_1 - \\mu_2 = 0\\))\n\n\\(H_1: \\mu_1 \\neq \\mu_2\\) (contraste bilateral) o bien \\(\\mu_1 &lt; \\mu_2\\) o \\(\\mu_1 &gt; \\mu_2\\) (contrastes unilaterales).\n\nSe calcula el estadístico de prueba (por ejemplo, \\(t\\) de Student o \\(Z\\), según el tamaño de la muestra y si se conoce la varianza poblacional), se determina la región de rechazo y, finalmente, si \\(p &lt; \\alpha\\) rechazo \\(H_0\\) y si \\(p &gt;  \\alpha\\) no rechazo \\(H_0\\).\nA continuación presentamos un esquema que sirve de guía para potenciales situaciones:\n\n\n\n\n\nG\nn1\nVariable respuesta  cuantitativaA\nComparación de gruposn1-&gt;A\nC\nComparación antes y después  de un tratamienton1-&gt;C\nB\nDistribución normalA-&gt;B\nb1\nt de StudentB-&gt;b1\nSi (2 grupos)b2\nMann-WhitneyB-&gt;b2\nNo (2 grupos)b3\nANOVAB-&gt;b3\nSi (3+ grupos)b4\nKruskal-WallisB-&gt;b4\nNo (3+ grupos)D\nDistribución normalC-&gt;D\nd1\nt de Student  (datos apareados)D-&gt;d1\nSi (mismo grupo)d2\nWilcoxonD-&gt;d2\nNo (mismo grupo)d3\nANOVA  (medidas repetidas)D-&gt;d3\nSi (2+ grupos)d4\nFriedmanD-&gt;d4\nNo (2+ grupos)\n\n\n\n\n\nHay algo más que los estadísticos nos “obligan” a considerar en el caso que compare medias independientes y se haya verificado la normalidad de la distribución, y se trata de considerar si las varianzas de las poblaciones que comparo son iguales o diferentes. El algoritmo de resolución es el siguiente:\n\n\n\n\n\nG\nA\nComparo mediasB\nMuestras independientesA-&gt;B\nC\nMuestras pareadasA-&gt;C\nb1\nVarianzas conocidasB-&gt;b1\nc1\nMismo tamaño de muestraC-&gt;c1\nb2\nPrueba Z (normal)b1-&gt;b2\nSíb3\nPrueba tb1-&gt;b3\nNoc2\nPrueba tc1-&gt;c2\n\n\n\n\n\n\nCuando trabajamos con muestras independientes, a menudo nos encontramos en el escenario de varianzas desconocidas. Esto nos obliga a realizar, de manera previa, un test de homogeneidad de varianzas para determinar si se puede asumir que son iguales o si, por el contrario, difieren. La razón es que, aunque se utiliza el estadístico \\(t\\) de Student para comparar las medias, su cálculo y la forma en que se distribuye varían en función de si las varianzas de los grupos son iguales o no.\nVariable resultado cualitativa\nCuando la variable dependiente es cualitativa (categórica), como por ejemplo Enfermo (Sí/No) o Expuesto (Sí/No), la comparación de grupos no se basa en medias, sino en proporciones. Para ello, se utilizan pruebas estadísticas específicas según el número de grupos y el diseño del estudio.\nA continuación, presentamos una guía análoga a la anterior:\n\n\n\n\n\nG\nn1\nVariable respuesta  cualitativaA\nComparación de gruposn1-&gt;A\nC\nComparación antes y después  de un tratamienton1-&gt;C\nB\nn &gt; 30 y frecuencia  por grupo &gt;= 5A-&gt;B\nb1\ntest de proporciones/  Chi-cuadradoB-&gt;b1\nSi (2+ grupos)b2\ntest exacto de FisherB-&gt;b2\nNo (2+ grupos)D\nMismo grupo  (dos categorías)C-&gt;D\nE\nVarios grupos  (n &gt;= 4 y k &gt;= 24)C-&gt;E\nd1\nMc NemarD-&gt;d1\ne1\nCochrane QE-&gt;e1"
  },
  {
    "objectID": "unidad_2/03_inferencia.html#ejemplo-práctico-en-r",
    "href": "unidad_2/03_inferencia.html#ejemplo-práctico-en-r",
    "title": "Inferencia estadística",
    "section": "Ejemplo práctico en R",
    "text": "Ejemplo práctico en R\nEn el archivo “bebe1.txt” se encuentran datos de una muestra aleatoria de los pesos al nacer de 186 bebés cuyas madres no consumieron cocaína durante el embarazo. En el archivo “bebe2.txt” se encuentran datos de una muestra aleatoria de 190 bebés cuyas madres consumieron cocaína durante el embarazo. Nos interesa conocer el peso promedio al nacer de los bebés de la primer base de datos y, de acuerdo a estos resultados, saber si el consumo de cocaína afecta el peso que tiene un bebé al nacer.\nPodemos resolver la primer parte manualmente:\n\\[\nIC = \\bar{x} \\pm Z_{1-\\alpha/2}\\frac{\\sigma}{\\sqrt{n}}\n\\]\ndonde:\n\nmedia (\\(\\bar{x}\\)): 3103 gramos\ndesvío estándar (\\(\\sigma\\)): 696\nnúmero de observaciones (\\(n\\)): 186\n\n\\[\nIC = 3103 \\pm 2,57\\frac{696}{\\sqrt{186}} = 3103 \\pm 131,16\n\\] \\[\nIC = 2971,85 - 3234,15\n\\]\nEl peso medio al nacer de los bebés oscila entre 2971,85 y 3234,15 gramos (\\(\\alpha = 0,99\\)).\nPara resolver las preguntas en R, comenzaremos cargando los paquetes necesarios:\n\nlibrary(tidyverse)\nlibrary(dlookr)\n\nCargamos los datos:\n\ndatos1 &lt;- read_csv2(\"datos/bebe1.txt\")\n\ndatos2 &lt;- read_csv2(\"datos/bebe2.txt\")\n\nLa función t.test() del paquete base utiliza la distribución \\(t\\) de Student:\n\nt.test(datos1,\n       conf.level = 0.99)\n\n\n    One Sample t-test\n\ndata:  datos1\nt = 60.804, df = 185, p-value &lt; 2.2e-16\nalternative hypothesis: true mean is not equal to 0\n99 percent confidence interval:\n 2970.178 3235.822\nsample estimates:\nmean of x \n     3103 \n\n\nLos IC producidos son similares a los calculados manualmente.\nPara comparar los datos de los bebés de madres que consumieron cocaína, debemos realizar un test de comparación de medias para dos poblaciones independientes. Para ello, comenzamos por chequear normalidad:\n\nshapiro.test(datos1$peso)\n\n\n    Shapiro-Wilk normality test\n\ndata:  datos1$peso\nW = 0.99293, p-value = 0.5092\n\ndatos1 |&gt; \n  plot_normality()\n\n\n\n\n\n\nshapiro.test(datos2$peso)\n\n\n    Shapiro-Wilk normality test\n\ndata:  datos2$peso\nW = 0.98951, p-value = 0.1774\n\ndatos2 |&gt; \n  plot_normality()\n\n\n\n\n\n\n\nSe cumple con el supuesto de normalidad, ahora procedemos a comparar varianzas usando la función var.test():\n\nvar.test(datos1$peso, datos2$peso)\n\n\n    F test to compare two variances\n\ndata:  datos1$peso and datos2$peso\nF = 1.1644, num df = 185, denom df = 189, p-value = 0.2986\nalternative hypothesis: true ratio of variances is not equal to 1\n95 percent confidence interval:\n 0.8736082 1.5526583\nsample estimates:\nratio of variances \n          1.164392 \n\n\nComo las varianzas son iguales, aplicamos un \\(t\\) test para comparar medias de dos poblaciones independientes con varianzas iguales planteando un contraste bilateral:\n\\[\nH_0: \\mu_1 = \\mu_2 \\\\\nH_1: \\mu_1 \\neq \\mu_2\n\\]\n\nt.test(x = datos1$peso, \n       y = datos2$peso, \n       alternative = \"two.sided\", \n       var.equal = TRUE)\n\n\n    Two Sample t-test\n\ndata:  datos1$peso and datos2$peso\nt = 5.8252, df = 374, p-value = 1.231e-08\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n 266.9644 539.0356\nsample estimates:\nmean of x mean of y \n     3103      2700 \n\n\nO también podríamos plantear un contraste unilateral, si consideramos que nuestra hipótesis de trabajo es que el peso de los bebés de madres que no consumen cocaína es mayor que aquellos de madres que sí consumen.\n\\[\nH_0: \\mu_1 = \\mu_2 \\\\\nH_1: \\mu_1 &gt; \\mu_2 \\quad ó \\quad \\mu_1 - \\mu_2 &gt; 0\n\\]\n\nt.test(x = datos1$peso, \n       y = datos2$peso, \n       alternative = \"greater\",\n       var.equal = TRUE)\n\n\n    Two Sample t-test\n\ndata:  datos1$peso and datos2$peso\nt = 5.8252, df = 374, p-value = 6.155e-09\nalternative hypothesis: true difference in means is greater than 0\n95 percent confidence interval:\n 288.9222      Inf\nsample estimates:\nmean of x mean of y \n     3103      2700 \n\n\nConcluimos que los pesos de los bebés hijos de madres que no consumen cocaína es significativamente mayor respecto de los hijos de madres que sí la consumen.\nInferencia sobre una población\nEn algunas situaciones, además de obtener intervalos de confianza para estimar parámetros como la media o la proporción poblacional, también puede ser de interés realizar contrastes de hipótesis sobre un valor específico de referencia. Este tipo de análisis se denomina inferencia sobre una población y permite evaluar si un parámetro poblacional es significativamente diferente de un valor dado.\nUn caso común en epidemiología y medicina es el estudio de la eficacia de un tratamiento. Supongamos que queremos evaluar si un nuevo fármaco reduce la presión arterial en pacientes con un determinado síndrome. Se sabe que, en ausencia de tratamiento, la presión arterial media en estos pacientes es de 165 mmHg. Para investigar el efecto del fármaco, se administra a un grupo de 15 pacientes y se mide su presión arterial después del tratamiento.\nQueremos evaluar si la presión arterial media en pacientes tratados con el nuevo fármaco es significativamente menor que la presión arterial histórica de 165 mmHg:\n\n\\(H_0: \\mu = 165~mmHg\\)\n\\(H_1: \\mu &lt; 165~mmHg\\)\n\nEste es un contraste unilateral a la izquierda, ya que buscamos evidencia de que el tratamiento reduce la presión arterial.\nCargamos los datos:\n\nenfermos &lt;- read_csv2(\"datos/enfermos.txt\")\n\nCalculamos la media y el desvío estándar:\n\nenfermos  |&gt; \n  summarise(media = mean(V1), \n            desvio = sd(V1))\n\n# A tibble: 1 × 2\n  media desvio\n  &lt;dbl&gt;  &lt;dbl&gt;\n1  152.   22.0\n\n\nEvaluamos normalidad:\n\nshapiro.test(enfermos$V1)\n\n\n    Shapiro-Wilk normality test\n\ndata:  enfermos$V1\nW = 0.94967, p-value = 0.5192\n\n\nEste grupo de enfermos tratados con el nuevo fármaco tienen una media de aproximadamente 151.9619928, con un desvío de 21.9788237. Además, los datos cumplen con el supuesto de normalidad.\nRealizamos la prueba de hipótesis para evaluar si este valor es significativamente menor a 165:\n\nt.test(x = enfermos$V1, \n       alternative = \"less\", \n       mu = 165, \n       conf.level = 0.95)\n\n\n    One Sample t-test\n\ndata:  enfermos$V1\nt = -2.2975, df = 14, p-value = 0.01876\nalternative hypothesis: true mean is less than 165\n95 percent confidence interval:\n     -Inf 161.9573\nsample estimates:\nmean of x \n  151.962 \n\n\nDado que \\(p &lt; 0.05\\), rechazamos la hipótesis nula y concluimos que, con un nivel de confianza del 95%, el nuevo fármaco reduce significativamente la presión arterial en estos pacientes.\n\nM. Rodríguez y Mendivelso (2018)\nHernández-Ávila (2011)\nV. Rodríguez et al. (2003)\nRíus Díaz et al. (2012)\nDaniel (2002)\nCáceres, Rafael Álvarez (2007)\nTriola (2018)\nNorman, GR y Streiner, DL (1996)\nGlantz, S (2006)\n«EPIDAT 4.2 - Consellería de Sanidade - Servizo Galego de Saúde» (s. f.)\n (2025)\nManitz et al. (2021)\n(Ryu 2024)\n(Champely 2020)"
  },
  {
    "objectID": "unidad_2/05_muestreo.html",
    "href": "unidad_2/05_muestreo.html",
    "title": "Selección de la muestra",
    "section": "",
    "text": "Toda investigación epidemiológica requiere, en principio, la selección de una muestra representativa de la población objeto de estudio. Esto implica:\n\nDeterminar el tamaño de la muestra.\nDefinir cómo se seleccionarán los sujetos (es decir, las unidades de análisis).\nCalcular la incertidumbre que introduce este proceso de muestreo en nuestras estimaciones.\n\nEl tamaño de la muestra se establece en la fase de diseño del estudio, habitualmente cuando se redacta el protocolo de investigación. Es importante considerar que este tamaño es, en cierto sentido, orientador, ya que su cálculo depende de factores que en ocasiones pueden ser subjetivos o estar limitados por la información previa disponible. Además, las exigencias varían según el objetivo del estudio. Por ejemplo:\n\nEn un estudio transversal, donde se estima una prevalencia, se puede aceptar una precisión menor.\nEn un estudio experimental, donde se compara la eficacia de un fármaco frente a un placebo, el tamaño de la muestra debe ser suficiente para detectar una “diferencia significativa” entre los grupos.\n\nCuando el objetivo de la investigación incluye la estimación de un intervalo de confianza (IC) para alguna variable, se busca que dicho IC tenga alta confiabilidad y sea lo más estrecho posible.\nEn el caso de los estudios de corte transversal, que se orientan a estimar una proporción, se formulan preguntas como:\n— ¿Cuál es la prevalencia, proporción o porcentaje del fenómeno en estudio?\nAl tomar una muestra para estimar lo que sucede en la realidad, debemos aceptar que la medición obtenida reflejará la situación poblacional, pero siempre con cierto grado de variabilidad (o error). La precisión es ese grado de variabilidad aceptable y se expresa a través de los intervalos de confianza, que definen los límites inferior y superior dentro de los cuales se espera encontrar el verdadero valor del parámetro."
  },
  {
    "objectID": "unidad_2/05_muestreo.html#introducción",
    "href": "unidad_2/05_muestreo.html#introducción",
    "title": "Selección de la muestra",
    "section": "",
    "text": "Toda investigación epidemiológica requiere, en principio, la selección de una muestra representativa de la población objeto de estudio. Esto implica:\n\nDeterminar el tamaño de la muestra.\nDefinir cómo se seleccionarán los sujetos (es decir, las unidades de análisis).\nCalcular la incertidumbre que introduce este proceso de muestreo en nuestras estimaciones.\n\nEl tamaño de la muestra se establece en la fase de diseño del estudio, habitualmente cuando se redacta el protocolo de investigación. Es importante considerar que este tamaño es, en cierto sentido, orientador, ya que su cálculo depende de factores que en ocasiones pueden ser subjetivos o estar limitados por la información previa disponible. Además, las exigencias varían según el objetivo del estudio. Por ejemplo:\n\nEn un estudio transversal, donde se estima una prevalencia, se puede aceptar una precisión menor.\nEn un estudio experimental, donde se compara la eficacia de un fármaco frente a un placebo, el tamaño de la muestra debe ser suficiente para detectar una “diferencia significativa” entre los grupos.\n\nCuando el objetivo de la investigación incluye la estimación de un intervalo de confianza (IC) para alguna variable, se busca que dicho IC tenga alta confiabilidad y sea lo más estrecho posible.\nEn el caso de los estudios de corte transversal, que se orientan a estimar una proporción, se formulan preguntas como:\n— ¿Cuál es la prevalencia, proporción o porcentaje del fenómeno en estudio?\nAl tomar una muestra para estimar lo que sucede en la realidad, debemos aceptar que la medición obtenida reflejará la situación poblacional, pero siempre con cierto grado de variabilidad (o error). La precisión es ese grado de variabilidad aceptable y se expresa a través de los intervalos de confianza, que definen los límites inferior y superior dentro de los cuales se espera encontrar el verdadero valor del parámetro."
  },
  {
    "objectID": "unidad_2/05_muestreo.html#cálculo-del-tamaño-muestral",
    "href": "unidad_2/05_muestreo.html#cálculo-del-tamaño-muestral",
    "title": "Selección de la muestra",
    "section": "Cálculo del tamaño muestral",
    "text": "Cálculo del tamaño muestral\nComo se mencionó anteriormente, se requiere conocer algunos datos para el cálculo del tamaño de muestra requerido, entre los que destacan:\n\nLa proporción esperada del evento: Aunque resulte paradójico, ya que es el parámetro que se desea estimar, el investigador suele contar con información previa que permite delimitar sus posibles valores. Si no fuera el caso, se utiliza el peor escenario, es decir, una proporción del 50%, ya que este valor maximiza el producto \\(p(1-p)\\) y, por ende, exige el mayor tamaño muestral. De esta forma, muchas veces se recurre a esto para tener al menos una idea del “límite superior” que estaríamos manejando.\nEl nivel de confianza deseado: Habitualmente se emplea un 95% (lo que equivale a un \\(\\alpha\\) de 0.05). Este valor indica la probabilidad de que el intervalo calculado contenga el parámetro poblacional verdadero. A mayor nivel de confianza (es decir, menor \\(\\alpha\\)), mayor es el coeficiente de confiabilidad y, por ende, mayor el tamaño muestral requerido.\nLa precisión (\\(\\delta\\)) deseada: Se refiere a la amplitud aceptable del intervalo de confianza. Cuanta más precisión se exija (es decir, un intervalo más estrecho), mayor será el tamaño de la muestra necesario.\n\nLa fórmula para el cálculo es la siguiente:\n\\[\nN = \\frac{p.q.(Z\\alpha)^2}{\\delta^2}\n\\] Donde:\n\n\n\\(N\\) es el tamaño muestra requerido.\n\n\\(p\\) es la proporción esperada de sujetos portadores del evento.\n\n\\(q = 1 – p\\) es la proporción complementaria (sujetos que no tienen la variable en estudio).\n\n\\(\\delta\\) es la precisión o magnitud de error aceptable.\n\n\\(Z\\alpha\\) es el valor crítico de la distribución normal asociado al nivel de significancia. Se obtiene de tablas de distribución normal de probabilidades y habitualmente se utiliza un valor \\(\\alpha\\) de 0.05, al que le corresponde un valor \\(Z\\) de 1.96\n\nCuando se conoce el tamaño de la población, es necesario aplicar una corrección por población finita:\n\\[\nN =\\frac{n^1}{1+\\frac{n^1}{Población}}\n\\] donde \\(n^1\\) es el \\(N\\) calculado con la ecuación anterior.\nEs fundamental entender que el tamaño de la muestra dependerá de \\(p\\) de \\(\\alpha\\) y de la precisión deseada, más que de la complejidad de la fórmula en sí. Existen numerosos softwares que calculan tamaños de muestra. Algunos están especializados en esta tarea (por ejemplo, EpiDat, EpiInfo, OpenEpi), mientras que otros lo incluyen como una opción dentro de un paquete más amplio de análisis. En el lenguaje R, funciones como power.prop.test() (para comparación de proporciones) y power.t.test() (para pruebas de medias basadas en la distribución \\(t\\) de Student) están incluidas en el paquete stats. Otros paquetes útiles incluyen samplingbook (Manitz et al. 2021), samplesize , pwr (Champely 2020) y epiR (Stevenson y Sergeant 2025).\nA continuación, se presenta un ejemplo práctico utilizando la función sample.size.prop() del paquete samplingbook. Supongamos que queremos estimar la prevalencia de diabetes en una ciudad de 600,000 habitantes, donde estudios previos sugieren una prevalencia cercana al 10.2%. El objetivo es determinar el tamaño de la muestra en un muestreo aleatorio simple.\nComenzamos cargando el paquete requerido:\n\nlibrary(samplingbook)\n\nCalculamos el tamaño muestral en base a los siguientes argumentos:\n\n\ne: número positivo que especifica la precisión que es la mitad del ancho del intervalo de confianza\n\np: proporción esperada de eventos con dominio entre los valores 0 y 1.\n\nN: número entero positivo para el tamaño de la población. El valor predeterminado es Inf, lo que significa que los cálculos se realizan sin corrección de población finita.\n\nlevel: nivel de confianza para los intervalos de confianza. El valor predeterminado es 95%.\n\n\nsample.size.prop(e = 0.02, P = 0.102, N = 600000, level = 0.95)\n\n\nsample.size.prop object: Sample size for proportion estimate\nWith finite population correction: N=6e+05, precision e=0.02 and expected proportion P=0.102\n\nSample size needed: 879\n\n\nEl resultado, tras aplicar la corrección por población finita, indica que se requieren 879 sujetos en la muestra. Por simplicidad, es habitual considerar infinita a la población objetivo, ya que hacerlo así minimiza los riesgos estadísticos:\n\nsample.size.prop(e = 0.02, P = 0.102, N = Inf, level = 0.95)\n\n\nsample.size.prop object: Sample size for proportion estimate\nWithout finite population correction: N=Inf, precision e=0.02 and expected proportion P=0.102\n\nSample size needed: 880\n\n\nSi se considera una población infinita, el tamaño muestral calculado es de 880 personas, apenas una unidad más, lo que demuestra que el ajuste por población finita en este caso es mínimo. En cambio, si no se conoce o no se confía en el valor de prevalencia y se opta por el peor escenario (\\(p = 0.5\\)):\n\nsample.size.prop(e = 0.02, P = 0.5, N = 600000, level = 0.95)\n\n\nsample.size.prop object: Sample size for proportion estimate\nWith finite population correction: N=6e+05, precision e=0.02 and expected proportion P=0.5\n\nSample size needed: 2392\n\n\nEn este caso, el tamaño muestral aumenta considerablemente hasta 2392 unidades, lo que refleja un enfoque más conservador.\nAdemás del cálculo del tamaño muestral, es esencial planificar la forma en que se seleccionarán los individuos. En estudios de corte transversal se debe realizar un muestreo probabilístico, ya que la técnica de muestreo influirá en los factores de expansión necesarios para generalizar los resultados a la población. Aunque el proceso de muestreo no se detalla en este curso, es vital considerarlo para garantizar la validez externa de la investigación."
  },
  {
    "objectID": "unidad_2/05_muestreo.html#conceptos-generales-sobre-muestreo",
    "href": "unidad_2/05_muestreo.html#conceptos-generales-sobre-muestreo",
    "title": "Selección de la muestra",
    "section": "Conceptos generales sobre muestreo",
    "text": "Conceptos generales sobre muestreo\nUn muestreo se considera probabilístico cuando se cumplen dos condiciones fundamentales:\n\nEs posible conocer de antemano la probabilidad de selección que tiene cada elemento de la población.\nEsta probabilidad es mayor que cero para todos los elementos.\n\nSolo con un muestreo probabilístico se puede medir el grado de precisión de las estimaciones realizadas, ya que se conocen las probabilidades de inclusión. Algunos de los métodos más usados son: Muestreo aleatorio simple, muestreo sistemático en fases, Muestreo aleatorio estratificado, muestreo por conglomerados, etc.\nMuestreo aleatorio simple (MAS)\nEn el MAS, cada elemento de la población tiene la misma probabilidad de ser seleccionado (es decir, es un método equiprobabilístico). La probabilidad de inclusión se expresa como:\n\\[\nP = \\frac{n}{N}\n\\]\ndonde:\n\n\\(n\\) es el tamaño de la muestra,\n\\(N\\) es el tamaño de la población.\n\nEste método requiere contar con un listado completo de la población y se utiliza principalmente en poblaciones cerradas (por ejemplo, escuelas, cárceles, hospitales o muestras de laboratorio), donde los elementos pueden identificarse y numerarse fácilmente.\nMuestreo sistemático (MS)\nEl muestreo sistemático es un procedimiento alternativo al MAS, adecuado para la selección de muestras equiprobabilísticas de poblaciones organizadas según algún orden conocido. Sus ventajas incluyen no requerir necesariamente un listado completo de todas las unidades, lo que resulta útil, por ejemplo, para seleccionar a los usuarios que llegan a un servicio hospitalario durante un período determinado.\nEl procedimiento consiste en:\n\nAsignar a cada elemento de la población un número del 1 a \\(N\\).\n\nCalcular el intervalo de selección:\n\\[\nk = \\frac{N}{n}\n\\]\n\nSeleccionar aleatoriamente un número \\(r\\) entre 1 y \\(k\\).\nElegir los elementos que ocupan las posiciones: \\(r, r+k, r + 2k,...\\), es decir, la secuencia \\(r, r+jk\\) con \\(j = 0, 1, 2, \\dots\\).\n\nLa probabilidad de ser seleccionado es igual que en el MAS\n\\[\nP = \\frac{n}{N}\n\\]\nMuestreo aleatorio estratificado (MAE)\nEste método se emplea cuando la población puede dividirse en subgrupos o estratos que difieren en las características de interés (por ejemplo, edad, sexo o nivel socioeconómico). La idea es asegurar que cada estrato esté representado en la muestra. El proceso habitual consiste en:\n\nElaborar listas separadas para cada estrato.\nSeleccionar una submuestra de cada uno de ellos.\n\nLa probabilidad de selección en cada estrato es:\n\\[\nP \\ estrato = \\frac{n \\ estrato}{N \\ estrato}\n\\]\nSi se aplica un muestreo proporcional, el tamaño de la muestra en cada estrato se calcula como:\n\\[\nn \\ estrato = n\\frac{N \\ estrato}{N}\n\\]\nEl objetivo es obtener una muestra cuya variabilidad interna se asemeje a la de la población, logrando que los estratos sean internamente homogéneos y heterogéneos entre sí.\nMuestreo por conglomerados monoetápico\nEl muestreo por conglomerados es ideal cuando la población es difícil de delimitar o está muy dispersa. Se divide la población en conglomerados (o grupos) que actúan como unidades de primera etapa (UPE) y no deben solaparse.\nLa población se divide en grupos (por ejemplo, áreas geográficas, instituciones o servicios). Se selecciona una muestra de \\(n\\) conglomerados y se incluyen todos las unidades de análisis de los conglomerados seleccionados. El tamaño del conglomerado se refiere al número de unidades elementales que contiene, pudiendo ser de igual o distinto tamaño. Los conglomerados deberían ser lo más heterogéneos dentro de ellos y lo más homogéneos entre ellos.\nMuestreo por conglomerados bietápico\nSe utiliza cuando existe gran variabilidad en el tamaño de los conglomerados o la población es muy grande. En este caso, después de dividir la población en conglomerados y seleccionar algunos de ellos como UPE, se realiza una subselección (muestreo secundario) de los elementos dentro de los conglomerados elegidos.\nExisten más posibilidades, que no abordaremos aquí, pero sí nos detendremos en discutir qué implicaciones tiene el muestreo en el proceso de estimación.\nImplicaciones en la Estimación y Ponderación\nEn métodos equiprobabilísticos (como el MAS o el MS), cada elemento tiene la misma probabilidad de selección. Sin embargo, en diseños muestrales más complejos (como el muestreo estratificado o por conglomerados), las probabilidades de selección pueden variar entre individuos.\nEn estos casos, la probabilidad total de selección de un individuo es el producto de las probabilidades de selección en cada etapa del muestreo. Una vez conocida esta probabilidad, se calcula el peso (o factor de expansión) como su inverso. Este peso se interpreta como el número de individuos en la población que representa cada individuo de la muestra.\n\nEl factor de expansión se interpreta como la cantidad de personas en la población, que representa una persona en la muestra.\n\nEl factor de expansión se utiliza para extrapolar los resultados obtenidos en la muestra a la población total. Por ejemplo, para estimar un total poblacional para una variable, se pondera el valor medido en cada individuo por su factor de expansión y se suman dichos valores."
  },
  {
    "objectID": "unidad_2/05_muestreo.html#ejemplo-práctico",
    "href": "unidad_2/05_muestreo.html#ejemplo-práctico",
    "title": "Selección de la muestra",
    "section": "Ejemplo práctico",
    "text": "Ejemplo práctico\nSupongamos que queremos estimar la prevalencia de sobrepeso/obesidad en estudiantes de enseñanza media de una localidad. Se tiene la siguiente información:\n\nPoblación total de alumnos: 966\nNúmero de escuelas (conglomerados): 21\nTamaño de la muestra global (ya calculado): 120 alumnos\n\nSi se realizara un muestreo aleatorio simple (MAS), se necesitaría disponer del listado completo de los 966 alumnos y, mediante un sorteo, seleccionar 120 individuos. En ese caso, la probabilidad de selección para cada alumno sería:\n\\[\nP = \\frac{n}{N} = \\frac{120}{966}=0,124\n\\]\nSupongamos ahora que decide realizar un muestreo por conglomerados biétapico. Cada escuela se considera un conglomerado; se seleccionan 10 de las 21 escuelas por MAS y, dentro de cada escuela seleccionada, se elige una submuestra de estudiantes también mediante MAS. Se decide que el número de estudiantes elegidos en cada conglomerado sea igual para todos.\nLos datos globales quedan de la siguiente forma:\n\n\n\n\n\nPoblación\nMuestra\n\n\n\nTamaño total\n966\n120\n\n\nNúmero de conglomerados\n21\n10\n\n\n\n\n\nLa decisión de cuántos conglomerados forman parte de la muestra, es una decisión del investigador: a veces puede ser necesario fijar un número, a veces puede ser más conveniente fijar el tamaño de la muestra en cada conglomerado. Por otra parte, puede que decidamos que el número de sujetos seleccionados en cada conglomerado sea igual para todos; que sea una fracción fija o algo a definir.\nDentro de los conglomerados seleccionados, se obtiene la siguiente distribución (en este ejemplo, la suma de los tamaños de los conglomerados seleccionados es 480 y en cada uno se eligen 12 alumnos):\n\n\n\n\nConglomerado\nTamaño\nMuestra\n\n\n\n5.1\n48\n12\n\n\n1.3\n28\n12\n\n\n2.3\n64\n12\n\n\n2.2\n43\n12\n\n\n3.1\n37\n12\n\n\n5.3\n86\n12\n\n\n1.4\n37\n12\n\n\n3.3\n56\n12\n\n\n2.4\n48\n12\n\n\n1.5\n33\n12\n\n\nTotal\n480\n120\n\n\n\n\n\nCálculo de las Probabilidades de Selección y Ponderaciones\nPara que un individuo sea seleccionado en este diseño, debe ocurrir primero que su conglomerado sea escogido y, posteriormente, que él sea seleccionado dentro de ese conglomerado. Si asumimos que:\n\n“Dos sucesos son independientes si la probabilidad de que ocurran ambos simultáneamente es igual al producto de las probabilidades de que ocurra cada uno de ellos”.\n\nLa probabilidad total de selección es el producto de:\n\n\nLa probabilidad de que el conglomerado sea seleccionado:\n\\[\nP_{conglomerado} = \\frac{10}{21}\n\\]\n\n\nLa probabilidad de que un individuo sea seleccionado dentro de su conglomerado:\n\\[\nP_{dentro} = \\frac{N°~alumnos~en~la~muestra~del~conglomerado}{Tamaño~del~conglomerado}\n\\]\n\n\nPara los individuos del conglomerado 5.1; la probabilidad de selección será:\n\\[\nP_{5.1} = \\frac{10}{21}*\\frac{12}{48} \\approx 0,1190 \\quad (11.9\\%)\n\\]\nLa ponderación o factor de expansión se obtiene como el inverso de la probabilidad de selección:\n\\[\nw = \\frac{1}{P}\n\\]\nEs decir:\n\\[\nw_{5.1} = \\frac{1}{1.190} \\approx 8.4\n\\]\nEsto significa que cada alumno seleccionado del conglomerado 5.1 representa aproximadamente a 8 alumnos de la población.\nSi se realizan los cálculos para cada uno de los conglomerados, se obtiene la siguiente tabla:\n\n\n\n\nConglomerado\nProbabilidad\nPonderación\n\n\n\n5.1\n11.90\n8.40\n\n\n1.3\n20.41\n4.90\n\n\n2.3\n8.93\n11.20\n\n\n2.2\n13.29\n7.53\n\n\n3.1\n15.44\n6.48\n\n\n5.3\n6.65\n15.05\n\n\n1.4\n15.44\n6.48\n\n\n3.3\n10.20\n9.80\n\n\n2.4\n11.90\n8.40\n\n\n1.5\n17.32\n5.78\n\n\n\n\n\nCada individuo seleccionado en el estudio tiene una probabilidad de inclusión que depende tanto de la probabilidad de que su escuela (conglomerado) sea seleccionada como de la probabilidad de que él sea elegido dentro de su escuela. La ponderación resultante (el inverso de esta probabilidad) se utiliza para “expandir” el resultado del estudio, de manera que cada alumno en la muestra represente a un número determinado de alumnos en la población total.\nPor ejemplo, en el conglomerado 5.1, cada alumno seleccionado representa aproximadamente a 8 alumnos de la población. De este modo, al calcular la prevalencia de sobrepeso/obesidad en la muestra, se aplican estos factores de expansión para obtener estimaciones que sean representativas de los 966 alumnos de la localidad.\nEste proceso es fundamental en estudios basados en muestreos complejos, ya que permite corregir posibles diferencias en las probabilidades de selección y garantizar que las estimaciones sean válidas y precisas para toda la población.\n\nHernández-Ávila (2011)\nDaniel (2002)\nM. Rodríguez y Mendivelso (2018)\nRíus Díaz et al. (2012)\nTriola (2018)\nCáceres, Rafael Álvarez (2007)\nNorman, GR y Streiner, DL (1996)\nGlantz, S (2006)\nElm et al. (2008)\nSchulz et al. (2010)\nField, Miles, y Field (2014)\nV. Rodríguez et al. (2003)\n«EPIDAT 4.2 - Consellería de Sanidade - Servizo Galego de Saúde» (s. f.)"
  },
  {
    "objectID": "unidad_2/07_IC_complejos.html",
    "href": "unidad_2/07_IC_complejos.html",
    "title": "Intervalos de confianza en muestras complejas",
    "section": "",
    "text": "En los estudios epidemiológicos de corte transversal, es común el uso de encuestas poblacionales para estimar la frecuencia de determinadas condiciones de salud o la presencia de factores de riesgo en la población.\nSin embargo, la implementación de estas encuestas enfrenta restricciones prácticas que hacen inviable o poco conveniente el muestreo aleatorio simple (MAS). En su lugar, se recurre a otras alternativas de muestreo, como la estratificación, la selección en etapas, la formación de conglomerados o el empleo de probabilidades de selección desiguales.\nLos diseños muestrales que incorporan combinaciones de estas estrategias se denominan muestreos complejos, por contraste con el MAS, en el que las unidades muestrales se seleccionan independientemente unas de otras y todas tienen igual probabilidad de selección y distribución.\nEl análisis de los datos obtenidos mediante diseños muestrales complejos presenta desafíos adicionales debido a la posible correlación entre observaciones dentro de un mismo conglomerado. Esta correlación impide asumir independencia de las observaciones, condición necesaria para sostener el supuesto de normalidad. Si no se considera el efecto del diseño muestral al realizar estimaciones e intervalos de confianza (IC), los resultados pueden ser erróneos. Aspectos clave como el efecto del diseño, los dominios de estimación y los factores de expansión deben ser incorporados en el análisis para obtener inferencias válidas.\n\n\nSubpoblaciones “naturales” que, a priori, son homogéneos en su interior pero heterogéneos entre sí. El diseño de la encuesta se hace de modo que se garantiza cubrir adecuadamente todos los estratos de interés. Algunas variables de estratos habituales son: sexo, edad (como grupo etario), nivel de educación, urbano/rural, etc.\n\nUnidades definidas dentro de cada estrato (si es muestreo polietápico), cuyo tamaño es conocido. Lo ideal es que la población dentro de un conglomerado sea lo más heterogénea posible, pero debe haber homogeneidad entre los conglomerados. Cada grupo debe ser una representación a pequeña escala de la población total. Los grupos deben ser mutuamente excluyentes y colectivamente exhaustivos.\n\nEs la probabilidad que cada unidad tiene de estar incluida en la muestra. En los muestreos complejos polietápicos se obtiene, en general, multiplicando las probabilidades asociadas al estrato, a cada Unidad Primaria de Muestreo (UPM o PSU en inglés) dentro del estrato, y a la unidad de segunda etapa dentro cada UPM.\n\nSubconjuntos de la población objetivo cuyos elementos pueden ser identificados en el marco muestral sin ambigüedad, y en los que se permite desagregar los resultados de la encuesta. Es aconsejable respetar estos dominios de estimación y no realizar inferencia de parámetros de interés para otros dominios no previstos que conlleva estimaciones inválidas.\n\nEs el valor asociado a cada unidad elegible y que responde a la muestra, que se construye a partir de la inversa de la probabilidad de inclusión de cada unidad o peso muestral inicial. Puede incluir distintos tipos de ajustes, para disminuir en lo posible los errores de cobertura y de no respuesta que afectan a la encuesta, y ser tratados por un proceso de calibración que lleva en general a ganar eficiencia y precisión en las estimaciones. Los factores de expansión finales son los que se emplean tanto para generar todas las estimaciones de una encuesta, como en los cálculos del error muestral al determinar la precisión alcanzada.\n\nMide la pérdida en precisión al utilizar un diseño muestral complejo en lugar de un diseño aleatorio simple, por ejemplo, un efecto de diseño de 1,5 indica que la varianza del diseño complejo es 1,5 veces más grande que la varianza de un diseño aleatorio simple, en otras palabras se dio un aumento en la varianza de un 50%."
  },
  {
    "objectID": "unidad_2/07_IC_complejos.html#introducción",
    "href": "unidad_2/07_IC_complejos.html#introducción",
    "title": "Intervalos de confianza en muestras complejas",
    "section": "",
    "text": "En los estudios epidemiológicos de corte transversal, es común el uso de encuestas poblacionales para estimar la frecuencia de determinadas condiciones de salud o la presencia de factores de riesgo en la población.\nSin embargo, la implementación de estas encuestas enfrenta restricciones prácticas que hacen inviable o poco conveniente el muestreo aleatorio simple (MAS). En su lugar, se recurre a otras alternativas de muestreo, como la estratificación, la selección en etapas, la formación de conglomerados o el empleo de probabilidades de selección desiguales.\nLos diseños muestrales que incorporan combinaciones de estas estrategias se denominan muestreos complejos, por contraste con el MAS, en el que las unidades muestrales se seleccionan independientemente unas de otras y todas tienen igual probabilidad de selección y distribución.\nEl análisis de los datos obtenidos mediante diseños muestrales complejos presenta desafíos adicionales debido a la posible correlación entre observaciones dentro de un mismo conglomerado. Esta correlación impide asumir independencia de las observaciones, condición necesaria para sostener el supuesto de normalidad. Si no se considera el efecto del diseño muestral al realizar estimaciones e intervalos de confianza (IC), los resultados pueden ser erróneos. Aspectos clave como el efecto del diseño, los dominios de estimación y los factores de expansión deben ser incorporados en el análisis para obtener inferencias válidas.\n\n\nSubpoblaciones “naturales” que, a priori, son homogéneos en su interior pero heterogéneos entre sí. El diseño de la encuesta se hace de modo que se garantiza cubrir adecuadamente todos los estratos de interés. Algunas variables de estratos habituales son: sexo, edad (como grupo etario), nivel de educación, urbano/rural, etc.\n\nUnidades definidas dentro de cada estrato (si es muestreo polietápico), cuyo tamaño es conocido. Lo ideal es que la población dentro de un conglomerado sea lo más heterogénea posible, pero debe haber homogeneidad entre los conglomerados. Cada grupo debe ser una representación a pequeña escala de la población total. Los grupos deben ser mutuamente excluyentes y colectivamente exhaustivos.\n\nEs la probabilidad que cada unidad tiene de estar incluida en la muestra. En los muestreos complejos polietápicos se obtiene, en general, multiplicando las probabilidades asociadas al estrato, a cada Unidad Primaria de Muestreo (UPM o PSU en inglés) dentro del estrato, y a la unidad de segunda etapa dentro cada UPM.\n\nSubconjuntos de la población objetivo cuyos elementos pueden ser identificados en el marco muestral sin ambigüedad, y en los que se permite desagregar los resultados de la encuesta. Es aconsejable respetar estos dominios de estimación y no realizar inferencia de parámetros de interés para otros dominios no previstos que conlleva estimaciones inválidas.\n\nEs el valor asociado a cada unidad elegible y que responde a la muestra, que se construye a partir de la inversa de la probabilidad de inclusión de cada unidad o peso muestral inicial. Puede incluir distintos tipos de ajustes, para disminuir en lo posible los errores de cobertura y de no respuesta que afectan a la encuesta, y ser tratados por un proceso de calibración que lleva en general a ganar eficiencia y precisión en las estimaciones. Los factores de expansión finales son los que se emplean tanto para generar todas las estimaciones de una encuesta, como en los cálculos del error muestral al determinar la precisión alcanzada.\n\nMide la pérdida en precisión al utilizar un diseño muestral complejo en lugar de un diseño aleatorio simple, por ejemplo, un efecto de diseño de 1,5 indica que la varianza del diseño complejo es 1,5 veces más grande que la varianza de un diseño aleatorio simple, en otras palabras se dio un aumento en la varianza de un 50%."
  },
  {
    "objectID": "unidad_2/07_IC_complejos.html#paquetes-survey-y-srvyr",
    "href": "unidad_2/07_IC_complejos.html#paquetes-survey-y-srvyr",
    "title": "Intervalos de confianza en muestras complejas",
    "section": "Paquetes survey y srvyr",
    "text": "Paquetes survey y srvyr\nEl paquete más conocido y utilizado del lenguaje R para trabajar con datos provenientes de muestreos complejos es survey (Lumley 2024). Desarrollado por Thomas Lumley que a su vez es autor del libro Complex Surveys - A Guide to Analysis Using R (2010).\nLas funciones que lo integran utilizan en sus argumentos la sintaxis fórmula preferentemente y no son compatibles con el universo “ordenado” de tidyverse. Con el objetivo de hacer compatible estas funciones crearon un paquete “wrapper” (envoltorio) llamado srvyr (Freedman Ellis y Schneider 2024), que incorpora sintaxis tidy, haciendo uso de tuberías y funciones tales como group_by(), summarise(), entre otras.\nEl primer paso para trabajar con las funciones de srvyr es crear un objeto con la información relacionada con el diseño muestral. Esta tarea se realiza con la función as_survey_design() que tiene estos argumentos principales:\n\ndatos |&gt;                      \n  as_survey_design(ids = ..., \n                   strata = ..., \n                   variables = ...,\n                   fpc = ...,\n                   nest = F,\n                   weights = ...)\n\ndonde:\n\nids: Variables que especifican identificadores de conglomerados desde el nivel más grande hasta el nivel más pequeño (dejar el argumento vacío, NULL, 1 o 0 indica que no hay conglomerados).\nstrata: Variables que identifican a los estratos. Si no hay estratos, se ignora esta especificación.\n\nUsualmente, se declara a partir de una variable. Por ejemplo, si la variable estrato define los estratos, sería,\n\ndatos |&gt; \n  as_survey_design(..., \n                   strata = estrato, \n                   ...)\n\n\nvariables: Variables que se incluirán en el diseño. El valor predeterminado es todas las variables de datos.\nfpc: Variables con el factor de corrección por población finita.\nnest: Si es TRUE, re-etiqueta los conglomerados considerando anidamiento dentro de los estratos. Necesario activar cuando las etiquetas de las categorías de los conglomerados en los distintos estratos se llaman igual.\nweights: Variables de ponderación de cada observación (inverso a la probabilidad).\n\nLos argumentos mínimos que debemos definir dependerá de la estructura muestral de la base de datos que estemos analizando y de las variables que tengamos a disposición. Habitualmente se tiene referencia de estratos, conglomerados y ponderaciones. También podemos encontrarnos con situaciones donde están definidos los tamaños poblacionales (variable fpc)."
  },
  {
    "objectID": "unidad_2/07_IC_complejos.html#ejemplo-práctico-en-r",
    "href": "unidad_2/07_IC_complejos.html#ejemplo-práctico-en-r",
    "title": "Intervalos de confianza en muestras complejas",
    "section": "Ejemplo práctico en R",
    "text": "Ejemplo práctico en R\nPara ejemplificar vamos a tomar a la Encuesta Mundial de Salud Escolar (EMSE) en su tercera edición que en la Argentina se realizó en 2018. Fue llevada a cabo por el Ministerio de Salud y Desarrollo Social de la Nación, contó con la colaboración de los Ministerios de Educación Nacional y Provinciales, la OPS/OMS Argentina, OPS/OMS Washington y el CDC.\nEl diseño de muestreo tuvo dos etapas (selección de escuelas y luego de divisiones al azar) para producir una muestra representativa de alumnos de 1º a 5º año de educación media a nivel nacional (8º EGB a 3º polimodal en el caso de la provincia de Buenos Aires) y provincial. Se relevaron 523 escuelas en todo el país y se encuestaron 57.095 alumnos de los cuales se analizaron 56.981 cuestionarios correspondientes a las edades de 13 a 17 años, con una tasa de respuesta global de 63%. Se puede acceder a sus datos abiertos en EMSE 2018\nActivamos los paquetes necesarios:\n\nlibrary(srvyr)\nlibrary(tidyverse)\n\nLeemos el archivo de datos “EMSE.txt” con:\n\ndatos &lt;- read_csv2(\"datos/EMSE.txt\")\n\n# dimensiones de la tabla de datos\nglimpse(datos) \n\nRows: 56,981\nColumns: 14\n$ record     &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, …\n$ q2         &lt;dbl&gt; 2, 2, 1, 1, 2, 1, 2, 2, 1, 2, 1, 2, 2, 2, 2, 2, 1, 1, 2, 1,…\n$ texto_q2   &lt;chr&gt; \"Femenino\", \"Femenino\", \"Masculino\", \"Masculino\", \"Femenino…\n$ q3         &lt;dbl&gt; 2, 2, 4, 2, 2, 2, 4, 4, 4, 2, 2, 4, 2, 4, 4, 4, 4, 2, 2, 2,…\n$ texto_q3   &lt;chr&gt; \"9no grado nivel Primario/Polimodal o 2do año nivel Secunda…\n$ q4         &lt;dbl&gt; NA, NA, 1.67, 1.73, NA, 1.72, NA, NA, NA, NA, 1.64, NA, NA,…\n$ q5         &lt;dbl&gt; NA, NA, 56, 70, NA, 48, NA, NA, NA, NA, 45, NA, NA, 40, 52,…\n$ q6         &lt;dbl&gt; 2, 1, 1, 1, 1, 1, 3, 1, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ texto_q6   &lt;chr&gt; \"Rara vez\", \"Nunca\", \"Nunca\", \"Nunca\", \"Nunca\", \"Nunca\", \"A…\n$ qn24       &lt;dbl&gt; 1, 2, NA, 2, 2, 2, 2, 2, 2, 1, 2, 1, 2, NA, 1, 2, 2, 2, 2, …\n$ texto_qn24 &lt;chr&gt; \"Si\", \"No\", \"Dato perdido\", \"No\", \"No\", \"No\", \"No\", \"No\", \"…\n$ psu        &lt;dbl&gt; 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19,…\n$ stratum    &lt;dbl&gt; 201801010, 201801010, 201801010, 201801010, 201801010, 2018…\n$ weight     &lt;dbl&gt; 457.09, 457.09, 261.44, 550.90, 457.09, 550.90, 274.43, 274…\n\n\nEl dataframe importado consta de 14 variables y 56.981 observaciones.\nCalculamos el total de la muestra expandida:\n\ndatos |&gt; \n  summarise(total_expandido = sum(weight))\n\n# A tibble: 1 × 1\n  total_expandido\n            &lt;dbl&gt;\n1         2637546\n\n\nLas variables relevantes para el diseño muestral que el documento de usuario define son:\n\npsu: Unidades primarias de muestreo (conglomerados)\nstratum: Estratos del muestreo\nweight: Ponderación\n\nAdemás, nuestras variables de interés son:\n\nrecord: Nro. de registro\nq2: Sexo\ntexto_q2: Sexo categorizado\nq3: Grado / año en el que se encuentra el estudiante\ntexto_q3: Grado / año en el que se encuentra el estudiante categorizado\nq4: Estatura sin zapatos (medido en metros)\nq5: Peso sin zapatos (medido en kilogramos)\nq6: Pregunta sobre hambre\ntexto_q6: Pregunta sobre hambre categorizada\nqn24: Pregunta sobre idea suicida\ntexto_qn24: Pregunta sobre idea suicida categorizada\n\nPara facilitar el análisis, vamos a renombrar nuestras variables de interés usando la función rename() de tidyverse:\n\ndatos &lt;- datos |&gt; \n  rename(sexo = texto_q2,\n         grado = texto_q3,\n         estatura_m = q4,\n         peso_kg = q5,\n         hambre = texto_q6,\n         idea_suicida = texto_qn24)\n\nA partir de las variables cuantitativas estatura_m y peso_kg vamos a construir una nueva variable llamada imc (Índice de Masa Corporal) :\n\ndatos &lt;- datos |&gt; \n  # IMC: peso sobre talla al cuadrado\n  mutate(imc = peso_kg/(estatura_m)^2)\n\nAhora podemos comenzar a definir el objeto de diseño muestral:\n\nd &lt;- datos |&gt; \n  as_survey_design(ids = psu, # conglomerados\n                   variables = c(record, psu, stratum, weight,\n                                 q2, sexo, q3, grado, \n                                 peso_kg, estatura_m, imc, q6, hambre,\n                                 qn24, idea_suicida), # variables\n                   strata = stratum, # estratos\n                   weights = weight, # ponderación\n                   nest = T) # anidación\n\nUna vez que tenemos creado el objeto “survey.design”, que en nuestro ejemplo se denomina d, podemos avanzar en el calculo de las estimaciones.\nSi bien podemos manipular datos dentro de este formato de diseño utilizando algunas de las funciones de tidyverse como select(), mutate(), filter() y rename(), lo conveniente es modificar previamente la estructura de la tabla de datos, ya sea para crear una nueva variable, como hicimos recién con IMC o cambiar una existente y luego generar el objeto con el diseño muestral para poder hacer uso de esos cambios en las estimaciones.\nEstimaciones\n\n\n\n\n\n\nTodos los códigos para las estimaciones comenzaran a partir del objeto de diseño creado inicialmente (llamado d en nuestro ejemplo) y evitando utilizar el nombre de la tabla de datos leída que solo tiene los datos de la muestra sin especificar el diseño de muestreo.\n\n\n\nEn primer lugar tomaremos la variable imc, variable cuantitativa continua que construimos.\nLa función survey_mean() computa estimaciones de medias en diseños complejos usando la sintaxis de tidyverse.\nLos siguientes son los argumentos comunes de la función:\n\nx: Variable o expresión o vacía.\nna.rm: Si es TRUE, omite los valores NA de la variable\nvartype: Obtiene la variabilidad como uno o más estimadores: error estándar (\"se\", predeterminado), intervalo de confianza (\"ci\"), varianza (\"var\") o coeficiente de variación (\"cv\").\nlevel: Nivel de confianza (solo se usa si el anterior es \"ci\")\ndeff: Si es TRUE, calcula el efecto de diseño para la estimación\n\nAplicamos la función con todas las posibles salidas de varianza sobre la variable de imc dentro de summarise() de tidyverse:\n\nd |&gt; \n  summarise(imc = survey_mean(x = imc,\n              na.rm = T,\n              vartype = c(\"se\", \"ci\", \"var\", \"cv\"),\n              level = 0.95))\n\n# A tibble: 1 × 6\n    imc imc_se imc_low imc_upp imc_var  imc_cv\n  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1  22.1 0.0869    22.0    22.3 0.00755 0.00392\n\n\nEl resultado muestra una media de IMC de 22,1, un error estándar de 0,09, un intervalo de confianza al 95% de (22,0-22,3), una varianza de 0,007 y un coeficiente de variación de 0,4 %.\nComo nos informan que hubo no respuesta en la encuesta y las faltantes de información en altura y/o peso provoca observaciones perdidas en la variable IMC, vamos a ver cuantos de estos valores perdidos tenemos en la variable.\nSi deseamos calcular totales, es decir a cuantos estudiantes representa estos valores perdidos podemos usar la función survey_total():\n\nd |&gt;  \n  filter(is.na(imc)) %&gt;%  # filtramos los NA en IMC\n  \n  summarise(imc_na = survey_total(vartype = \"ci\"))\n\n# A tibble: 1 × 3\n   imc_na imc_na_low imc_na_upp\n    &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;\n1 976104.    878120.   1074088.\n\n\nHay aproximadamente 976.104 IC 95% (87.820-1.074.088) estudiantes sin valor en la variable imc del total de la población de 2.637.546 (un 37 %).\nSi en lugar de la media quisiéramos la mediana podemos cambiar la función y usar survey_median():\n\nd |&gt; \n  summarise(imc = survey_median(x = imc,\n              na.rm = T,\n              vartype = c(\"se\", \"ci\", \"var\", \"cv\"),\n              level = 0.95))\n\n# A tibble: 1 × 6\n    imc imc_se imc_low imc_upp imc_var  imc_cv\n  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1  21.5 0.0772    21.4    21.7 0.00596 0.00359\n\n\nPodemos estratificar estas estimaciones haciendo uso del group_by(), por ejemplo con la variable sexo del estudiante:\n\nd |&gt; \n  group_by(sexo)  |&gt; \n  summarise(media_imc = survey_mean(imc, \n                                    vartype = \"ci\", \n                                    na.rm = T))\n\n# A tibble: 3 × 4\n  sexo         media_imc media_imc_low media_imc_upp\n  &lt;chr&gt;            &lt;dbl&gt;         &lt;dbl&gt;         &lt;dbl&gt;\n1 Dato perdido       0             0             0  \n2 Femenino          22.0          21.8          22.2\n3 Masculino         22.3          22.1          22.5\n\n\nComo hay datos perdidos en la variable sexo nos aparece una categoría más en los grupos de los resultados. Al estar vacía en las estimaciones (evidentemente son “no respuestas” completas) podemos deshacernos de esa línea con un filter():\n\nd |&gt; \n  filter(sexo != \"Dato perdido\") |&gt; # o filter(!is.na(q2))\n  \n  group_by(sexo) |&gt; \n  summarise(media_imc = survey_mean(imc, \n                                    vartype = \"ci\", \n                                    na.rm = T))\n\n# A tibble: 2 × 4\n  sexo      media_imc media_imc_low media_imc_upp\n  &lt;chr&gt;         &lt;dbl&gt;         &lt;dbl&gt;         &lt;dbl&gt;\n1 Femenino       22.0          21.8          22.2\n2 Masculino      22.3          22.1          22.5\n\n\nEsta tabla muestra las estimaciones de imc según sexo con sus intervalos de confianza.\nOtra variable de la encuesta es la respuesta a la pregunta “Durante los últimos 30 días ¿con qué frecuencia te quedaste con hambre porque no había suficiente comida en tu hogar?”. Las categorías válidas fueron: Nunca, Rara vez, Algunas veces, Casi siempre, Siempre y Dato perdido si no hubo respuesta.\nPara abordar estas variables cualitativas donde queremos obtener proporciones el paquete tiene la función survey_prop() que se utiliza declarando la variable de interés en un group_by() previo:\n\nd |&gt;  \n  group_by(hambre) %&gt;% # variable para la estimación\n  summarise(prop_hambre = survey_prop(vartype = \"ci\")*100) # usamos 100 para %\n\n# A tibble: 6 × 4\n  hambre        prop_hambre prop_hambre_low prop_hambre_upp\n  &lt;chr&gt;               &lt;dbl&gt;           &lt;dbl&gt;           &lt;dbl&gt;\n1 Algunas veces       9.34            8.49           10.3  \n2 Casi siempre        1.36            1.18            1.56 \n3 Dato perdido        0.804           0.678           0.952\n4 Nunca              67.8            66.6            69.0  \n5 Rara vez           20.1            19.2            20.9  \n6 Siempre             0.627           0.487           0.807\n\n\nEl resultado muestra las categorías desordenadas en relación a lo que significa cada una respecto a la definición de la pregunta, es decir a la ordinalidad de la variable, aunque si está ordenada respecto a la forma en que lo hace el lenguaje R (alfabético).\nPara darle el orden adecuado, deberíamos convertir la variable hambre a factor y declarar correctamente sus niveles. Conviene hacer esto previo a la creación del objeto de diseño pero el paquete también lo permite una vez creado:\n\nd &lt;- d |&gt; \n  mutate(hambre = factor(hambre,\n                         c(\"Nunca\", \"Rara vez\", \"Algunas veces\",\n                           \"Casi siempre\", \"Siempre\", \"Dato perdido\")))\n\nRepetimos el código anterior:\n\nd |&gt; \n  group_by(hambre) |&gt; \n  summarise(prop_hambre = survey_prop(vartype = \"ci\")*100)\n\n# A tibble: 6 × 4\n  hambre        prop_hambre prop_hambre_low prop_hambre_upp\n  &lt;fct&gt;               &lt;dbl&gt;           &lt;dbl&gt;           &lt;dbl&gt;\n1 Nunca              67.8            66.6            69.0  \n2 Rara vez           20.1            19.2            20.9  \n3 Algunas veces       9.34            8.49           10.3  \n4 Casi siempre        1.36            1.18            1.56 \n5 Siempre             0.627           0.487           0.807\n6 Dato perdido        0.804           0.678           0.952\n\n\nEl ordenamiento de las categorías ahora es el correcto producto de la transformación a factor.\nPara estratificar estas proporciones lo único que debemos hacer es incorporar la variable de agrupamiento dentro del group_by():\n\nd |&gt; \n  group_by(sexo, hambre) |&gt; \n  summarise(prop_hambre = survey_prop(vartype = \"ci\")*100) \n\n# A tibble: 18 × 5\n# Groups:   sexo [3]\n   sexo         hambre        prop_hambre prop_hambre_low prop_hambre_upp\n   &lt;chr&gt;        &lt;fct&gt;               &lt;dbl&gt;           &lt;dbl&gt;           &lt;dbl&gt;\n 1 Dato perdido Nunca              54.3            43.9            64.3  \n 2 Dato perdido Rara vez           23.5            17.3            31.2  \n 3 Dato perdido Algunas veces      14.2             8.18           23.5  \n 4 Dato perdido Casi siempre        1.07            0.459           2.49 \n 5 Dato perdido Siempre             1.18            0.589           2.36 \n 6 Dato perdido Dato perdido        5.74            3.45            9.41 \n 7 Femenino     Nunca              68.1            66.7            69.4  \n 8 Femenino     Rara vez           19.7            18.7            20.8  \n 9 Femenino     Algunas veces       9.78            8.96           10.7  \n10 Femenino     Casi siempre        1.18            0.933           1.49 \n11 Femenino     Siempre             0.549           0.389           0.775\n12 Femenino     Dato perdido        0.673           0.530           0.854\n13 Masculino    Nunca              67.8            66.2            69.4  \n14 Masculino    Rara vez           20.3            19.2            21.5  \n15 Masculino    Algunas veces       8.75            7.69            9.94 \n16 Masculino    Casi siempre        1.56            1.20            2.01 \n17 Masculino    Siempre             0.700           0.518           0.946\n18 Masculino    Dato perdido        0.842           0.624           1.13 \n\n\nEste esquema de agrupamientos o estratificaciones se anidan en el orden en que se declaran dentro del group_by() siendo la última variable la de estimación. Por ende, los porcentajes también salen anidados, tomando como denominador para el calculo del 100 % al total de cada categoría de sexo en el ejemplo anterior.\nTambién podemos hacer que para el calculo de los porcentajes se tome como denominador el total general de la siguiente forma:\n\nd |&gt; \n  group_by(interact(sexo, hambre)) |&gt; \n  summarise(prop_hambre = survey_prop(vartype = \"ci\")*100)\n\n# A tibble: 18 × 5\n   sexo         hambre        prop_hambre prop_hambre_low prop_hambre_upp\n   &lt;chr&gt;        &lt;fct&gt;               &lt;dbl&gt;           &lt;dbl&gt;           &lt;dbl&gt;\n 1 Dato perdido Nunca              0.539          0.429            0.677 \n 2 Dato perdido Rara vez           0.234          0.160            0.342 \n 3 Dato perdido Algunas veces      0.141          0.0755           0.263 \n 4 Dato perdido Casi siempre       0.0107         0.00464          0.0245\n 5 Dato perdido Siempre            0.0117         0.00594          0.0232\n 6 Dato perdido Dato perdido       0.0570         0.0328           0.0990\n 7 Femenino     Nunca             35.0           33.7             36.4   \n 8 Femenino     Rara vez          10.2            9.58            10.8   \n 9 Femenino     Algunas veces      5.03           4.63             5.47  \n10 Femenino     Casi siempre       0.608          0.481            0.768 \n11 Femenino     Siempre            0.283          0.200            0.400 \n12 Femenino     Dato perdido       0.346          0.270            0.443 \n13 Masculino    Nunca             32.2           31.1             33.4   \n14 Masculino    Rara vez           9.66           9.11            10.2   \n15 Masculino    Algunas veces      4.16           3.63             4.76  \n16 Masculino    Casi siempre       0.741          0.569            0.965 \n17 Masculino    Siempre            0.333          0.246            0.450 \n18 Masculino    Dato perdido       0.400          0.295            0.543 \n\n\nUn argumento incluido dentro de las posibilidades de survey_prop() es el método para su calculo (prop_method). En este documento no vamos a profundizar en los métodos que la función ofrece, solo mencionaremos las posibilidades permitidas según la ayuda del paquete:\n\n\"likelihood\": utiliza la distribución de chi-cuadrado escalada (Rao-Scott) para la log-likelihood de una distribución binomial.\n\"asin\": usa la transformación estabilizadora de la varianza para la distribución binomial, la raíz cuadrada del arcoseno, y luego transforma el intervalo a la escala de probabilidad.\n\"beta\": usa la función beta incompleta como en binom.test, con una tamaño de muestra efectivo basado en la varianza estimada de la proporción. (Korn y Graubard, 1998).\n\"mean\": utiliza un intervalo tipo Wald en la escala de probabilidad, igual que confint().\n\"logit\": ajusta un modelo de regresión logística y calcula un intervalo tipo Wald en la escala logarítmica de probabilidades, que luego se transforma a la escala de probabilidad. Es el método predeterminado de la función.\n\nOtra función útil es survey_count() que estima el conteo poblacional de las observaciones según categoría:\n\nd |&gt;  \n  survey_count(hambre,\n               vartype = \"ci\") \n\n# A tibble: 6 × 4\n  hambre               n    n_low    n_upp\n  &lt;fct&gt;            &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 Nunca         1788701. 1683670. 1893733.\n2 Rara vez       529025.  498681.  559370.\n3 Algunas veces  246227.  213940.  278513.\n4 Casi siempre    35857.   30276.   41438.\n5 Siempre         16542.   12251.   20832.\n6 Dato perdido    21194.   16920.   25468.\n\n\nAquí lo usamos acompañando con el intervalo de confianza del 95% (predeterminado).\nAhora tomaremos otra variable de la EMSE2018 llamada idea_suicida que refiere a la pregunta: “Durante los últimos 12 meses, ¿alguna vez consideraste seriamente la posibilidad de intentar suicidarte?”. Sus posibles respuestas son \"Si\", \"No\" y el habitual \"Dato perdido\". Estimaremos su proporción para toda la población pero también para una subpoblación en particular, por ejemplo para “3er año/12vo grado nivel Polimodal o 5to año nivel Secundario”.\nComo es habitual que necesitemos obtener estimaciones en subgrupos o subpoblaciones determinados por categorías definidas de una variable, debemos tener cuidado al interpretar los elementos del muestro para que esas subpoblaciones hayan sido consideradas y por lo tanto estén incluídas como dominio de estimación.\nEstos subgrupos requeridos pueden no coincidir con los estratos de la muestra compleja generando un inconveniente para las estimaciones, dado que las ponderaciones muestrales serían correctas pero la probabilidad de muestreo seguramente no, lo que produce estimaciones puntuales correctas con errores estándar incorrectos.\nLas funciones del paquete srvyr maneja estos detalles sin ningún esfuerzo especial por parte del analista siempre y cuando utilice la muestra completa para definir el objeto de diseño de la encuesta pero no asegura que los resultados tengan la calidad necesaria.\nMostremos el código entonces:\n\nd |&gt;  \n  group_by(idea_suicida) |&gt; \n  \n  summarise(prop_suicidio = survey_prop(vartype = c(\"ci\", \"cv\"))*100) \n\n# A tibble: 3 × 5\n  idea_suicida prop_suicidio prop_suicidio_low prop_suicidio_upp\n  &lt;chr&gt;                &lt;dbl&gt;             &lt;dbl&gt;             &lt;dbl&gt;\n1 Dato perdido          2.16              1.90              2.45\n2 No                   76.8              76.1              77.6 \n3 Si                   21.0              20.3              21.7 \n# ℹ 1 more variable: prop_suicidio_cv &lt;dbl&gt;\n\n\nSeleccionamos una subpoblación correspondiente a los estudiantes de “3er año/12vo grado nivel Polimodal o 5to año nivel Secundario” (código 5 de q3):\n\nd |&gt; \n  filter(q3 == 5) |&gt; \n  \n  group_by(idea_suicida) |&gt; \n  \n  summarise(prop_suicidio = survey_prop(vartype = c(\"ci\", \"cv\"))*100)\n\n# A tibble: 3 × 5\n  idea_suicida prop_suicidio prop_suicidio_low prop_suicidio_upp\n  &lt;chr&gt;                &lt;dbl&gt;             &lt;dbl&gt;             &lt;dbl&gt;\n1 Dato perdido          1.71              1.27              2.30\n2 No                   79.3              77.6              81.0 \n3 Si                   18.9              17.5              20.4 \n# ℹ 1 more variable: prop_suicidio_cv &lt;dbl&gt;\n\n\nUn 21,0% (95% IC: 20,3-21,7%) del total de estudiantes dicen haber considerado un intento de suicidio en el último año, mientras que un 18,9% (95% IC: 17,5-20,4%) dice lo mismo en el grupo de “3er año/12vo grado nivel Polimodal o 5to año nivel Secundario”.\nAgregamos a los resultados algo importante, que va a garantizar la calidad de la estimaciones y evitar estar informando valores poco confiables para los cuales el muestreo quizás no esté preparado. Este es el coeficiente de variación (CV). Comparativamente vamos a encontrar valores más elevados de CV, en la medida que se agreguen filtros o agrupamientos anidados que reduzcan la muestra de la estimación.\nQue pasa si filtramos y nos quedamos además solo con las mujeres que respondieron que se quedaron con hambre “algunas veces” en los últimos 30 días:\n\nd |&gt; \n  filter(q3 == 5, sexo == \"Femenino\", hambre == \"Algunas veces\") |&gt; \n  \n  group_by(idea_suicida) |&gt; \n  summarise(prop_suicidio = survey_prop(vartype = c(\"ci\", \"cv\"))*100)\n\n# A tibble: 3 × 5\n  idea_suicida prop_suicidio prop_suicidio_low prop_suicidio_upp\n  &lt;chr&gt;                &lt;dbl&gt;             &lt;dbl&gt;             &lt;dbl&gt;\n1 Dato perdido          3.15             0.941              10.0\n2 No                   63.5             53.7                72.3\n3 Si                   33.4             25.6                42.2\n# ℹ 1 more variable: prop_suicidio_cv &lt;dbl&gt;\n\n\nLos CV de las estimaciones siguen creciendo, por ejemplo en el caso de la respuesta Si a un 12,6 % en comparación de la población completa que era de 1,6 %. Este aumento tiene que ver sobre todo con el tamaño muestral que nos queda luego de tantos filtros:\n\n# Total muestra\nd |&gt;  \n  survey_count(idea_suicida)\n\n# A tibble: 3 × 3\n  idea_suicida        n   n_se\n  &lt;chr&gt;           &lt;dbl&gt;  &lt;dbl&gt;\n1 Dato perdido   56885.  4176.\n2 No           2026726. 56584.\n3 Si            553935. 21644.\n\n# Subgrupo seleccionado\nd |&gt; \n  filter(q3 == 5, sexo == \"Femenino\", hambre == \"Algunas veces\") |&gt; \n  survey_count(idea_suicida)\n\n# A tibble: 3 × 3\n  idea_suicida      n  n_se\n  &lt;chr&gt;         &lt;dbl&gt; &lt;dbl&gt;\n1 Dato perdido   600.  356.\n2 No           12084. 1695.\n3 Si            6348.  808.\n\n\nSolo 6.347 estudiantes expandidos que respondieron que Si sobre los 553.934 totales que lo hicieron. Y cuantos respecto a la muestra sin expandir?\nPodemos verlo así:\n\n# Total muestra\nd |&gt;  \n  group_by(idea_suicida) |&gt; \n  \n  summarise(n = unweighted(n()))\n\n# A tibble: 3 × 2\n  idea_suicida     n\n  &lt;chr&gt;        &lt;int&gt;\n1 Dato perdido  1353\n2 No           43666\n3 Si           11962\n\n# Subgrupo seleccionado\nd |&gt; \n  filter(q3 == 5, sexo == \"Femenino\", hambre == \"Algunas veces\") |&gt;\n  \n  group_by(idea_suicida) |&gt; \n  \n  summarise(n = unweighted(n()))\n\n# A tibble: 3 × 2\n  idea_suicida     n\n  &lt;chr&gt;        &lt;int&gt;\n1 Dato perdido     9\n2 No             287\n3 Si             194\n\n\n194 estudiantes de la muestra sin expandir respondieron que Si sobre 11.962 sin aplicar los filtros."
  },
  {
    "objectID": "unidad_2/07_IC_complejos.html#errores-estándar-según-efecto-del-diseño",
    "href": "unidad_2/07_IC_complejos.html#errores-estándar-según-efecto-del-diseño",
    "title": "Intervalos de confianza en muestras complejas",
    "section": "Errores estándar según efecto del diseño",
    "text": "Errores estándar según efecto del diseño\nEn este punto vamos a comparar estimaciones en función de construir objetos de diseños muestrales diferentes sobre la misma base de datos.\nDiseño complejo y pesos (igual al ejemplo anterior):\n\nd_complejo &lt;- datos |&gt; \n  as_survey_design(ids = psu, # conglomerados\n                   variables = c(record, psu, stratum, weight,\n                                 q2, sexo, q3, grado, \n                                 peso_kg, estatura_m, imc, q6, hambre,\n                                 qn24, idea_suicida), # variables\n                   strata = stratum, # estratos\n                   weights = weight, # ponderación\n                   nest = T) # anidación\n\nDiseño sin pesos ni estratos:\n\nd_simple &lt;- datos |&gt; \n  as_survey_design(ids = psu, # conglomerados\n                   variables = c(record, psu, stratum, weight,\n                                 q2, sexo, q3, grado, \n                                 peso_kg, estatura_m, imc, q6, hambre,\n                                 qn24, idea_suicida), # variables\n                   strata = NULL, # estratos\n                   weights = NULL, # ponderación\n                   nest = T) # anidación\n\nDiseño con pesos y sin estratos:\n\nd_ponde &lt;- datos |&gt; \n  as_survey_design(ids = psu, # conglomerados\n                   variables = c(record, psu, stratum, weight,\n                                 q2, sexo, q3, grado, \n                                 peso_kg, estatura_m, imc, q6, hambre,\n                                 qn24, idea_suicida), # variables\n                   strata = NULL, # estratos\n                   weights = weight, # ponderación\n                   nest = T) # anidación\n\nTenemos tres diseños distintos: un diseño complejo basado en conglomerados, estratos y pesos, un diseño sólo con pesos y un diseño simple sin estratos ni ponderación. Sabemos que el primero es el diseño “real” con el que se llevó a cabo la recolección de los datos.\nAhora volvamos a la variable imc para estimar su media en cada diseño:\n\nd_complejo |&gt; \n  summarise(media_IMC = survey_mean(x = imc,\n                                    na.rm = T,\n                                    vartype = c(\"ci\", \"cv\")))\n\n# A tibble: 1 × 4\n  media_IMC media_IMC_low media_IMC_upp media_IMC_cv\n      &lt;dbl&gt;         &lt;dbl&gt;         &lt;dbl&gt;        &lt;dbl&gt;\n1  22.13994      21.96900      22.31089  0.003923691\n\nd_simple  |&gt;  \n  summarise(media_IMC = survey_mean(x = imc,\n                                    na.rm = T,\n                                    vartype = c(\"ci\", \"cv\")))\n\n# A tibble: 1 × 4\n  media_IMC media_IMC_low media_IMC_upp media_IMC_cv\n      &lt;dbl&gt;         &lt;dbl&gt;         &lt;dbl&gt;        &lt;dbl&gt;\n1  22.23472      22.16037      22.30906  0.001669130\n\nd_ponde  |&gt;  \n  summarise(media_IMC = survey_mean(x = imc,\n                                    na.rm = T,\n                                    vartype = c(\"ci\", \"cv\")))\n\n# A tibble: 1 × 4\n  media_IMC media_IMC_low media_IMC_upp media_IMC_cv\n      &lt;dbl&gt;         &lt;dbl&gt;         &lt;dbl&gt;        &lt;dbl&gt;\n1  22.13994      21.98614      22.29375  0.003467962\n\n\nLa estimación puntual (imc = 22,14) es la misma para los análisis que usan el diseño complejo y los que usan sólo pesos. A su vez es diferente si las estimaciones se llevan a cabo sin considerar el factor de expansión (muestreo simple - imc = 22,23). Los intervalos de confianza estimados son diferentes para todos los casos porque la varianza se vincula con la estructura del muestreo (estratos y conglomerados). Si no se utilizan pesos en las estimaciones, los estimadores no serán representativos de la población muestreada.\nSi solo se utilizan pesos sin considerar el diseño complejo, en general, se infravalorará la dispersión de los estimadores, llevando a intervalos de confianza excesivamente estrechos y a niveles de significación reales mayores que los nominales (como si se tratase de un MAS). Basta comparar el resultado correcto IMC 22,14 (95% IC: 21,97-22,31) frente al producido con el diseño simple IMC 22,23 (95% IC: 22,16-22,30).\nComo vemos, el lenguaje R permite que construyamos el objeto de diseño de la muestra compleja con la forma que indiquemos pero esto no siempre es correcto. Debemos conocer previamente la forma en que se diseño el muestreo y como se expresa en las variables de la tabla de datos para hacerlo adecuadamente. Es fundamental leer detenidamente los documentos de utilización de los datos de toda encuesta donde se hayan recolectado datos mediante muestreos complejos.\nCriterios de calidad en las estimaciones\nTodas las estimaciones elaboradas a partir de datos obtenidos por encuestas poblacionales con muestreos complejos están sujetas al error muestral, lo que hace necesario evaluar su validez estadística mediante diversos indicadores de precisión y confiabilidad.\nVeamos algunos de estos indicadores de calidad:\nCoeficiente de variación\nEs el principal indicador de calidad de una estimación. Esta medida configura un acercamiento al error de muestreo que permite verificar si la inferencia es válida. Se caracteriza por ser proporcional a la amplitud del intervalo de confianza, que provee una versión estandarizada y relativa de la precisión alrededor de la estimación puntual.\nUn umbral aproximado de CV mayor a 20%-30% puede asumirse como un valor de referencia útil a nivel regional para señalar una cifra de poco confiable, Gutiérrez y otros (2020). Para calcular el coeficiente de variación de las estimaciones en estas estimaciones basta con incluirlo dentro del argumento vartype.\nTamaño de muestra\nEste criterio se encuentra generalmente relacionado al anterior y es relevante a la hora de decidir la calidad de la estimación. La cobertura de los intervalos de confianza y la distribución de los estimadores dependen de que, tanto el tamaño de la subpoblación como su tamaño de muestra asociado, no sean pequeños. Con este espíritu, Gutiérrez y otros (2020) proponen que todas las estimaciones basadas en un tamaño de muestra expandida menor a 100 unidades deberían ser marcadas como no confiables.\nConteo de casos no ponderado\nCuando la incidencia de un fenómeno es muy baja y el diseño de la encuesta no lo tuvo en cuenta, entonces es posible que las estimaciones asociadas a tamaños, totales y proporciones sobre este fenómeno no sean confiables. Por ejemplo, National Research Council (2015) plantea que si el número de casos no ponderados es menor a 50 unidades entonces la estimación no sería publicable.\n\n«EPIDAT 4.2 - Consellería de Sanidade - Servizo Galego de Saúde» (s. f.)\nBell et al. (2012)\nSakshaug y West (2014)\nNational Research Council; Division of Behavioral and Social Sciences and Education (2015)\nAycaguer (2000)\nLumley (2011)\nGutiérrez et al. (2020)"
  },
  {
    "objectID": "unidad_3/02_mod_estadistico.html",
    "href": "unidad_3/02_mod_estadistico.html",
    "title": "Introducción al modelado estadístico",
    "section": "",
    "text": "Uno de los propósitos fundamentales del modelado estadístico es explicar la realidad de la manera más “simple” posible, centrándose en su esencia y omitiendo elementos cuya variabilidad podría introducir ruido en la interpretación de los fenómenos. Existen eventos en la naturaleza que pueden ser explicados con exactitud si se cuenta con los datos adecuados. Por ejemplo, el cálculo del volumen de un cubo o la predicción de la trayectoria de un objeto en caída libre bajo condiciones ideales. Los modelos que explican estos fenómenos se denominan modelos determinísticos.\nSin embargo, cuando se estudian fenómenos en sistemas complejos, la situación se vuelve más desafiante debido a la influencia de factores externos que impiden explicar completamente la variable dependiente a partir de otras variables. Para abordar esta incertidumbre, se emplean modelos que incorporan un componente de error, conocidos como modelos probabilísticos. Estos modelos constituyen la base de la estadística inferencial y se utilizan en el análisis de regresión para describir y predecir relaciones entre variables.\n\nEn los modelos determinísticos, se asume una relación exacta entre las variables. Esto significa que tanto los parámetros como las variables son conocidos sin error, y el modelo genera siempre el mismo resultado para un conjunto de datos dado. Sin embargo, estos modelos no consideran la variabilidad causada por el azar o la incertidumbre.\nUn ejemplo clásico de modelos determinísticos son los modelos SIR (Susceptibles, Infectados, Recuperados), empleados para estudiar la propagación de enfermedades en poblaciones cerradas. Estos modelos asumen tasas de transmisión y recuperación constantes, ignorando la variabilidad individual y otros factores externos.\n\nLos modelos probabilísticos asumen que los fenómenos observados no son completamente predecibles, ya que ciertas variables están sujetas al azar. Estos modelos no generan un único resultado, sino una distribución de resultados expresada en términos de probabilidades.\nUn modelo de regresión describe la relación entre la variable dependiente (\\(Y\\)) y una o más variables independientes (\\(X\\)), incorporando un componente sistemático (función del modelo) y un componente aleatorio (residuo o error residual).\nMatemáticamente, esto se representa como:\n\\[\nY = \\underbrace{\\beta_0 + \\beta_1 X_1}_{Componente \\: sistemático} + \\underbrace{\\epsilon}_{Componente \\: aleatorio}\n\\]\nLa función puede ser lineal o no lineal según la naturaleza y distribución de la variable dependiente. El componente aleatorio es esa parte de la variación de \\(Y\\) que no puede ser totalmente explicada por la variación de la/s variable/s independiente/s. En algunos casos el error tendrá valor positivo y en otros tendrá valor negativo. El promedio del error es igual a 0.\nUna vez establecida esta función estaremos en condiciones de:\n\nComprender cómo se comporta la variable respuesta \\(Y\\) en función de la/s variable/s independientes (\\(X\\)).\nEstimar o predecir el valor de \\(Y\\) para determinados valores de \\(X\\).\nCalcular intervalos de confianza para estas estimaciones o predicciones.\n\nLos modelos probabilísticos pueden clasificarse en:\n\nParamétricos: Asumen que los datos provienen de una distribución específica con un conjunto fijo de parámetros (por ejemplo, media, varianza).\nNo paramétricos: Se utilizan cuando la distribución subyacente de los datos es desconocida o no sigue un patrón estándar."
  },
  {
    "objectID": "unidad_3/02_mod_estadistico.html#introducción",
    "href": "unidad_3/02_mod_estadistico.html#introducción",
    "title": "Introducción al modelado estadístico",
    "section": "",
    "text": "Uno de los propósitos fundamentales del modelado estadístico es explicar la realidad de la manera más “simple” posible, centrándose en su esencia y omitiendo elementos cuya variabilidad podría introducir ruido en la interpretación de los fenómenos. Existen eventos en la naturaleza que pueden ser explicados con exactitud si se cuenta con los datos adecuados. Por ejemplo, el cálculo del volumen de un cubo o la predicción de la trayectoria de un objeto en caída libre bajo condiciones ideales. Los modelos que explican estos fenómenos se denominan modelos determinísticos.\nSin embargo, cuando se estudian fenómenos en sistemas complejos, la situación se vuelve más desafiante debido a la influencia de factores externos que impiden explicar completamente la variable dependiente a partir de otras variables. Para abordar esta incertidumbre, se emplean modelos que incorporan un componente de error, conocidos como modelos probabilísticos. Estos modelos constituyen la base de la estadística inferencial y se utilizan en el análisis de regresión para describir y predecir relaciones entre variables.\n\nEn los modelos determinísticos, se asume una relación exacta entre las variables. Esto significa que tanto los parámetros como las variables son conocidos sin error, y el modelo genera siempre el mismo resultado para un conjunto de datos dado. Sin embargo, estos modelos no consideran la variabilidad causada por el azar o la incertidumbre.\nUn ejemplo clásico de modelos determinísticos son los modelos SIR (Susceptibles, Infectados, Recuperados), empleados para estudiar la propagación de enfermedades en poblaciones cerradas. Estos modelos asumen tasas de transmisión y recuperación constantes, ignorando la variabilidad individual y otros factores externos.\n\nLos modelos probabilísticos asumen que los fenómenos observados no son completamente predecibles, ya que ciertas variables están sujetas al azar. Estos modelos no generan un único resultado, sino una distribución de resultados expresada en términos de probabilidades.\nUn modelo de regresión describe la relación entre la variable dependiente (\\(Y\\)) y una o más variables independientes (\\(X\\)), incorporando un componente sistemático (función del modelo) y un componente aleatorio (residuo o error residual).\nMatemáticamente, esto se representa como:\n\\[\nY = \\underbrace{\\beta_0 + \\beta_1 X_1}_{Componente \\: sistemático} + \\underbrace{\\epsilon}_{Componente \\: aleatorio}\n\\]\nLa función puede ser lineal o no lineal según la naturaleza y distribución de la variable dependiente. El componente aleatorio es esa parte de la variación de \\(Y\\) que no puede ser totalmente explicada por la variación de la/s variable/s independiente/s. En algunos casos el error tendrá valor positivo y en otros tendrá valor negativo. El promedio del error es igual a 0.\nUna vez establecida esta función estaremos en condiciones de:\n\nComprender cómo se comporta la variable respuesta \\(Y\\) en función de la/s variable/s independientes (\\(X\\)).\nEstimar o predecir el valor de \\(Y\\) para determinados valores de \\(X\\).\nCalcular intervalos de confianza para estas estimaciones o predicciones.\n\nLos modelos probabilísticos pueden clasificarse en:\n\nParamétricos: Asumen que los datos provienen de una distribución específica con un conjunto fijo de parámetros (por ejemplo, media, varianza).\nNo paramétricos: Se utilizan cuando la distribución subyacente de los datos es desconocida o no sigue un patrón estándar."
  },
  {
    "objectID": "unidad_3/02_mod_estadistico.html#modelo-lineal-general",
    "href": "unidad_3/02_mod_estadistico.html#modelo-lineal-general",
    "title": "Introducción al modelado estadístico",
    "section": "Modelo lineal general",
    "text": "Modelo lineal general\nEl modelo lineal general (MLG) describe la relación lineal entre una variable respuesta numérica con distribución normal (\\(Y\\)) y una o más variables independientes. Se expresa matemáticamente como:\n\\[\nY_i = X_i\\beta + \\epsilon_i\n\\]\nDonde:\n\n\\(Y_i\\): Vector de valores de la variable dependiente (también llamada variable respuesta o resultado), que representa el efecto observado de un proceso o interacción causal.\n\\(X_i\\): Matriz de diseño que contiene las variables independientes (variables explicativas o predictores), las cuales representan la influencia medida sobre la variable dependiente.\n\\(\\beta\\): Vector de coeficientes (parámetros), que cuantifican la magnitud y dirección de la influencia de cada predictor.\n\\(\\epsilon_i\\): Error aleatorio, que refleja la variabilidad no explicada por las variables independientes. Este componente puede deberse a diferencias individuales, errores de medición o calibración, y puede controlarse aumentando el tamaño muestral.\n\nCasos específicos del MLG\n\nRegresión lineal simple: Examina la relación entre \\(Y\\) y una variable independiente numérica.\nAnálisis de la varianza (ANOVA): Evalúa las diferencias en la media de \\(Y\\) respecto a los niveles de una variable categórica.\nRegresión lineal múltiple: Explora la relación entre \\(Y\\) y dos o más variables independientes, que pueden ser numéricas y/o categóricas.\n\n\n\n\n\n\n\nLa principal fuente del “error” se debe a la variabilidad entre individuos propia de la naturaleza (error aleatorio). Puede haber otras fuentes de error, incluso no detectadas y que deben ser tenidas en cuenta, como pueden ser errores en la medición, calibración o incluso por una mala elección del método.\n\n\n\nSupuestos del MLG:\nPara garantizar la validez del modelo, deben cumplirse los siguientes supuestos:\n\nLinealidad: Relación lineal entre la variable dependiente e independientes.\nIndependencia: Las observaciones deben ser independientes.\nHomoscedasticidad: La varianza de los errores debe ser constante.\nNormalidad de los errores: Los errores deben seguir una distribución normal.\n\nSi alguno de estos supuestos se viola, podría ser necesario aplicar transformaciones a los datos, emplear técnicas alternativas o ajustar el modelo según corresponda."
  },
  {
    "objectID": "unidad_3/02_mod_estadistico.html#construcción-del-mlg-en-r",
    "href": "unidad_3/02_mod_estadistico.html#construcción-del-mlg-en-r",
    "title": "Introducción al modelado estadístico",
    "section": "Construcción del MLG en R",
    "text": "Construcción del MLG en R\nPara ajustar un MLG en R, se utiliza la función lm() cuyas letras vienen de linear models (modelos lineales), ingresando los argumentos el formato fórmula. Esto significa especificar primero el nombre de la variable dependiente y luego la variable independiente.\nLa estructura sintáctica es:\n\nlm(variable_dependiente ~ variable_independiente)\n\nLa función muestra resultados básicos, tales como la relación entre las variables que son parte del modelo y los coeficientes. Habitualmente lm() suele asignarse a un objeto de regresión para que contenga todos los resultados producto del ajuste:\n\nmodelo &lt;- lm(variable_dependiente ~ variable_independiente, data = datos)\n\nTodos los ajustes de modelos lineales que produce la función lm() tienen la forma de una lista de 12 componentes. Sus componentes pueden ser llamados en resúmenes más completos, usando la función:\n\nsummary(modelo)\n\nLa salida de esta función incluye:\n\nCall: Fórmula del modelo.\nResiduals: Distribución de los residuos (mínimo, máximo, mediana y percentiles 25-75).\nCoefficients: Incluye el intercepto y la pendiente, junto con errores estándar, valores t y p-valores asociados.\nResidual standard error: Error estándar de los residuos con sus grados de libertad.\nMultiple R-squared: Coeficiente de determinación \\(R^2\\).\nAdjusted R-squared: Coeficiente \\(R^2\\) ajustado.\nF-statistic: Estadístico \\(F\\) y su p-valor asociado.\n\nAdemás, la salida de summary(modelo) utiliza asteriscos (*) para indicar la significación de los coeficientes. Debajo de la tabla de coeficientes se encuentra la referencia del significado de los códigos, que van desde el 0 hasta el 1 como posible resultado del valor de probabilidad, y donde:\n\n\n\n\nCódigo\nRango\n\n\n\n***\n0 a 0,001\n\n\n**\n0,001 a 0,01\n\n\n*\n0,01 a 0,05\n\n\n.\n0,05 a 0,1\n\n\n\n0,1 a 1\n\n\n\n\n\nLos elementos de la lista también pueden llamarse de forma independiente, siendo los más importantes:\n\n\ncoefficients: Vector con los coeficientes del modelo, pueden visualizarse usando alguno de los siguientes comandos:\n\nmodelo$coefficients\n\ncoef(modelo)\n\n\n\nresiduals: Los residuos o residuales para cada valor que surgen de la diferencia entre los valores predictivos calculados por el modelo y los valores reales. Se visualizan desde el objeto resultado de la regresión:\n\nmodelo$residuals\n\nO bien usando el comando:\n\nresid(modelo)\n\n\n\nfitted.values: Los valores calculados por el modelo en base a los datos existentes en la variable independiente. Los encontramos en:\n\nmodelo$fitted.values\n\nO usando la función:\n\nfitted(modelo)\n\n\n\nOtra función interesante para el análisis del objeto de regresión es confint() que calcula los intervalos de confianza de los coeficientes o parámetros del modelo de regresión. Para este modelo la línea de ejecución de la función es:\n\nconfint(modelo)\n\nSi agregamos la función round() podemos redondear los valores con la cantidad de decimales que necesitemos.\n\nround(confint(modelo),2)  \n\nEn forma predeterminada los IC se calculan al 95%, pero mediante el argumento level podemos modificarlo, por ejemplo al 99%:\n\nconfint(modelo, level = 0.99)\n\n\n\n\nAgresti (2015)\nTriola (2018)"
  },
  {
    "objectID": "unidad_3/04_reg_lineal.html",
    "href": "unidad_3/04_reg_lineal.html",
    "title": "Regresión lineal simple",
    "section": "",
    "text": "Para entender el modelo de regresión lineal simple, volvamos al ejemplo del peso en niñas menores de 6 meses de la unidad anterior, donde habíamos trazado una recta que nos ilustraba la relación lineal del peso en función de la edad.\nSegún el modelo estadístico para la función lineal de \\(Y\\) según \\(X\\):\n\\[\nY(X) =  \\beta_0 + \\beta_1 + X_1\n\\]\nHemos ajustado un modelo cuyos parámetros son:\n\\[\n\\hat{y} = b_0 + b_1X_1\n\\] donde \\(b_1\\) nos está indicando cuánto se modifica \\(\\hat{y}\\) por cada unidad de aumento de \\(X_1\\).\n\\[\n\\hat{y} = 2468,9 + 100,7 \\: edad \\: (en \\: semanas)\n\\]\nSe interpreta que por cada semana este grupo de lactantes ha aumentado en promedio 100 gramos, o que cada 1 mes (4 semanas) aumentan una media de 400 gramos.\nPodemos observar que hay una relación lineal y que esta relación no es perfecta. Existe cierta dispersión entre los puntos sugiriendo que alguna variación en el peso no se asocia con un incremento de la edad (por ejemplo dos lactantes de 15 semanas. Tienen la misma edad y 1.600 grs de diferencia. Cabría preguntarse si esas niñas que se “alejan” tanto de la recta de regresión no tienen algún antecedente distinto del resto). Más adelante veremos cómo se interpretan esas diferentes distancias entre las observaciones y la recta de regresión.\nAhora vamos a concentrarnos en la relación entre la varianza de la muestra, a través del desvío estándar (\\(ds = \\sqrt{varianza}\\)) y la magnitud de la asociación. Se muestran cuatro ejemplos en los cuáles se fue aumentando progresivamente el desvío estándar de los datos. Observen cómo a medida que aumenta la variabilidad entre los individuos va disminuyendo el coeficiente de correlación y el coeficiente \\(b_1\\) (pendiente de la recta)\n\n\n\n\n\n\n\n\nPodemos observar que cuanto mayor es la varianza en una muestra:\n\nMayor es la variabilidad de \\(y\\) en torno a la recta de regresión\nMayor es la imprecisión asociada a la estimativa de los parámetros de regresión"
  },
  {
    "objectID": "unidad_3/04_reg_lineal.html#introducción",
    "href": "unidad_3/04_reg_lineal.html#introducción",
    "title": "Regresión lineal simple",
    "section": "",
    "text": "Para entender el modelo de regresión lineal simple, volvamos al ejemplo del peso en niñas menores de 6 meses de la unidad anterior, donde habíamos trazado una recta que nos ilustraba la relación lineal del peso en función de la edad.\nSegún el modelo estadístico para la función lineal de \\(Y\\) según \\(X\\):\n\\[\nY(X) =  \\beta_0 + \\beta_1 + X_1\n\\]\nHemos ajustado un modelo cuyos parámetros son:\n\\[\n\\hat{y} = b_0 + b_1X_1\n\\] donde \\(b_1\\) nos está indicando cuánto se modifica \\(\\hat{y}\\) por cada unidad de aumento de \\(X_1\\).\n\\[\n\\hat{y} = 2468,9 + 100,7 \\: edad \\: (en \\: semanas)\n\\]\nSe interpreta que por cada semana este grupo de lactantes ha aumentado en promedio 100 gramos, o que cada 1 mes (4 semanas) aumentan una media de 400 gramos.\nPodemos observar que hay una relación lineal y que esta relación no es perfecta. Existe cierta dispersión entre los puntos sugiriendo que alguna variación en el peso no se asocia con un incremento de la edad (por ejemplo dos lactantes de 15 semanas. Tienen la misma edad y 1.600 grs de diferencia. Cabría preguntarse si esas niñas que se “alejan” tanto de la recta de regresión no tienen algún antecedente distinto del resto). Más adelante veremos cómo se interpretan esas diferentes distancias entre las observaciones y la recta de regresión.\nAhora vamos a concentrarnos en la relación entre la varianza de la muestra, a través del desvío estándar (\\(ds = \\sqrt{varianza}\\)) y la magnitud de la asociación. Se muestran cuatro ejemplos en los cuáles se fue aumentando progresivamente el desvío estándar de los datos. Observen cómo a medida que aumenta la variabilidad entre los individuos va disminuyendo el coeficiente de correlación y el coeficiente \\(b_1\\) (pendiente de la recta)\n\n\n\n\n\n\n\n\nPodemos observar que cuanto mayor es la varianza en una muestra:\n\nMayor es la variabilidad de \\(y\\) en torno a la recta de regresión\nMayor es la imprecisión asociada a la estimativa de los parámetros de regresión"
  },
  {
    "objectID": "unidad_3/04_reg_lineal.html#presupuestos-del-modelo",
    "href": "unidad_3/04_reg_lineal.html#presupuestos-del-modelo",
    "title": "Regresión lineal simple",
    "section": "Presupuestos del modelo",
    "text": "Presupuestos del modelo\nCuando planeamos realizar un análisis de regresión con un conjunto de datos es necesario saber que para que podamos plantearlo adecuadamente deben cumplirse ciertas condiciones, que llamaremos Presupuestos del modelo:\n\nIndependencia: los valores de \\(y\\) deben ser independientes unos de otros\nLinealidad: la relación entre \\(x\\) e \\(y\\) debe ser una función lineal\nHomocedasticidad: la varianza de \\(y\\) debe mantenerse constante para los distintos valores de \\(x\\)\n\nNormalidad: \\(y\\) debe tener una distribución normal\n\n¿Cómo se obtiene la recta de regresión? ¿Cómo se calculan los coeficientes de la regresión?\nEn el ejemplo del peso según edad en niñas menores de 6 meses, la idea es encontrar una función lineal (que gráficamente es una recta) que aplicada a los valores de \\(x\\) nos permita aproximar los valores de \\(y\\). La ecuación de la recta que describe la relación entre \\(x\\) e \\(y\\):\n\\[\n\\hat{y} = b_0 + b_1x\n\\]\nPor muy bueno que sea el modelo de regresión \\(y\\) e \\(\\hat{y}\\) rara vez coincidirán.\nEntonces podríamos pensar que la mejor recta que permita predecir (o aproximar) los valores de \\(y\\) en función de \\(x\\) es aquella que minimice estos errores residuales (que algunos serán en más y otros serán en menos).\nGráficamente:\n\n\n\n\n\n\n\n\ndonde:\n\n\\(\\hat{y}\\) es la ecuación de la “mejor” recta que puede trazarse entre estos puntos\n\\(b_0\\) es la ordenada al origen o constante, también llamada alfa. Es el punto donde la recta de regresión corta al eje de ordenadas.\n\\(b_1\\) es la pendiente de la recta (más adelante veremos cuál es la interpretación de estos coeficientes).\n\nConsideremos qué pasa en el caso de la niña 7. Veamos las distancias para este punto.\n\n\\(y_7\\) es el valor “real” del peso de la niña 7.\n\\(\\hat{y}_7\\) es el valor estimado de \\(y\\) que obtendremos a través de la regresión.\n\\(y – \\hat{y} = e\\) (residuo o error residual) es el desvío de \\(y\\) del valor ajustado \\(\\hat{y}\\) en la ecuación de la regresión estimada.\n\nPara poder operar con el valor de estos errores (ya que algunos tendrán valor positivo y otros valor negativo) se los eleva al cuadrado. Esta técnica se denomina “método de los mínimos cuadrados” y consiste en adoptar como estimativas de los parámetros de la regresión (o sea los coeficientes \\(b_0\\) y \\(b_1\\) y por ende la recta de regresión) los valores que minimizan la suma de los cuadrados de los residuos o error (SCE) para todas las observaciones de \\(y\\). Lo podemos expresar así:\n\\[\nSCE = \\sum{\\hat{e}^2} = \\sum{(y-\\hat{y})^2}\n\\]\nSabíamos que:\n\\[\n\\hat{y} = b_0 + b_1x\n\\]\nEntonces si reemplazamos:\n\\[\nSCE = \\sum_{i=1}^{i=n} (y_i-\\hat{y}_i)^2 =  \\sum_{i=1}^{i=n}(y_i-(\\hat{\\beta}_0 + \\hat{\\beta}_1x_1))^2\n\\]\nEs posible obtener los estimadores \\(\\beta_1\\) y \\(\\beta_0\\).\n\\[\n\\hat{\\beta}_1 = \\frac{\\sum_{i=1}^{n}(x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^{n}(x_i-\\bar{x})^2} = \\frac{S_{xy}}{S_{xx}}\n\\]\nOtra fórmula para \\(\\beta_1\\):\n\\[\n\\hat{\\beta}_1 = \\frac{\\sum x_iy_i-\\frac{\\sum x_i \\sum y_i}{n}}{\\sum x_i^2 - \\frac{(\\sum x_i)^2}{n}}\n\\]\n\\[\n\\hat{\\beta}_0 = \\bar{y} - \\beta_1\\bar{x}\n\\]\nEl método de los mínimos cuadrados fue creado por Johann Carl Friedrich Gauss (1777-1855). Tiene además la ventaja que el promedio de los errores residuales = 0 y que para cada estimación, la varianza del error es mínima.\nTest de hipótesis para \\(\\beta_1\\) e Intervalo de Confianza 95%\nComo siempre que trabajamos con una muestra, será necesario aplicar los procesos de inferencia. Es por eso que los softwares ofrecen un test de hipótesis para el coeficiente.\nLa hipótesis nula podría entenderse como que \\(x\\) no logra explicar la variación de \\(y\\) (entonces la pendiente de la recta sería nula)\n\\[\nH_0: \\beta_1 = 0\n\\]\nAl calcular el modelo de regresión, todos los software estiman el coeficiente y el error estándar del mismo (\\(SE\\)) y testean el coeficiente.\nBondad de ajuste\nHasta ahora hemos aprendido a explicar la variación de \\(y\\) según la variación de \\(x\\) mediante un modelo en donde las desviaciones entre el valor observado (“real”) y el estimado (“modelo de regresión”) son las menores posibles.\nAhora debemos saber, según los datos que tenemos, cuán bueno es el modelo que ajustamos (qué capacidad tiene de explicar la variabilidad de \\(y\\) o, si lo quiero utilizar para realizar una predicción, cuánto se alejará mi valor estimado del verdadero, “real” valor de \\(y\\)). Esta evaluación la realizaremos mediante la descomposición de la varianza del modelo. Por definición la varianza o variabilidad total es la sumatoria de la diferencia entre cada valor de \\(y\\) con el promedio de \\(y\\) (elevado al cuadrado ya que hay valores negativos y positivos que si los sumamos se anularían).\nLa variabilidad total del modelo es la suma entre la variabilidad que logró explicar la regresión y la variabilidad residual.\n\\[\n\\sum (y_i-\\bar{y})^2 = \\sum (\\hat{y}-\\bar{y})^2 + \\sum (y_i -\\hat{y}_i)^2\n\\]\n\\[\n\\frac{Suma \\; de \\; cuadrados}{totales \\; (SCT)} \\; \\frac{Suma \\; de \\; cuadrados}{de \\; la \\; regresion \\; (SCR)} \\; \\frac{Suma \\; de \\; cuadrados}{residuales \\; (SCE)}\n\\]\nCuanto mayor sea la variabilidad que logre explicar la regresión en relación a los residuos, tanto mejor será el modelo. Este es el fundamento para el cálculo del coeficiente de determinación (\\(R^2\\))\n\\[\nR^2 = \\frac{SCR}{SCT} = 1 - \\frac{SCE}{SCT}\n\\]\n\\(R^2\\) expresa la proporción de la variación total que logra explicar el modelo de regresión. Su valor oscila entre 0 y 1, es una cantidad adimensional.\n\nCuando el ajuste es bueno \\(R^2\\) será cercano a 1, cuando el ajuste es malo \\(R^2\\) será cercano a 0.\nEn la Regresión lineal simple el coeficiente de determinación (\\(R^2\\)) es igual al \\(r\\) de Pearson elevado al cuadrado.\n\nPara visualizar simulaciones al respecto pueden visitar el siguiente enlace:\n➡️Viendo la teoría. Una introducción visual a probabilidad y estadística"
  },
  {
    "objectID": "unidad_3/04_reg_lineal.html#ejemplo-práctico-en-r",
    "href": "unidad_3/04_reg_lineal.html#ejemplo-práctico-en-r",
    "title": "Regresión lineal simple",
    "section": "Ejemplo práctico en R",
    "text": "Ejemplo práctico en R\nPara llevar a cabo el análisis en R y presentar las funciones y paquetes que nos pueden ayudar en la tarea vamos a trabajar con el set de datos cancer_USA.txt que utilizamos para el ejemplo de covarianza y correlación.\nComenzaremos por cargar los paquetes necesarios para el análisis:\n\n# Tablas de coeficientes\nlibrary(gtsummary)\n\n# Chequeo de supuestos\nlibrary(easystats)\nlibrary(lmtest) \nlibrary(nortest)\n\n# Paletas aptas daltonismo\nlibrary(scico)\n\n# Manejo de datos\nlibrary(tidyverse) \n\nCargamos los datos y revisamos la estructura de la tabla:\n\n# Carga datos\ndatos &lt;- read_csv2(\"datos/cancer_USA.txt\")\n\n# Explora datos\nglimpse(datos)\n\nRows: 213\nColumns: 10\n$ condado            &lt;chr&gt; \"Belknap County\", \"Carroll County\", \"Cheshire Count…\n$ estado             &lt;chr&gt; \"New Hampshire\", \"New Hampshire\", \"New Hampshire\", …\n$ tasa_mortalidad    &lt;dbl&gt; 182.6, 168.8, 162.8, 181.6, 155.0, 163.2, 173.1, 17…\n$ mediana_edad       &lt;dbl&gt; 46.1, 50.3, 42.0, 48.1, 41.9, 40.1, 42.6, 43.5, 37.…\n$ mediana_edad_cat   &lt;chr&gt; \"45+ años\", \"45+ años\", \"36-39 años\", \"45+ años\", \"…\n$ mediana_ingresos   &lt;dbl&gt; 59831, 57556, 56008, 42491, 56353, 71233, 62429, 79…\n$ pct_pobreza        &lt;dbl&gt; 10.0, 10.7, 11.8, 14.9, 11.6, 8.7, 9.5, 6.1, 11.8, …\n$ pct_salud_publica  &lt;dbl&gt; 16.8, 13.4, 12.7, 22.5, 14.0, 12.7, 13.1, 9.0, 12.4…\n$ pct_sec_incompleta &lt;dbl&gt; 15.4, 14.8, 6.1, 13.2, 6.6, 12.9, 10.9, 13.5, 6.1, …\n$ pct_desempleo      &lt;dbl&gt; 5.3, 5.8, 6.1, 6.9, 5.0, 5.9, 5.2, 5.6, 6.5, 5.8, 1…\n\n\nRecordemos que la base de datos tiene 213 observaciones y 10 variables y la variable dependiente es tasa_mortalidad. Para ejemplificar los pasos de una regresión lineal simple, evaluaremos la asociación entre la variable dependiente y mediana_ingresos.\nPresupuestos\nPara que un modelo lineal sea válido, debe cumplir con cuatro supuestos fundamentales: independencia, linealidad, homocedasticidad y normalidad. Aunque la verificación rigurosa de estos criterios suele realizarse a partir del análisis de residuos tras ajustar el modelo, es recomendable realizar una evaluación preliminar de los datos para identificar posibles problemas desde el inicio.\nLa independencia (o ausencia de autocorrelación) puede determinarse, en gran medida, a partir del conocimiento sobre la fuente de los datos y su método de recolección. Sin embargo, siempre es recomendable verificarla en los residuales del modelo.\nLa linealidad se refiere a la relación entre las variables tasa_mortalidad y mediana_ingresos. Para comprobarla, se puede utilizar un diagrama de dispersión:\n\ndatos |&gt; \n  \n  ggplot(mapping = aes(x = mediana_ingresos, y = tasa_mortalidad)) +\n  \n  # gráfico de dispersión\n  geom_point(color = \"#502345\") +\n  \n  # tema\n  theme_minimal()\n\n\n\n\n\n\n\nObservamos en el gráfico una clara relación inversa entre las variables, dado que los condados donde las personas tienen mayores ingresos la mortalidad por cáncer es menor y viceversa. Podemos dibujar la recta de regresión lineal sobre el diagrama de dispersión adicionando una capa más al gráfico mediante geom_smooth() e indicando method = \"lm\" como método. Además de la recta se puede ver el intervalo de confianza (zona gris alrededor de ella).\n\ndatos |&gt;  \n  ggplot(mapping = aes(x = mediana_ingresos, y = tasa_mortalidad)) +\n  \n # diagrama de dispersión\n   geom_point(color = \"#502345\") +\n  \n  # añade línea de regresión\n  geom_smooth(method = \"lm\", color = \"#1B0D33\") + \n  \n  # cambia color de fondo\n  theme_minimal()\n\n\n\n\n\n\n\nUsaremos la función cor() para estimar la correlación entre las dos variables:\n\ncor(datos$mediana_ingresos, datos$tasa_mortalidad,\n    method = \"pearson\")\n\n[1] -0.5041398\n\n\nEl valor es negativo, lo que confirma lo observado en la nube de puntos anterior. Para poder descartar que esta correlación negativa se debe al azar, debemos calcular su significancia:\n\ncor.test(datos$mediana_ingresos, datos$tasa_mortalidad)\n\n\n    Pearson's product-moment correlation\n\ndata:  datos$mediana_ingresos and datos$tasa_mortalidad\nt = -8.4795, df = 211, p-value = 3.938e-15\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n -0.5980408 -0.3965857\nsample estimates:\n       cor \n-0.5041398 \n\n\nEn este ejemplo \\(p&lt;0,05\\), por lo tanto la correlación es significativa.\nLa homocedasticidad implica que la varianza de los residuos se mantiene aproximadamente constante a lo largo de los valores de la variable independiente. Se puede evaluar gráficamente mediante la dispersión de los residuos respecto a los valores ajustados o a través de contrastes de hipótesis, como el test de Breusch-Pagan.\nFinalmente, la normalidad de los residuos se puede evaluar mediante el test de Lilliefors, disponible en el paquete nortest (Gross y Ligges 2015):\n\nlillie.test(datos$tasa_mortalidad)\n\n\n    Lilliefors (Kolmogorov-Smirnov) normality test\n\ndata:  datos$tasa_mortalidad\nD = 0.042152, p-value = 0.4707\n\n\nTambién se puede verificar gráficamente con un QQ plot:\n\ndatos |&gt; \n  ggplot(mapping = aes(sample = tasa_mortalidad)) +\n  \n  # añade qqplot\n  stat_qq() +\n  stat_qq_line() +\n  \n  # cambia nombres de los ejes X e Y\n  labs(title = \"QQplot\", \n       x = \"Theoretical Quantiles\", \n       y = \"Sample Quantiles\") +\n  \n  # modifico color de fondo\n  theme_minimal()\n\n\n\n\n\n\n\nTanto el test de hipótesis como los gráficos de cuantiles nos informan que las distribuciones de la variable dependiente cumple con el criterio de “normalidad”.\nAjuste del modelo\nComo vimos anteriormente, la función lm() del paquete stats nos permite ajustar modelos de regresión lineal:\n\nlm(tasa_mortalidad ~ mediana_ingresos, data = datos)\n\n\nCall:\nlm(formula = tasa_mortalidad ~ mediana_ingresos, data = datos)\n\nCoefficients:\n     (Intercept)  mediana_ingresos  \n       201.41284          -0.00052  \n\n\nEn este caso, Intercept representa el valor de mediana_ingresos cuando tasa_mortalidad vale cero (ordenada al origen) y el coeficiente de mediana_ingresos representa la pendiente de la recta.\nEstos resultados obtenidos y aplicados en la fórmula del modelo simple quedarían así:\n\\[\n\\operatorname{tasa mortalidad} = \\alpha + \\beta_{1}(\\operatorname{mediana ingresos}) + \\epsilon\n\\]\n\\[\n\\operatorname{tasa mortalidad} = 201.4128 + -0.0005*\\operatorname{mediana ingresos} + \\epsilon\n\\]\nGuardamos el modelo como un objeto:\n\nmodelo &lt;- lm(tasa_mortalidad ~ mediana_ingresos, data = datos)\n\nPodemos acceder a la salida del modelo mediante la función summary():\n\nsummary(modelo)\n\n\nCall:\nlm(formula = tasa_mortalidad ~ mediana_ingresos, data = datos)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-33.040  -8.210   1.093   7.637  35.932 \n\nCoefficients:\n                   Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       2.014e+02  3.521e+00  57.200  &lt; 2e-16 ***\nmediana_ingresos -5.200e-04  6.133e-05  -8.479 3.94e-15 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 12.32 on 211 degrees of freedom\nMultiple R-squared:  0.2542,    Adjusted R-squared:  0.2506 \nF-statistic:  71.9 on 1 and 211 DF,  p-value: 3.938e-15\n\n\nLa asociación entre la tasa de mortalidad por cáncer y la mediana de ingresos es estadísticamente significativa (\\(p &lt; 0.001\\)).\nTodos los ajustes de modelos lineales que produce la función lm() tienen la forma de una lista.La manera de conocer su clase es class() y su estructura mediante str()\n\nclass(modelo)\n\n[1] \"lm\"\n\nstr(modelo)\n\nList of 12\n $ coefficients : Named num [1:2] 201.41284 -0.00052\n  ..- attr(*, \"names\")= chr [1:2] \"(Intercept)\" \"mediana_ingresos\"\n $ residuals    : Named num [1:213] 12.3 -2.68 -9.49 2.28 -17.11 ...\n  ..- attr(*, \"names\")= chr [1:213] \"1\" \"2\" \"3\" \"4\" ...\n $ effects      : Named num [1:213] -2516.47 104.46 -10.27 1.26 -17.89 ...\n  ..- attr(*, \"names\")= chr [1:213] \"(Intercept)\" \"mediana_ingresos\" \"\" \"\" ...\n $ rank         : int 2\n $ fitted.values: Named num [1:213] 170 171 172 179 172 ...\n  ..- attr(*, \"names\")= chr [1:213] \"1\" \"2\" \"3\" \"4\" ...\n $ assign       : int [1:2] 0 1\n $ qr           :List of 5\n  ..$ qr   : num [1:213, 1:2] -14.5945 0.0685 0.0685 0.0685 0.0685 ...\n  .. ..- attr(*, \"dimnames\")=List of 2\n  .. .. ..$ : chr [1:213] \"1\" \"2\" \"3\" \"4\" ...\n  .. .. ..$ : chr [1:2] \"(Intercept)\" \"mediana_ingresos\"\n  .. ..- attr(*, \"assign\")= int [1:2] 0 1\n  ..$ qraux: num [1:2] 1.07 1.01\n  ..$ pivot: int [1:2] 1 2\n  ..$ tol  : num 1e-07\n  ..$ rank : int 2\n  ..- attr(*, \"class\")= chr \"qr\"\n $ df.residual  : int 211\n $ xlevels      : Named list()\n $ call         : language lm(formula = tasa_mortalidad ~ mediana_ingresos, data = datos)\n $ terms        :Classes 'terms', 'formula'  language tasa_mortalidad ~ mediana_ingresos\n  .. ..- attr(*, \"variables\")= language list(tasa_mortalidad, mediana_ingresos)\n  .. ..- attr(*, \"factors\")= int [1:2, 1] 0 1\n  .. .. ..- attr(*, \"dimnames\")=List of 2\n  .. .. .. ..$ : chr [1:2] \"tasa_mortalidad\" \"mediana_ingresos\"\n  .. .. .. ..$ : chr \"mediana_ingresos\"\n  .. ..- attr(*, \"term.labels\")= chr \"mediana_ingresos\"\n  .. ..- attr(*, \"order\")= int 1\n  .. ..- attr(*, \"intercept\")= int 1\n  .. ..- attr(*, \"response\")= int 1\n  .. ..- attr(*, \".Environment\")=&lt;environment: R_GlobalEnv&gt; \n  .. ..- attr(*, \"predvars\")= language list(tasa_mortalidad, mediana_ingresos)\n  .. ..- attr(*, \"dataClasses\")= Named chr [1:2] \"numeric\" \"numeric\"\n  .. .. ..- attr(*, \"names\")= chr [1:2] \"tasa_mortalidad\" \"mediana_ingresos\"\n $ model        :'data.frame':  213 obs. of  2 variables:\n  ..$ tasa_mortalidad : num [1:213] 183 169 163 182 155 ...\n  ..$ mediana_ingresos: num [1:213] 59831 57556 56008 42491 56353 ...\n  ..- attr(*, \"terms\")=Classes 'terms', 'formula'  language tasa_mortalidad ~ mediana_ingresos\n  .. .. ..- attr(*, \"variables\")= language list(tasa_mortalidad, mediana_ingresos)\n  .. .. ..- attr(*, \"factors\")= int [1:2, 1] 0 1\n  .. .. .. ..- attr(*, \"dimnames\")=List of 2\n  .. .. .. .. ..$ : chr [1:2] \"tasa_mortalidad\" \"mediana_ingresos\"\n  .. .. .. .. ..$ : chr \"mediana_ingresos\"\n  .. .. ..- attr(*, \"term.labels\")= chr \"mediana_ingresos\"\n  .. .. ..- attr(*, \"order\")= int 1\n  .. .. ..- attr(*, \"intercept\")= int 1\n  .. .. ..- attr(*, \"response\")= int 1\n  .. .. ..- attr(*, \".Environment\")=&lt;environment: R_GlobalEnv&gt; \n  .. .. ..- attr(*, \"predvars\")= language list(tasa_mortalidad, mediana_ingresos)\n  .. .. ..- attr(*, \"dataClasses\")= Named chr [1:2] \"numeric\" \"numeric\"\n  .. .. .. ..- attr(*, \"names\")= chr [1:2] \"tasa_mortalidad\" \"mediana_ingresos\"\n - attr(*, \"class\")= chr \"lm\"\n\n\nLa función tbl_regression() del paquete gtsummary (Sjoberg et al. 2021), nos permite generar una tabla con los coeficientes del modelo. El nivel de confianza puede ajustarse con el argumento conf.level y el argumento intercept muestra u oculta el intercepto:\n\ntbl_regression(modelo, \n               intercept = T, \n               conf.level = .95)\n\n\n\n\n\nCharacteristic\nBeta\n95% CI\np-value\n\n\n\n(Intercept)\n201\n194, 208\n&lt;0.001\n\n\nmediana_ingresos\n0.00\n0.00, 0.00\n&lt;0.001\n\n\n\nAbbreviation: CI = Confidence Interval\n\n\n\n\n\nResiduales\nEl residuo o residual de una estimación se define como la diferencia entre el valor observado y el valor predicho por el modelo de regresión. Son fundamentales para evaluar la bondad de ajuste y verificar los supuestos básicos de los modelos lineales. Para resumir el conjunto de residuales, se pueden emplear dos enfoques:\n\nLa sumatoria del valor absoluto de cada residual.\nLa sumatoria del cuadrado de cada residual (RSS, Residual Sum of Squares), basada en el método de los mínimos cuadrados, amplifica las desviaciones extremas y permite minimizar errores en la estimación de parámetros.\n\nLos residuales se almacenan en el objeto de regresión y pueden visualizarse mediante:\n\nresid(modelo)\n\nCuanto mayor sea la sumatoria de los cuadrados de los residuales, menor será la precisión con la que el modelo predice el valor de la variable dependiente a partir de la variable predictora.\nUn análisis gráfico resulta muy útil para evaluar los supuestos del modelo. Aplicando la función plot() al objeto de regresión:\n\npar(mfrow = c(2,2))\nplot(modelo)\n\n\n\n\n\n\n\nEsta función genera cuatro gráficos automáticos:\n\nResiduals vs Fitted: Permite evaluar la linealidad. La línea roja debería ser lo más horizontal posible y sin curvaturas pronunciadas. Si hay curvatura, podría indicar la necesidad de un término no lineal (cuadrático, logarítmico, etc.) o la omisión de una variable importante en el modelo.\nNormal Q-Q: Evalúa la normalidad de los residuos. Los puntos deberían ajustarse a la diagonal. Desviaciones importantes sugieren un incumplimiento del supuesto de normalidad.\nScale-Location: Indica si los residuos se distribuyen uniformemente a lo largo del rango de los predictores, verificando la homocedasticidad. Una línea aproximadamente horizontal con puntos dispersos de manera aleatoria es una buena señal.\nResiduals vs Leverage: Ayuda a identificar valores influyentes. Un punto extremo no necesariamente es influyente, pero aquellos fuera de las líneas rojas punteadas (altas puntuaciones de distancia de Cook) pueden estar afectando considerablemente el modelo.\n\nOtra opción para evaluar gráficamente los supuestos del modelo es la función check_model() del paquete performance , el cual forma parte del ecosistema easystats (Lüdecke et al. 2022):\n\ncheck_model(modelo)\n\n\n\n\n\n\n\nPodemos elegir que gráficos visualizar usando el argumento check:\n\ncheck_model(modelo, check = c(\"normality\",\"qq\", \"linearity\",\n                              \"homogeneity\", \"outliers\"))\n\n\n\n\n\n\n\nPara evaluar la linealidad se puede aplicar el test RESET de Ramsey, mediante la función resettest() del paquete lmtest (Zeileis y Hothorn 2002a):\n\nresettest(modelo)\n\n\n    RESET test\n\ndata:  modelo\nRESET = 0.82653, df1 = 2, df2 = 209, p-value = 0.439\n\n\nLa hipótesis nula indica que las variables se relacionan de forma lineal; un \\(p&lt;0,05\\) sugiere lo contrario. En nuestro modelo, el valor p de 0.439 respalda la suposición de linealidad.\nOtro supuesto fundamental es que los residuales deben distribuirse de forma normal (con media 0). Además del QQ-plot, se puede aplicar el test de Lilliefors:\n\nlillie.test(modelo$residuals)\n\n\n    Lilliefors (Kolmogorov-Smirnov) normality test\n\ndata:  modelo$residuals\nD = 0.05804, p-value = 0.07821\n\n\nCon un p-valor de 0.078 se confirma la normalidad.\nEl test de Breusch-Pagan, implementado en lmtest y performance, evalúa si la varianza de los residuos es constante. Parte de la hipótesis nula de homocedasticidad o varianza constante en las perturbaciones y la enfrenta a la alternativa de varianza variable, por lo que es válido decir que cumple con el supuesto de homocedasticidad si el valor \\(p&gt;0,05\\):\n\n# paquete lmtest\nbptest(modelo)\n\n\n    studentized Breusch-Pagan test\n\ndata:  modelo\nBP = 2.5078, df = 1, p-value = 0.1133\n\n# paquete performance\ncheck_heteroscedasticity(modelo)\n\nOK: Error variance appears to be homoscedastic (p = 0.087).\n\n\nIncluso cuando el modelo cumple todos los supuestos, es importante identificar observaciones atípicas o de alto leverage, ya que podrían condicionar el ajuste. Los valores atípicos u outliers son observaciones que no se ajustan bien al modelo, con residuos excesivamente grandes. Por otro lado, los puntos con alto leverage tienen valores extremos en los predictores, lo que los hace potencialmente influyentes.\nPara ello se pueden analizar los residuales estandarizados y los residuales estudentizados. Los residuales estandarizados se obtienen normalizando los residuales por su desviación estándar. Esta aproximación permite identificar valores atípicos que se alejan más de \\(\\pm3\\) desviaciones estándar. Sin embargo, si un outlier influye lo suficiente en el modelo como para atraer la línea de regresión, su residual podría ser pequeño y pasar desapercibido. Una alternativa más robusta es utilizar los residuales estudentizados. Se trata de un proceso iterativo en el que se va excluyendo cada vez una observación \\(i\\) distinta y se reajusta el modelo con las \\(n-1\\) restantes. En cada proceso de exclusión y reajuste se calcula la diferencia (\\(d_i\\)) entre el valor predicho para \\(i\\) habiendo y sin haber excluido esa observación. Finalmente, se normalizan las diferencias \\(d_i\\) . Aquellos valores \\(d_i\\) cuyo residual estudentizado supera \\(\\pm 3\\) suelen considerarse significativos.\nEstos dos procesos sobre los residuales se pueden calcular en R mediante las funciones rstandar() y rstudent():\n\n# Creación del dataset de residuales\nresiduales &lt;- tibble(\n  residuales = residuals(modelo),\n  resid_normalizados = rstandard(modelo),\n  resid_estudentizados = rstudent(modelo)\n)\n\nPodemos visualizar la comparación de estos residuales mediante un boxplot usando facets:\n\nresiduales |&gt; \n  \n  # Pasa a formato long\n  pivot_longer(cols = everything(),\n               names_to = \"tipo\") |&gt; \n  \n  # Genera gráfico\n  ggplot(mapping = aes(y = value, fill = tipo)) +\n  \n  geom_boxplot() +\n  \n  scale_fill_scico_d() +\n  \n  facet_wrap(~ tipo, scales = \"free_y\") + # subdivide en facets\n  \n  theme_minimal()\n\n\n\n\n\n\n\nSi encontramos residuales con valores absolutos superiores a 3 en el gráfico de residuales estudentizados, es recomendable investigarlos más a fondo. Podemos identificarlos con:\n\ntable(rstudent(modelo) &gt; 3)\n\n\nFALSE \n  213 \n\n\nPara evaluar la influencia de observaciones, se utiliza la distancia de Cook, que combina la magnitud del residual y el leverage. Valores superiores a 1 se consideran generalmente influyentes. Esto se puede visualizar mediante:\n\nplot(modelo, which = 5)\n\n\n\n\n\n\n\nAl observar el gráfico, se debe prestar especial atención a los puntos situados en las esquinas superior e inferior, es decir, aquellos que aparecen fuera de las líneas discontinuas rojas. Estos puntos suelen presentar altas puntuaciones en la distancia de Cook, lo que indica que pueden influir significativamente en los resultados de la regresión.\nAdemás, el paquete performance incluye la función check_outliers() para detectar valores atípicos según su distancia de Cook:\n\ncheck_outliers(modelo)\n\nOK: No outliers detected.\n- Based on the following method and threshold: cook (0.7).\n- For variable: (Whole model)\n\n\nEn el ejemplo analizado, no se identificaron valores influyentes evidentes. Sin embargo, si se detecta alguna observación fuera de estos límites, es recomendable estudiarla de forma individual para determinar, por ejemplo, si su elevada influencia se debe a un error de registro. En tal caso podremos eliminar la observación (o corregirla) y analizar los casos restantes. Pero si el dato es correcto, quizás sea diferente de las otras observaciones y encontrar las causas de este fenómeno puede llegar a ser la parte más interesante del análisis. Por supuesto que todo esto dependerá del contexto del problema que uno esta estudiando.\nBondad de ajuste del modelo\nUna vez ajustado un modelo, es necesario verificar su eficiencia, ya que, aun siendo la línea que mejor se ajusta a las observaciones entre todas las posibles, el modelo puede resultar inadecuado. Entre las medidas más utilizadas para evaluar la calidad del ajuste se encuentran el error estándar de los residuales, el test \\(F\\) y el coeficiente de determinación \\(R^2\\). Estos valores aparecen al final de la salida de la función summary(), donde se pueden identificar el RSE (Residual Standard Error), el \\(R^2\\) (Multiple R-squared) y el \\(R^2\\) ajustado (Adjusted R-squared).\nPodemos acceder al valor de \\(R^2\\) del modelo usando la función r2() del paquete performance:\n\nr2(modelo)\n\n# R2 for Linear Regression\n       R2: 0.254\n  adj. R2: 0.251\n\n\nEl \\(R^2\\) oscila entre 0 y 1, de modo que valores cercanos a 1 indican un buen ajuste del modelo lineal a los datos. Por otro lado, el \\(R^2\\) ajustado penaliza la inclusión de variables independientes poco relevantes en la explicación de la variable dependiente, razón por la cual su valor es menor o igual al \\(R^2\\). En nuestro ejemplo los valores de \\(R^2\\) obtenidos nos indican que el modelo lineal simple no se ajusta demasiado bien a los datos. Esto significa que solamente un 25.1 % de la variación en la tasa de mortalidad por cáncer se explica únicamente por la mediana de ingresos como variable explicativa; el restante 74.9 % no se explica, lo que sugiere que agregar otras variables independientes podría mejorar el ajuste y, por ende, la confiabilidad de las predicciones.\nLa salida de summary() también incluye un estadístico \\(F\\) de Snedecor y su \\(p\\)-valor correspondiente, que se utilizan en el contraste ómnibus para evaluar, de forma global, la idoneidad del modelo. En nuestro ejemplo, \\(p&gt;0,05\\), por lo que, al 95 % de confianza, se rechaza la hipótesis nula y se concluye que el modelo lineal es adecuado para el conjunto de datos. Cabe mencionar que, cuando el modelo de regresión tiene una única variable explicativa, este contraste es equivalente al contraste del parámetro \\(\\beta_1​\\).\nOtra forma de verificar de manera independiente la significación del modelo es mediante la función anova(), que plantea el contraste de la regresión mediante el análisis de la varianza:\n\nanova(modelo)\n\nAnalysis of Variance Table\n\nResponse: tasa_mortalidad\n                  Df Sum Sq Mean Sq F value    Pr(&gt;F)    \nmediana_ingresos   1  10912 10911.8  71.901 3.938e-15 ***\nResiduals        211  32021   151.8                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nLa tabla de análisis de varianza muestra resultados coherentes con el bloque final de summary(modelo), presentando un valor \\(F\\) de 71.9 y un \\(p\\)-valor significativo.\nInterpretación de resultados\nSe realizó una regresión lineal simple para analizar la relación entre la mediana de ingresos y la tasa de mortalidad por cáncer en 174 condados de la Costa Este de Estados Unidos. El diagrama de dispersión mostró una relación lineal negativa moderada entre ambas variables, lo que se confirmó con un coeficiente de correlación de Pearson de\n-0.5. Además, el análisis de regresión reveló que la relación es estadísticamente significativa (t = -8.48, p = 0).\nEl coeficiente de pendiente para la mediana de ingresos fue de\n0, lo que indica que, por cada unidad de incremento en la mediana de ingresos, la tasa de mortalidad por cáncer disminuye aproximadamente en 0,05 puntos porcentuales. El \\(R^2\\) del modelo muestra que el 25.1 % de la variación en la tasa de mortalidad se explica únicamente con esta variable, lo que sugiere la existencia de otros factores que también influyen en la mortalidad por cáncer.\nFinalmente, la gráfica de dispersión de los valores pronosticados estandarizados frente a los residuos estandarizados evidenció que se cumplen los supuestos de homogeneidad de varianza y linealidad, y que los residuos se distribuyen de forma aproximadamente normal. Se cumplieron todos los presupuestos necesarios para validar la regresión y no se encontraron valores atípicos influyentes.\n\nBallester Díez y Tenías Burillo (2003)\nDaniel (2002)\nEscuela Nacional de Sanidad (ENS). Instituto de Salud Carlos III. Ministerio de Ciencias e Innovación. Madrid (2009)\nHernández-Ávila (2011)\nWeisberg (2005)\nZeileis y Hothorn (2002b)\n (2025)\nWickham et al. (2019)\nSchloerke et al. (2024)"
  },
  {
    "objectID": "unidad_3/06_confusion_interaccion.html",
    "href": "unidad_3/06_confusion_interaccion.html",
    "title": "Confusión e interacción",
    "section": "",
    "text": "Antes de comenzar a construir modelos de regresión lineal múltiple, es necesario repasar algunos conceptos fundamentales…\nLos estudios epidemiológicos suelen partir de modelos teóricos conocidos y vinculados con el problema de investigación. Las variables recolectadas se definen en la etapa de diseño del estudio, cada una cumpliendo un rol específico. Generalmente, se identifican dos variables principales y excluyentes: la exposición (variable independiente) y el resultado (variable dependiente). Una vez seleccionadas la variable dependiente e independiente, aas demás variables medidas o no medidas en el estudio se denominan covariables.\nDentro del proceso salud-enfermedad, las covariables pueden asumir diversos roles, como confusoras, mediadoras de efecto, intermedias, colisionadoras, exposiciones en competencia, entre otros. Algunos de estos roles suelen estar definidos previamente por la literatura; otros pueden surgir o ser identificados durante el análisis."
  },
  {
    "objectID": "unidad_3/06_confusion_interaccion.html#introducción",
    "href": "unidad_3/06_confusion_interaccion.html#introducción",
    "title": "Confusión e interacción",
    "section": "",
    "text": "Antes de comenzar a construir modelos de regresión lineal múltiple, es necesario repasar algunos conceptos fundamentales…\nLos estudios epidemiológicos suelen partir de modelos teóricos conocidos y vinculados con el problema de investigación. Las variables recolectadas se definen en la etapa de diseño del estudio, cada una cumpliendo un rol específico. Generalmente, se identifican dos variables principales y excluyentes: la exposición (variable independiente) y el resultado (variable dependiente). Una vez seleccionadas la variable dependiente e independiente, aas demás variables medidas o no medidas en el estudio se denominan covariables.\nDentro del proceso salud-enfermedad, las covariables pueden asumir diversos roles, como confusoras, mediadoras de efecto, intermedias, colisionadoras, exposiciones en competencia, entre otros. Algunos de estos roles suelen estar definidos previamente por la literatura; otros pueden surgir o ser identificados durante el análisis."
  },
  {
    "objectID": "unidad_3/06_confusion_interaccion.html#confusión",
    "href": "unidad_3/06_confusion_interaccion.html#confusión",
    "title": "Confusión e interacción",
    "section": "Confusión",
    "text": "Confusión\nUna variable de confusión distorsiona la medida de asociación entre las variables principales del estudio. En presencia de confusión, pueden observarse los siguientes escenarios:\n\nAsociación espuria: Efecto observado donde en realidad no existe.\nConfusión positiva: Exageración o atenuación de una asociación real.\nConfusión negativa: Inversión del sentido de una asociación real.\n\nSegún Gordis (2017), en un estudio que evalúa si la exposición (\\(X\\)) causa un resultado (\\(Y\\)), se dice que un tercer factor (\\(Z\\)) es una variable de confusión si cumple con los siguientes criterios:\n\n\\(Z\\) es un factor de riesgo conocido para \\(Y\\).\n\\(Z\\) está asociado con la exposición \\(X\\), pero no es un resultado de \\(X\\).\n\nPara conceptualizar este y otros mecanismos, es útil utilizar gráficos acíclicos dirigidos (DAGs, por sus siglas en inglés). Estos diagramas representan las relaciones causales sin formar ciclos cerrados y conectan variables mediante flechas dirigidas.\n\n\n\n\nPara quienes necesiten profundizar, se recomienda leer el artículo:\n➡️ De Irala, J., et al. (2001). ¿Qué es una variable de confusión? Medicina Clínica, 117(10), 377–385. https://doi.org/10.1016/S0025-7753(01)72121-5\nManejo de la confusión\nDentro de las estrategias para manejar la confusión, podemos pensar en dos momentos:\n\nA la hora de diseñar y llevar a cabo el estudio:\n\nEmparejamiento individual.\nEmparejamiento de grupo.\n\n\nAl momento de analizar los datos:\n\nEstratificación.\nAjuste estadístico.\n\n\n\nEl ajuste estadístico, característico de los análisis multivariados, permite estimar el efecto específico de cada variable independiente sobre la dependiente, controlando por las demás variables.\nUn criterio comúnmente aceptado es que un factor se considera confusor si su ajuste provoca un cambio de al menos el 10% en la magnitud de la asociación. Por ejemplo, edad y sexo son variables frecuentemente confusoras en estudios epidemiológicos y generalmente son pocos los trabajos que no presentan datos ajustados por estas covariables..\nEn este curso utilizaremos la regresión lineal múltiple y los modelos lineales generalizados para manejar la confusión, ajustando por múltiples covariables. A continuación, se ejemplifica gráficamente con un diagrama de dispersión.\n\n\n\n\n\n\n\n\nLa recta de regresión muestra una correlación positiva entre los valores de las variables, \\(X\\) e \\(Y\\), con una ecuación de la forma:\n\\[\n\\hat{y} = \\beta_0 + \\beta_1x_1 + \\epsilon\n\\]La salida en R de este modelo se vería de la siguiente forma:\n\n\n\nCall:\nlm(formula = y ~ x, data = datos_conf)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-7.4701 -2.4124 -0.0179  2.5926  6.9821 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  2.45017    1.13262   2.163   0.0354 *  \nx            0.77889    0.09642   8.078 1.45e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.559 on 49 degrees of freedom\nMultiple R-squared:  0.5712,    Adjusted R-squared:  0.5624 \nF-statistic: 65.26 on 1 and 49 DF,  p-value: 1.45e-10\n\n\nEl intercepto (\\(\\beta_0\\)) es 2.45 y la pendiente (\\(\\beta_1\\)) es 0.78, explicando el 56% de la variabilidad de \\(Y\\) (\\(R^2\\) = 0.56). En la ecuación podemos representarlo como:\n\\[\n\\hat{y} = 2.45 + 0.78x_1 + \\epsilon\n\\]\nAhora incorporemos la covariable \\(Z\\), con categorías A y B, que sospechamos tiene un rol de confusión en el modelo teórico.\n\n\n\n\n\n\n\n\nEl gráfico de dispersión muestra que hay una diferencia entre las rectas de regresión que se mantiene prácticamente constante (paralelas) en todo su desarrollo. Esa distancia medida en valores de \\(Y\\) es \\(\\beta_2\\):\n\\[\n\\hat{y} = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + \\epsilon\n\\]\nVisto en resultados de consola:\n\n\n\nCall:\nlm(formula = y ~ x + z, data = datos_conf)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-7.958 -1.502 -0.225  1.840  5.652 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.65684    1.00484   0.654    0.516    \nx            0.65260    0.08372   7.795 4.49e-10 ***\nzB           4.55436    0.93258   4.884 1.20e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.939 on 48 degrees of freedom\nMultiple R-squared:  0.7135,    Adjusted R-squared:  0.7016 \nF-statistic: 59.77 on 2 and 48 DF,  p-value: 9.347e-14\n\n\nEl coeficiente \\(\\beta_1\\) de la variable independiente principal (\\(X\\)) varió al incorporar la nueva variable (\\(Z\\)), pasando de 0,78 (cruda) a 0,65 (ajustada), es decir que disminuyó casi un 20%. A la vez, la covariable tiene una relación significativa con la variable dependiente (\\(Y\\)) y el modelo aumenta el \\(R^2\\) ajustado a 0,70.\nEntonces podemos ver que la regresión multiple ajustó el efecto de \\(X\\) sobre \\(Y\\), teniendo en cuenta el efecto confusor de \\(Z\\) que sospechabamos. El valor de \\(Y\\) ahora es 0,66 (\\(\\beta_0\\)) + 0,65* el valor de x (\\(\\beta_1*x\\)) mientras \\(Z\\) es igual al nivel de referencia A, en cambio \\(Y\\) vale 0,66 (\\(\\beta_0\\)) + 0,65* el valor de \\(X\\) (\\(\\beta_1*x\\)) + 4,55 (\\(\\beta_2\\)) cuando \\(Z\\) es igual a B."
  },
  {
    "objectID": "unidad_3/06_confusion_interaccion.html#interacción-o-modificación-de-efecto",
    "href": "unidad_3/06_confusion_interaccion.html#interacción-o-modificación-de-efecto",
    "title": "Confusión e interacción",
    "section": "Interacción o modificación de efecto",
    "text": "Interacción o modificación de efecto\nMacMahon (1972) definió la interacción de la siguiente manera:\n\n\n\n\n\n\n“Cuando la incidencia de la enfermedad en presencia de dos o más factores de riesgo difiere de la incidencia que sería previsible por sus efectos individuales”\n\n\n\nEsto puede manifestarse como:\n\nSinergismo (interacción positiva): El efecto combinado es mayor que la suma de sus efectos individuales.\nAntagonismo (interacción negativa): El efecto combinado es menor.\n\nLa modificación del efecto ocurre cuando la magnitud de la asociación entre la exposición (\\(X\\)) y el resultado (\\(Y\\)) varía según los niveles de una tercera variable (\\(Z\\)).\n\n\n\n\nPara quienes tengan interés en profundizar el tema, pueden leer el artículo:\n➡️ De Irala, J., et al. (2001). ¿Qué es una variable modificadora del efecto? Medicina Clínica, 117(8), 297–302. https://doi.org/10.1016/S0025-7753(01)72092-1\nPara identificar la interacción en regresión lineal múltiple, se incluyen términos de interacción (\\(X * Z\\)), que representan una nueva variable con efectos multiplicativos. El término de interacción implica el exceso de la variabilidad de los datos que no puede ser explicada por la suma de las variables consideradas.\nEn cursos anteriores de Epidemiología se estudió que, frente a un modificador de efecto (ME) lo más adecuado era presentar las medidas de asociación según los estratos formados por las categorías de la variable ME (no estimar una medida ajustada para ambos estratos, como se hace en caso de variables confusoras).\nUn ejemplo similar al recién mostrado para la confusión, pero para la interacción podría ser:\n\n\n\n\n\n\n\n\n\\[\\hat{y} = \\beta_0 + \\beta_1x_1 + \\epsilon\\]\nPartimos de esta relación entre \\(X\\) e \\(Y\\) representada por la recta de la ecuación con los valores de la siguiente tabla:\n\n\n\nCall:\nlm(formula = y ~ x, data = datos_int)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-9.8410 -3.8804  0.3709  3.2871  8.4102 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   4.0035     1.4065   2.847   0.0062 ** \nx             0.5419     0.1132   4.789 1.31e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.554 on 55 degrees of freedom\nMultiple R-squared:  0.2943,    Adjusted R-squared:  0.2814 \nF-statistic: 22.93 on 1 and 55 DF,  p-value: 1.307e-05\n\n\nEl intercepto (\\(\\beta_0\\)) es de 4,0 y el coeficiente \\(\\beta_1\\) (pendiente) significativo de 0,54.\nAhora incorporemos la covariable \\(Z\\), con categorías A y B, que sospechamos tiene un rol de interacción.\n\n\n\n\n\n\n\n\nEl gráfico de dispersión muestra que hay una diferencia entre las rectas de regresión que tienen distintas pendientes según el valor de \\(X\\). Esa diferencia no es aditiva y pasa a ser multiplicativa y da lugar a la ecuación:\n\\[\\hat{y} = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + \\beta_3x_1x_2 + \\epsilon\\]\nVisto en resultados de consola:\n\n\n\nCall:\nlm(formula = y ~ x * z, data = datos_int)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.1427 -1.8739 -0.4598  1.3199  5.2230 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  2.77269    0.91587   3.027 0.003804 ** \nx            0.22708    0.07719   2.942 0.004830 ** \nzB           2.49329    1.51850   1.642 0.106522    \nx:zB         0.45574    0.12213   3.732 0.000465 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.364 on 53 degrees of freedom\nMultiple R-squared:  0.8168,    Adjusted R-squared:  0.8064 \nF-statistic: 78.75 on 3 and 53 DF,  p-value: &lt; 2.2e-16\n\n\nEl término de interacción es significativo, aunque la variable \\(Z\\) por sí misma no lo sea, lo que implica que la significancia se encuentra asociada específicamente a uno de los niveles de \\(Z\\) (en este caso, la categoría B). Dado que no es posible separar los niveles de \\(Z\\), esta variable debe permanecer en el modelo. Además, la inclusión del término de interacción mejora el \\(R^2\\) ajustado del modelo, que pasa de 0,28 a 0,80. Esto demuestra que, cuando la interacción es significativa, la significancia estadística de los efectos simples de las variables \\(X\\) y \\(Z\\) se vuelve menos relevante.\nEl modelo ajustado para \\(Y\\) es el siguiente:\n\nPara \\(Z = A\\):\n\n\\[\ny = 2,77 + 0,23 x\n\\]\n\nPara \\(Z = B\\):\n\n\\[\n2,77 + 0,23x + 2,49 + 0,46x\n\\]\nEsto muestra una interacción sinérgica entre la categoría B de la variable \\(Z\\) y la variable \\(X\\), reflejada en el efecto sobre \\(Y\\). La pendiente de la recta para \\(Z = B\\) es mayor que para \\(Z = A\\), lo que indica que el efecto de \\(X\\) sobre \\(Y\\) varía según el nivel de \\(Z\\).\nLimitaciones de los términos de interacción\nAunque el análisis de interacción en modelos lineales generales es una herramienta útil para explorar relaciones complejas entre variables, es importante ser consciente de sus limitaciones y asumirlas en el diseño, análisis e interpretación de los resultados. Entre ellas encontramos:\n\nLinealidad y Simplicidad del Modelo: El modelo lineal general asume relaciones lineales entre las variables, pero las interacciones a veces no muestran linealidad.\nSobreajuste en Modelos con Múltiples Interacciones: La inclusión de muchos términos de interacción, puede sobreajustar el modelo y dificultar su interpretación y generalización.\nColinealidad: La colinealidad entre las variables independientes puede ser exacerbada al incluir términos de interacción, lo que dificultaría la identificación de relaciones significativas.\nPérdida de Potencia Estadística: Al incluir términos de interacción, el número de parámetros a estimar aumenta, reduciendo los grados de libertad y pudiendo disminuir la potencia estadística.\nDificultades en la Interpretación: Los coeficientes de interacción pueden ser difíciles de interpretar, especialmente cuando las variables son continuas.\nHeterogeneidad de Efectos: Los términos de interacción pueden no capturar variaciones en los efectos entre subgrupos específicos.\nErrores de Especificación del Modelo: Si las interacciones relevantes no se incluyen o se modelan incorrectamente, el modelo puede estar mal especificado.\nSensibilidad a la Codificación de las Variables: La codificación de variables categóricas puede influir en la interpretación de los coeficientes de interacción.\nLimitaciones Computacionales: En grandes conjuntos de datos o modelos complejos, la inclusión de múltiples interacciones puede aumentar significativamente la carga computacional y los recursos necesarios."
  },
  {
    "objectID": "unidad_4/01_est_caso_control.html",
    "href": "unidad_4/01_est_caso_control.html",
    "title": "Estudios de casos y controles",
    "section": "",
    "text": "Un estudio de casos y controles (CC) es un estudio analítico observacional que comienza con la identificación de dos grupos:\n\nCasos: individuos que presentan algún evento particular de salud.\nControles: individuos que no presentan el evento.\n\nEl objetivo es comparar la frecuencia de exposición a un factor de interés entre ambos grupos. La hipótesis subyacente es que si la exposición es más frecuente en los casos que en los controles, podría tratarse de un factor de riesgo. En cambio, si la exposición es menos frecuente en los casos, podría actuar como un factor protector.\nLos principales aspectos metodológicos a tener en cuenta para llevar adelante un estudio de casos y controles son:\n\nDefinición precisa de la variable dependiente (evento de interés).\nDefinición de las variables independientes (factores de exposición).\nCriterios de selección de los casos (fuente y definición).\nCriterios de selección de los controles (fuente y criterios de inclusión/exclusión).\nAnálisis estadístico.\n\nEs fundamental tener criterios precisos para la definición de caso. Los criterios pueden estar basados en resultados de pruebas de laboratorio o pueden estar basados en la historia clínica o en encuestas.\nSegún la definición de casos, se diferencian dos tipos importantes de estudios de casos y controles:\n\nEstudios de casos y controles con casos incidentes: Incluyen solo casos nuevos de la enfermedad durante el período de estudio.\nEstudios de casos y controles con casos prevalentes: Consideran todos los individuos que tienen la enfermedad en un momento determinado.\n\nAlgunos autores consideran los estudios de CC como una estrategia alternativa para estudiar una cohorte. Esto permite diferenciar dos enfoques:\n\n\nCasos en cohortes de densidad de incidencia: Los casos se reclutan conforme aparecen (incidentes). Este enfoque es útil cuando la exposición tiene períodos de latencia prolongados.\n\nCasos en cohortes de incidencia acumulada: Los casos se seleccionan al final del estudio, con un período de seguimiento previamente definido.\n\n\nLa fuente de los controles quedará definida en la medida en que se definan claramente los criterios de selección de casos, así como la población hipotética de origen. Algunos puntos a considerar son los siguientes:\n\nLos controles deben seleccionarse de la misma base poblacional (de la cohorte hipotética) de donde se originaron los casos. Si uno de estos controles desarrollara el evento en cuestión, pasaría a formar parte del grupo de los casos. Esto último implica que los controles estuvieron en riesgo de desarrollar el evento en forma simultánea a los casos.\nLos controles deben seleccionarse independientemente de su condición de expuestos o no expuestos.\nLa probabilidad de selección para los controles debe ser proporcional al tiempo que el sujeto estuvo en riesgo de desarrollar el evento o enfermedad en estudio.\nPara minimizar confusión, se pueden emparejar controles con casos en función de variables confusoras.\nLos procedimientos para medir la exposición, deben ser los mismos, tanto en casos como en controles.\n\nLas fuentes de controles pueden incluir población general, hospitales, vecindarios, registros de mortalidad, entre otros."
  },
  {
    "objectID": "unidad_4/01_est_caso_control.html#introducción",
    "href": "unidad_4/01_est_caso_control.html#introducción",
    "title": "Estudios de casos y controles",
    "section": "",
    "text": "Un estudio de casos y controles (CC) es un estudio analítico observacional que comienza con la identificación de dos grupos:\n\nCasos: individuos que presentan algún evento particular de salud.\nControles: individuos que no presentan el evento.\n\nEl objetivo es comparar la frecuencia de exposición a un factor de interés entre ambos grupos. La hipótesis subyacente es que si la exposición es más frecuente en los casos que en los controles, podría tratarse de un factor de riesgo. En cambio, si la exposición es menos frecuente en los casos, podría actuar como un factor protector.\nLos principales aspectos metodológicos a tener en cuenta para llevar adelante un estudio de casos y controles son:\n\nDefinición precisa de la variable dependiente (evento de interés).\nDefinición de las variables independientes (factores de exposición).\nCriterios de selección de los casos (fuente y definición).\nCriterios de selección de los controles (fuente y criterios de inclusión/exclusión).\nAnálisis estadístico.\n\nEs fundamental tener criterios precisos para la definición de caso. Los criterios pueden estar basados en resultados de pruebas de laboratorio o pueden estar basados en la historia clínica o en encuestas.\nSegún la definición de casos, se diferencian dos tipos importantes de estudios de casos y controles:\n\nEstudios de casos y controles con casos incidentes: Incluyen solo casos nuevos de la enfermedad durante el período de estudio.\nEstudios de casos y controles con casos prevalentes: Consideran todos los individuos que tienen la enfermedad en un momento determinado.\n\nAlgunos autores consideran los estudios de CC como una estrategia alternativa para estudiar una cohorte. Esto permite diferenciar dos enfoques:\n\n\nCasos en cohortes de densidad de incidencia: Los casos se reclutan conforme aparecen (incidentes). Este enfoque es útil cuando la exposición tiene períodos de latencia prolongados.\n\nCasos en cohortes de incidencia acumulada: Los casos se seleccionan al final del estudio, con un período de seguimiento previamente definido.\n\n\nLa fuente de los controles quedará definida en la medida en que se definan claramente los criterios de selección de casos, así como la población hipotética de origen. Algunos puntos a considerar son los siguientes:\n\nLos controles deben seleccionarse de la misma base poblacional (de la cohorte hipotética) de donde se originaron los casos. Si uno de estos controles desarrollara el evento en cuestión, pasaría a formar parte del grupo de los casos. Esto último implica que los controles estuvieron en riesgo de desarrollar el evento en forma simultánea a los casos.\nLos controles deben seleccionarse independientemente de su condición de expuestos o no expuestos.\nLa probabilidad de selección para los controles debe ser proporcional al tiempo que el sujeto estuvo en riesgo de desarrollar el evento o enfermedad en estudio.\nPara minimizar confusión, se pueden emparejar controles con casos en función de variables confusoras.\nLos procedimientos para medir la exposición, deben ser los mismos, tanto en casos como en controles.\n\nLas fuentes de controles pueden incluir población general, hospitales, vecindarios, registros de mortalidad, entre otros."
  },
  {
    "objectID": "unidad_4/01_est_caso_control.html#clasificación-de-los-estudios-de-casos-y-controles",
    "href": "unidad_4/01_est_caso_control.html#clasificación-de-los-estudios-de-casos-y-controles",
    "title": "Estudios de casos y controles",
    "section": "Clasificación de los estudios de casos y controles",
    "text": "Clasificación de los estudios de casos y controles\nEn los últimos tiempos, se ha profundizado en los aspectos metodológicos de los estudios de casos y controles; y también se ha clarificado la estrecha relación que existe con los estudios de cohorte, lo que ha permitido el desarrollo de diferentes variantes. Describiremos brevemente algunas de ellas a continuación:\n\nEstudios caso-cohorte. En esta variante, la definición de casos y controles se encuentra anidada en una cohorte fija, se utiliza un enfoque de incidencia acumulada.\nEstudios de casos y controles anidado o de grupo de riesgo. Es similar al anterior, sólo que se trata de una cohorte dinámica, y un enfoque de densidad de incidencia.\nEstudios caso-autocontrol. Esta variante utiliza al mismo sujeto que se consideró como caso, como su propio control. Este tipo de estrategia se suele utilizar para exposiciones que son de corta duración y que cambian en el tiempo y con eventos que son fáciles de detectar.\nEstudios de mortalidad proporcional. Tanto los casos como los controles se obtienen de los registros de mortalidad poblacionales. Rothman dice que este tipo de controles son aceptables, sólo si la distribución de la exposición entre los grupos es similar a la que presenta la base poblacional.\nEstudios de caso-caso. En esta estrategia se compara la historia de exposición en subgrupos de casos. En el caso de enfermedades infecciosas, es posible conformar diversos subgrupos de enfermedad, partiendo de datos de vigilancia epidemiológica.\n\nQuienes desean profundizar estos tópicos sobre el diseño de los CyC, pueden leer el capítulo correspondiente de “Epidemiología. Diseño y análisis de estudios” de Hernández-Ávila (2011) . Comenzaremos ahora con la parte de análisis de estos diseños."
  },
  {
    "objectID": "unidad_4/01_est_caso_control.html#análisis-de-estudios-de-casos-y-controles",
    "href": "unidad_4/01_est_caso_control.html#análisis-de-estudios-de-casos-y-controles",
    "title": "Estudios de casos y controles",
    "section": "Análisis de estudios de casos y controles",
    "text": "Análisis de estudios de casos y controles\nEn estudios de CC, la medida de asociación principal es el odds ratio (OR) de la exposición. Este se calcula comparando la frecuencia de exposición entre casos y controles:\n\\[\nOR = \\frac{Odds~exposición~casos}{Odds~exposición~controles}\n\\]\nEl modelo multivariado a utilizar para estudios de casos y controles no apareados es la regresión logística, que desarrollaremos a continuación. Por otro lado, los estudios de casos y controles apareados se analizan mediante modelos de regresión logística condicional, los cuales abordaremos brevemente al final de la unidad.\n\nEscuela Nacional de Sanidad (ENS). Instituto de Salud Carlos III. Ministerio de Ciencias e Innovación. Madrid (2009)\nField, Miles, y Field (2014)\nOrtega Calvo y Cayuela Domínguez (2002)\nRothman (2012)\nSilva Ayçaguer (1995)\nThompson (1994)"
  },
  {
    "objectID": "unidad_4/03_sep_datos.html",
    "href": "unidad_4/03_sep_datos.html",
    "title": "Separación de datos binomiales",
    "section": "",
    "text": "En los modelos de regresión logística, se analizan las relaciones entre una variable dependiente de tipo binaria y una o más variables independientes que pueden ser de distintos tipos (numéricas continuas, numéricas discretas, categóricas, binarias, etc.). Sin embargo, en algunas situaciones, la relación entre las variables independientes y la variable respuesta puede derivar en problemas de separación de datos, afectando las estimaciones del modelo."
  },
  {
    "objectID": "unidad_4/03_sep_datos.html#introducción",
    "href": "unidad_4/03_sep_datos.html#introducción",
    "title": "Separación de datos binomiales",
    "section": "",
    "text": "En los modelos de regresión logística, se analizan las relaciones entre una variable dependiente de tipo binaria y una o más variables independientes que pueden ser de distintos tipos (numéricas continuas, numéricas discretas, categóricas, binarias, etc.). Sin embargo, en algunas situaciones, la relación entre las variables independientes y la variable respuesta puede derivar en problemas de separación de datos, afectando las estimaciones del modelo."
  },
  {
    "objectID": "unidad_4/03_sep_datos.html#tipos-de-separación-de-datos",
    "href": "unidad_4/03_sep_datos.html#tipos-de-separación-de-datos",
    "title": "Separación de datos binomiales",
    "section": "Tipos de separación de datos",
    "text": "Tipos de separación de datos\nSeparación total\nTambién conocida como separación perfecta o completa, ocurre cuando un predictor o combinación lineal de predictores clasifica perfectamente las observaciones en una de las dos categorías de la variable respuesta. En estos casos, las probabilidades predichas para una de las categorías son 0 o 1, y los coeficientes de regresión para las variables involucradas se vuelven indefinidos. Esto provoca un fallo en la convergencia del modelo, ya que no se pueden calcular los logaritmos para odds ratios (OR) iguales a 0 o a infinito (\\(\\infty\\)), generando un mensaje de error.\nSeparación parcial\nLa separación parcial se presenta cuando un predictor o combinación lineal de predictores predice perfectamente la variable respuesta solo en algunos niveles de las variables independientes. Es decir, separa en gran medida las categorías de la variable respuesta. Esto puede llevar a la estimación de coeficientes y errores estándar extremadamente grandes, con intervalos de confianza muy amplios, lo que hace que las inferencias realizadas sean poco fiables.\nTomemos como ejemplo la base “covid_cancer.txt”, que contiene datos de un estudio de casos y controles realizado en la ciudad de Santa Fe, Argentina en pacientes con cáncer hospitalizados por formas severas de COVID-191.\nCargamos paquetes necesarios:\n\nlibrary(gtsummary)\nlibrary(janitor)\nlibrary(tidyverse)\n\nCargamos datos:\n\n# Carga datos\ndatos &lt;- read_delim(\"datos/covid_cancer.txt\")\n\n# Explora datos\nglimpse(datos)\n\nRows: 79\nColumns: 13\n$ fallecido         &lt;dbl&gt; 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0…\n$ sexo              &lt;chr&gt; \"M\", \"M\", \"F\", \"F\", \"F\", \"M\", \"F\", \"M\", \"F\", \"M\", \"M…\n$ edad              &lt;dbl&gt; 60, 65, 70, 73, 60, 67, 81, 35, 79, 47, 58, 70, 57, …\n$ comorbilidades    &lt;chr&gt; \"Sí\", \"Sí\", \"Sí\", \"Sí\", \"No\", \"Sí\", \"Sí\", \"No\", \"Sí\"…\n$ disnea            &lt;chr&gt; NA, \"Sí\", \"Sí\", \"No\", \"No\", NA, \"No\", NA, \"Sí\", NA, …\n$ neumonia_severa   &lt;chr&gt; \"Sí\", \"Sí\", \"Sí\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\"…\n$ inf_secundaria    &lt;chr&gt; \"Sí\", NA, \"Sí\", \"No\", \"No\", \"No\", \"Sí\", \"No\", \"No\", …\n$ complicaciones    &lt;chr&gt; \"Sí\", NA, \"Sí\", \"No\", \"No\", \"No\", \"Sí\", \"No\", \"Sí\", …\n$ infil_bilateral   &lt;chr&gt; \"Sí\", \"Sí\", \"Sí\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\"…\n$ asist_resp        &lt;chr&gt; \"Sí\", \"Sí\", \"Sí\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\"…\n$ cancer_tipo       &lt;chr&gt; \"Sólido\", \"Sólido\", \"Sólido\", \"Sólido\", \"Sólido\", \"S…\n$ tr_quimioterapia  &lt;chr&gt; \"Sí\", \"Sí\", \"Sí\", \"Sí\", \"Sí\", \"Sí\", \"Sí\", \"Sí\", \"Sí\"…\n$ tr4_quimioterapia &lt;chr&gt; \"Sí\", \"Sí\", \"Sí\", \"Sí\", \"Sí\", \"Sí\", \"No\", \"No\", \"No\"…\n\n\nLas variables de interés incluyen:\n\nfallecido: fallecimiento por COVID-19 (No: 0, Sí: 1)\nsexo: sexo biológico del/a paciente (M: masculino, F: femenino)\nedad: edad en años al momento de la hospitalización\ncomorbilidades: presencia de comorbilidades (Sí, No)\ndisnea: dificultad para respirar (Sí, No)\nneumonia_severa: paciente con neumonía severa (Sí, No)\ninf_secundaria: presencia de infección secundaria (Sí, No)\ncomplicaciones: complicaciones del COVID-19 (Sí, No)\ninfil_bilateral: infiltración bilateral en radiografía (Sí, No)\nasist_resp: paciente que recibió asistencia respiratoria (Sí, No)\ncancer_tipo: tipo de cáncer (Sólido, Hematológico)\ntr_quimioterapia: paciente que recibió quimioterapia (Sí, No)\ntr4_quimioterapia: paciente que recibió quimioterapia en el último mes (Sí, No)\n\nNuestra variable dependiente es si el/la paciente falleció a causa del COVID-19. Comenzaremos por ajustar modelos de regresión logística univariados para evaluar qué variables explicativas están asociadas significativamente a la variable dependiente:\n\n# Regresiones logísticas simples\ndatos |&gt; \n  tbl_uvregression(y = fallecido,\n                   method = glm,\n                   method.args = list(family = binomial),\n                   exponentiate = T) |&gt; \n  bold_p()\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\nN\nOR\n95% CI\np-value\n\n\n\nsexo\n78\n\n\n\n\n\n    F\n\n—\n—\n\n\n\n    M\n\n1.95\n0.61, 6.56\n0.3\n\n\nedad\n74\n1.04\n0.99, 1.10\n0.12\n\n\ncomorbilidades\n53\n\n\n\n\n\n    No\n\n—\n—\n\n\n\n    Sí\n\n0.93\n0.27, 3.30\n&gt;0.9\n\n\ndisnea\n51\n\n\n\n\n\n    No\n\n—\n—\n\n\n\n    Sí\n\n9.19\n2.18, 49.2\n0.004\n\n\nneumonia_severa\n79\n\n\n\n\n\n    No\n\n—\n—\n\n\n\n    Sí\n\n8.30\n2.37, 31.2\n0.001\n\n\ninf_secundaria\n36\n\n\n\n\n\n    No\n\n—\n—\n\n\n\n    Sí\n\n8.33\n1.38, 59.1\n0.023\n\n\ncomplicaciones\n36\n\n\n\n\n\n    No\n\n—\n—\n\n\n\n    Sí\n\n1,139,380,581\n0.00,\n\n&gt;0.9\n\n\ninfil_bilateral\n40\n\n\n\n\n\n    No\n\n—\n—\n\n\n\n    Sí\n\n1.56\n0.42, 5.98\n0.5\n\n\nasist_resp\n39\n\n\n\n\n\n    No\n\n—\n—\n\n\n\n    Sí\n\n6.00\n1.26, 44.4\n0.039\n\n\ncancer_tipo\n74\n\n\n\n\n\n    Hematológico\n\n—\n—\n\n\n\n    Sólido\n\n3,844,194\n0.00,\n\n&gt;0.9\n\n\ntr_quimioterapia\n49\n\n\n\n\n\n    No\n\n—\n—\n\n\n\n    Sí\n\n5.20\n0.85, 101\n0.14\n\n\ntr4_quimioterapia\n39\n\n\n\n\n\n    No\n\n—\n—\n\n\n\n    Sí\n\n1.40\n0.33, 6.48\n0.7\n\n\n\nAbbreviations: CI = Confidence Interval, OR = Odds Ratio\n\n\n\n\n\nAl ejecutar este código aparecerá el siguiente mensaje de advertencia: Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred. Esto indica que todas o alguna de las variables explicativas provoca separación parcial de los datos.\nAl observar la tabla generada, notamos que las variables disnea, neumonia_severa, inf_secundaria, asist_resp y tr_quimioterapia presentan intervalos de confianza excesivamente amplios, lo que sugiere la presencia de separación parcial. Si generamos tablas de 2x2 de cada variable explicativa en función de la variable fallecido, podemos visualizar este fenómeno:\n\n# disnea\ndatos |&gt; \n  tabyl(disnea, fallecido, \n        show_na = F) |&gt; \n  adorn_percentages() |&gt; \n  adorn_pct_formatting()\n\n disnea     0     1\n     No 91.2%  8.8%\n     Sí 52.9% 47.1%\n\n# neumonía severa\ndatos |&gt; \n  tabyl(neumonia_severa, fallecido, \n        show_na = F) |&gt; \n  adorn_percentages() |&gt; \n  adorn_pct_formatting()\n\n neumonia_severa     0     1\n              No 90.3%  9.7%\n              Sí 52.9% 47.1%\n\n# infección secundaria\ndatos |&gt; \n  tabyl(inf_secundaria, fallecido, \n        show_na = F) |&gt; \n  adorn_percentages() |&gt; \n  adorn_pct_formatting()\n\n inf_secundaria     0     1\n             No 86.2% 13.8%\n             Sí 42.9% 57.1%\n\n# asistencia respiratoria\ndatos |&gt; \n  tabyl(asist_resp, fallecido, \n        show_na = F) |&gt; \n  adorn_percentages() |&gt; \n  adorn_pct_formatting()\n\n asist_resp     0     1\n         No 88.9% 11.1%\n         Sí 57.1% 42.9%\n\n# quimioterapia\ndatos |&gt; \n  tabyl(tr_quimioterapia, fallecido, \n        show_na = F) |&gt; \n  adorn_percentages() |&gt; \n  adorn_pct_formatting()\n\n tr_quimioterapia     0     1\n               No 92.9%  7.1%\n               Sí 71.4% 28.6%\n\n\nEstos análisis muestran que variables como disnea, neumonia_severa, inf_secundaria, asist_resp y tr_quimioterapia explican casi completamente la mortalidad por COVID-19 severo.\nSi evaluamos las variables complicaciones y cancer_tipo:\n\n# complicaciones\ndatos |&gt; \n  tabyl(complicaciones, fallecido, \n        show_na = F) |&gt; \n  adorn_percentages() |&gt; \n  adorn_pct_formatting()\n\n complicaciones      0     1\n             No 100.0%  0.0%\n             Sí  42.9% 57.1%\n\n# tipo de cáncer\ndatos |&gt; \n  tabyl(cancer_tipo, fallecido, \n        show_na = F) |&gt; \n  adorn_percentages() |&gt; \n  adorn_pct_formatting()\n\n  cancer_tipo      0     1\n Hematológico 100.0%  0.0%\n       Sólido  80.3% 19.7%\n\n\nObservamos que todos los pacientes sin complicaciones o con cáncer hematológico sobrevivieron, lo cual también indica separación parcial de los datos.\nAhora, ajustemos un modelo para evaluar el efecto de la interacción entre el tipo de cáncer y haber recibido quimioterapia en los 30 días previos, controlando por la presencia de neumonía severa y asistencia respiratoria:\n\nfit1 &lt;- glm(fallecido ~ cancer_tipo * tr4_quimioterapia + \n              neumonia_severa + asist_resp, \n            data = datos, family = binomial)\n\n\nsummary(fit1)\n\n\nCall:\nglm(formula = fallecido ~ cancer_tipo * tr4_quimioterapia + neumonia_severa + \n    asist_resp, family = binomial, data = datos)\n\nCoefficients: (1 not defined because of singularities)\n                                       Estimate Std. Error z value Pr(&gt;|z|)  \n(Intercept)                            -19.4976  2399.5450  -0.008   0.9935  \ncancer_tipoSólido                       16.7936  2399.5448   0.007   0.9944  \ntr4_quimioterapiaSí                      0.8431     1.0135   0.832   0.4055  \nneumonia_severaSí                        2.6769     1.3791   1.941   0.0522 .\nasist_respSí                             0.2547     1.3683   0.186   0.8523  \ncancer_tipoSólido:tr4_quimioterapiaSí        NA         NA      NA       NA  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 38.024  on 31  degrees of freedom\nResidual deviance: 26.599  on 27  degrees of freedom\n  (47 observations deleted due to missingness)\nAIC: 36.599\n\nNumber of Fisher Scoring iterations: 15\n\n\nEl resumen del modelo muestra el mensaje: Coefficients: (1 not defined because of singularities), indicando que los errores estándar para el intercepto y cancer_tipo son muy grandes. Además, no se muestran los coeficientes ni el p-valor para la interacción. Esto se puede visualizar tabulando los coeficientes:\n\ntbl_regression(fit1, exponentiate = T)\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\nOR\n95% CI\np-value\n\n\n\ncancer_tipo\n\n\n\n\n\n    Hematológico\n—\n—\n\n\n\n    Sólido\n19,650,260\n0.00,\n\n&gt;0.9\n\n\ntr4_quimioterapia\n\n\n\n\n\n    No\n—\n—\n\n\n\n    Sí\n2.32\n0.33, 21.1\n0.4\n\n\nneumonia_severa\n\n\n\n\n\n    No\n—\n—\n\n\n\n    Sí\n14.5\n1.30, 416\n0.052\n\n\nasist_resp\n\n\n\n\n\n    No\n—\n—\n\n\n\n    Sí\n1.29\n0.05, 17.7\n0.9\n\n\ncancer_tipo * tr4_quimioterapia\n\n\n\n\n\n    Sólido * Sí\n\n\n\n\n\n\nAbbreviations: CI = Confidence Interval, OR = Odds Ratio\n\n\n\n\n\nRecibirás advertencias como: Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred y Warning in regularize.values(x, y, ties, missing(ties), na.rm = na.rm) : collapsing to unique 'x' values, lo que sugiere separación parcial de los datos.\nPara evaluar separación de estos datos, podemos crear una nueva variable que represente el cruce entre tipo de cáncer y quimioterapia usando la función fct_cross() de tidyverse:\n\ndatos |&gt; \n  mutate(tipo_ca_quim = fct_cross(cancer_tipo, tr4_quimioterapia)) |&gt; \n  tabyl(tipo_ca_quim, fallecido, show_na = F) |&gt; \n  adorn_percentages() |&gt; \n  adorn_pct_formatting()\n\n    tipo_ca_quim      0     1\n Hematológico:No 100.0%  0.0%\n       Sólido:No  76.5% 23.5%\n       Sólido:Sí  71.4% 28.6%\n\n\nEste análisis muestra que ninguno de los pacientes con cáncer hematológico recibió quimioterapia en los últimos 30 días, lo que también contribuye a la separación parcial.\nFinalmente, para evaluar la separación de datos de manera más formal, utilizamos el paquete detectseparation (Kosmidis, Schumacher, y Schwendinger 2022). Ajustamos nuevamente el modelo con el argumento method = \"detect_separation\":\n\nlibrary(detectseparation)\n\n# Tipo de cáncer\nglm(fallecido ~ cancer_tipo, data = datos, family = binomial,\n    method = \"detect_separation\")\n\nImplementation: ROI | Solver: lpsolve \nSeparation: TRUE \nExistence of maximum likelihood estimates\n      (Intercept) cancer_tipoSólido \n             -Inf               Inf \n0: finite value, Inf: infinity, -Inf: -infinity\n\n# Neumonía severa\nglm(fallecido ~ neumonia_severa, data = datos, family = binomial,\n    method = \"detect_separation\")\n\nImplementation: ROI | Solver: lpsolve \nSeparation: FALSE \nExistence of maximum likelihood estimates\n      (Intercept) neumonia_severaSí \n                0                 0 \n0: finite value, Inf: infinity, -Inf: -infinity\n\n# Modelo con interacción\nglm(fallecido ~ cancer_tipo * tr4_quimioterapia + \n              neumonia_severa + asist_resp, \n            data = datos, \n            family = binomial,\n            method = \"detect_separation\")\n\nImplementation: ROI | Solver: lpsolve \nSeparation: TRUE \nExistence of maximum likelihood estimates\n                          (Intercept)                     cancer_tipoSólido \n                                 -Inf                                   Inf \n                  tr4_quimioterapiaSí                     neumonia_severaSí \n                                    0                                     0 \n                         asist_respSí cancer_tipoSólido:tr4_quimioterapiaSí \n                                    0                                    NA \n0: finite value, Inf: infinity, -Inf: -infinity\n\n\nEn el caso de separación completa, un nivel de la variable explicativa debería modelar perfectamente los “Sí” y el otro los “No”, y al intentar correr el modelo nos encontraríamos con un error de convergencia que impediría continuar con el análisis."
  },
  {
    "objectID": "unidad_4/03_sep_datos.html#soluciones-a-la-separación-de-datos",
    "href": "unidad_4/03_sep_datos.html#soluciones-a-la-separación-de-datos",
    "title": "Separación de datos binomiales",
    "section": "Soluciones a la separación de datos",
    "text": "Soluciones a la separación de datos\nRemuestreo, recolección o submuestreo\nSiempre que fuera posible, recolectar más datos puede mitigar la separación total al introducir más variabilidad en los datos. En su defecto, se puede considerar el submuestreo o el sobremuestreo para balancear las clases. Sin embargo, estas técnicas deben aplicarse con cuidado para evitar sesgos.\nEliminación o recategorización de variables\nEn casos de separación parcial, una opción es simplificar el modelo eliminando las variables causantes de la separación. Si esas variables son esenciales, otra alternativa es agrupar niveles de las variables categóricas para reducir la separación. Esto puede hacer que el modelo sea más robusto al reducir la complejidad y mejorar la estabilidad de las estimaciones.\nUso de modelos alternativos a la regresión logística\nLos modelos de pseudo-likelihood, como la regresión logística condicional y la máxima verosimilitud penalizada, permiten ajustar el modelo incluso cuando la separación de datos impide la convergencia con métodos tradicionales. Los modelos penalizados, que incluyen las regresiones Ridge, Lasso y Elastic Net, añaden un término de penalización que controla el crecimiento excesivo de los coeficientes en casos de separación parcial, facilitando la convergencia y mejorando la estabilidad del modelo. Por otro lado, los modelos bayesianos introducen distribuciones previas sobre los coeficientes, permitiendo una mayor flexibilidad para manejar la separación de datos y ofreciendo estimaciones más estables y confiables. Sin embargo, el ajuste e interpretación de estos modelos es complejo y escapa al alcance de este curso, por lo que no serán abordados en detalle.\n\nAgresti (2015)\n(base?)\nWickham et al. (2019)\nFirke (2024)"
  },
  {
    "objectID": "unidad_4/03_sep_datos.html#footnotes",
    "href": "unidad_4/03_sep_datos.html#footnotes",
    "title": "Separación de datos binomiales",
    "section": "Notas",
    "text": "Notas\n\nGastiazoro MP, Cardozo MA, Ricardo T, Ramos JG, Ballina A, Maillo M, et al. Clinical features in cancer patients with COVID-19 in Santa Fe and Buenos Aires, Argentina. J Clin Images Med Case Rep [Internet]. el 14 de febrero de 2023 [citado el 9 de agosto de 2024];4(2). Disponible en: https://jcimcr.org/articles/JCIMCR-v4-2285.html↩︎"
  },
  {
    "objectID": "unidad_4/05_mod_predict.html",
    "href": "unidad_4/05_mod_predict.html",
    "title": "Modelos predictivos",
    "section": "",
    "text": "En construcción\n\n\n Volver arribaReutilizaciónCC BY-NC 4.0"
  },
  {
    "objectID": "unidad_5/02_reg_poisson.html",
    "href": "unidad_5/02_reg_poisson.html",
    "title": "Regresión de Poisson",
    "section": "",
    "text": "La distribución de Poisson es una distribución de probabilidad que se utiliza para modelar situaciones en las que contamos eventos discretos (número de accidentes, personas que sufren un infarto, visitas a una consulta médica, número de hijos, etc.). Estas situaciones comparten la característica de involucrar números finitos, relativamente pequeños y siempre positivos. Los eventos ocurren dentro de un intervalo definido, que suele ser de tiempo, aunque también puede referirse a otros tipos de intervalos, como el tamaño de la población. En esencia, la distribución de Poisson busca modelar el número de veces en que ocurre un evento durante un intervalo determinado.\nLa distribución de Poisson toma valores enteros no negativos: 0, 1, 2, 3, 4, etc. Tiene un único parámetro, lambda (\\(\\lambda\\)), que representa tanto la media como la varianza en la distribución. Es decir, que cuanto más grande es el valor esperado, mayor dispersión tienen los valores.\nVeamos cómo se aplica la regresión de Poisson en un estudio de cohortes. Supongamos que tenemos un estudio de cohortes clásico con dos grupos de comparación: un grupo expuesto y otro no expuesto. Para cada grupo se dispone el número de eventos (\\(d\\)) y la cantidad de personas-tiempo seguidas (\\(n\\)). Como recordarán, la tasa de incidencia (\\(\\lambda\\)), la podemos calcular como el cociente:\n\\[\n\\lambda = \\frac{d}{n}\n\\]\nSe asume que el número de eventos observados en cada grupo sigue una distribución de Poisson, con un valor esperado igual al producto de la tasa de incidencia por las personas-tiempo.\n\\[\nE(d) = \\lambda * n\n\\]\nAsí, la probabilidad de observar \\(d\\) eventos se calcula como:\n\\[\nP(x=d)= \\frac{(\\lambda n)^d e^{-\\lambda n}}{d}  \n\\]\nOmitiendo el desarrollo matemático, la regresión de Poisson consiste en establecer un modelo en el que diferentes covariables tienen un efecto lineal sobre el logaritmo de la tasa de incidencia del subgrupo correspondiente. Generalmente, el modelo de Poisson se expresa como:\n\\[\nln\\lambda = \\alpha+\\beta_1x_1+\\beta_2x_2+\\dots+\\beta_nx_n \\qquad (*)\n\\]\nEn forma equivalente, podemos expresar la tasa de incidencia como:\n\\[\n\\lambda=e^{\\beta_0+\\beta_1x_1+\\dots+\\beta_nx_n}\n\\] Siguiendo un razonamiento análogo al de la regresión logística, los parámetros del modelo pueden interpretarse como riesgos relativos (cocientes de tasas), representados como potencias de base \\(e\\). Formalmente, el riesgo relativo entre dos niveles de exposición se expresa como:\\[\nRR_{x*/x}=e^{\\sum^k_i=1\\beta_i(x^*_i-x_i)}=\\prod^k_{i=1}e^{\\beta_i(x^*_i-x_i)}  \n\\]Donde el símbolo \\(\\prod\\) (productoria) implica una secuencia de productos.\nVolviendo a la ecuación de la tasa de incidencia:\n\\[\n\\lambda = \\frac{d}{n}\n\\]\nSi aplicamos logaritmo natural:\n\\[\nln(\\lambda)=ln \\frac{d}{n}=ln(d)-ln(n)\n\\] Igualando con la ecuación \\((*)\\) tenemos:\n\\[\nln\\lambda=\\alpha+\\beta_1x_1+\\beta_2x_2+\\dots+\\beta_nx_n\n\\] \\[\nln(d)=ln(n)+\\alpha+\\beta_1x_1+\\beta_2x_2+\\dots+\\beta_nx_n  \n\\]Al término \\(ln(n)\\) se lo denomina offset. En general, es un valor que debemos proporcionarle al software para ajustar un modelo de Poisson y no se estima a partir de los datos.\nCon la ecuación del modelo establecida, el siguiente paso es estimar los coeficientes y evaluar la calidad del ajuste. Los coeficientes se calculan utilizando métodos como la Estimación de Máxima Verosimilitud (MLE). A partir de estos coeficientes, podemos realizar inferencias, como el test de Wald para evaluar la hipótesis nula (\\(H_0 : \\beta_i = 0\\)), calcular los intervalos de confianza (IC), y obtener los riesgos relativos (RR) con sus respectivos IC. La bondad de ajuste del modelo se evaluará a través de la función Deviance siguiendo el mismo esquema jerárquico de evaluación que en el caso de la regresión logística."
  },
  {
    "objectID": "unidad_5/02_reg_poisson.html#introducción",
    "href": "unidad_5/02_reg_poisson.html#introducción",
    "title": "Regresión de Poisson",
    "section": "",
    "text": "La distribución de Poisson es una distribución de probabilidad que se utiliza para modelar situaciones en las que contamos eventos discretos (número de accidentes, personas que sufren un infarto, visitas a una consulta médica, número de hijos, etc.). Estas situaciones comparten la característica de involucrar números finitos, relativamente pequeños y siempre positivos. Los eventos ocurren dentro de un intervalo definido, que suele ser de tiempo, aunque también puede referirse a otros tipos de intervalos, como el tamaño de la población. En esencia, la distribución de Poisson busca modelar el número de veces en que ocurre un evento durante un intervalo determinado.\nLa distribución de Poisson toma valores enteros no negativos: 0, 1, 2, 3, 4, etc. Tiene un único parámetro, lambda (\\(\\lambda\\)), que representa tanto la media como la varianza en la distribución. Es decir, que cuanto más grande es el valor esperado, mayor dispersión tienen los valores.\nVeamos cómo se aplica la regresión de Poisson en un estudio de cohortes. Supongamos que tenemos un estudio de cohortes clásico con dos grupos de comparación: un grupo expuesto y otro no expuesto. Para cada grupo se dispone el número de eventos (\\(d\\)) y la cantidad de personas-tiempo seguidas (\\(n\\)). Como recordarán, la tasa de incidencia (\\(\\lambda\\)), la podemos calcular como el cociente:\n\\[\n\\lambda = \\frac{d}{n}\n\\]\nSe asume que el número de eventos observados en cada grupo sigue una distribución de Poisson, con un valor esperado igual al producto de la tasa de incidencia por las personas-tiempo.\n\\[\nE(d) = \\lambda * n\n\\]\nAsí, la probabilidad de observar \\(d\\) eventos se calcula como:\n\\[\nP(x=d)= \\frac{(\\lambda n)^d e^{-\\lambda n}}{d}  \n\\]\nOmitiendo el desarrollo matemático, la regresión de Poisson consiste en establecer un modelo en el que diferentes covariables tienen un efecto lineal sobre el logaritmo de la tasa de incidencia del subgrupo correspondiente. Generalmente, el modelo de Poisson se expresa como:\n\\[\nln\\lambda = \\alpha+\\beta_1x_1+\\beta_2x_2+\\dots+\\beta_nx_n \\qquad (*)\n\\]\nEn forma equivalente, podemos expresar la tasa de incidencia como:\n\\[\n\\lambda=e^{\\beta_0+\\beta_1x_1+\\dots+\\beta_nx_n}\n\\] Siguiendo un razonamiento análogo al de la regresión logística, los parámetros del modelo pueden interpretarse como riesgos relativos (cocientes de tasas), representados como potencias de base \\(e\\). Formalmente, el riesgo relativo entre dos niveles de exposición se expresa como:\\[\nRR_{x*/x}=e^{\\sum^k_i=1\\beta_i(x^*_i-x_i)}=\\prod^k_{i=1}e^{\\beta_i(x^*_i-x_i)}  \n\\]Donde el símbolo \\(\\prod\\) (productoria) implica una secuencia de productos.\nVolviendo a la ecuación de la tasa de incidencia:\n\\[\n\\lambda = \\frac{d}{n}\n\\]\nSi aplicamos logaritmo natural:\n\\[\nln(\\lambda)=ln \\frac{d}{n}=ln(d)-ln(n)\n\\] Igualando con la ecuación \\((*)\\) tenemos:\n\\[\nln\\lambda=\\alpha+\\beta_1x_1+\\beta_2x_2+\\dots+\\beta_nx_n\n\\] \\[\nln(d)=ln(n)+\\alpha+\\beta_1x_1+\\beta_2x_2+\\dots+\\beta_nx_n  \n\\]Al término \\(ln(n)\\) se lo denomina offset. En general, es un valor que debemos proporcionarle al software para ajustar un modelo de Poisson y no se estima a partir de los datos.\nCon la ecuación del modelo establecida, el siguiente paso es estimar los coeficientes y evaluar la calidad del ajuste. Los coeficientes se calculan utilizando métodos como la Estimación de Máxima Verosimilitud (MLE). A partir de estos coeficientes, podemos realizar inferencias, como el test de Wald para evaluar la hipótesis nula (\\(H_0 : \\beta_i = 0\\)), calcular los intervalos de confianza (IC), y obtener los riesgos relativos (RR) con sus respectivos IC. La bondad de ajuste del modelo se evaluará a través de la función Deviance siguiendo el mismo esquema jerárquico de evaluación que en el caso de la regresión logística."
  },
  {
    "objectID": "unidad_5/02_reg_poisson.html#supuestos-del-modelo-de-poisson",
    "href": "unidad_5/02_reg_poisson.html#supuestos-del-modelo-de-poisson",
    "title": "Regresión de Poisson",
    "section": "Supuestos del modelo de Poisson",
    "text": "Supuestos del modelo de Poisson\nA modo de resumen: el modelo de Poisson se utiliza para modelar el conteo de eventos que ocurren en intervalos de tiempo, en poblaciones, o incluso en espacios geográficos. La variable respuesta en este modelo toma valores enteros y positivos, y depende de un solo parámetro (\\(\\lambda\\)).\nLos principales supuestos del modelo son:\n\nIndependencia de las observaciones: Cada observación debe ser independiente de las demás. Esto significa que la ocurrencia de un evento no debe influir en la probabilidad de que ocurran otros eventos.\nConstancia del parámetro \\(\\lambda\\) a lo largo del tiempo: Para que se cumpla este supuesto, la media y la varianza deben ser iguales (equidispersión). Si \\(\\lambda\\) no es constante, el modelo puede no ser adecuado.\nProporcionalidad de eventos al tamaño del intervalo: La cantidad de eventos en un intervalo dado debe ser proporcional al tamaño del intervalo. Esto significa que si duplicamos el intervalo, esperaríamos aproximadamente el doble de eventos.\nMutuamente excluyentes en el tiempo: No pueden ocurrir dos o más eventos en el mismo instante puntual. Cada evento debe ser individual y ocurrir en un momento distinto.\n\nUna característica fundamental de la distribución de Poisson es que la media y la varianza deben ser iguales, lo que se conoce como el supuesto de equidispersión. Cuando la varianza observada en los datos excede la varianza esperada bajo este modelo, se enfrenta a un problema de sobredispersión. Un método común para detectar sobredispersión es calcular el coeficiente de variación (CV), definido como el cociente entre la varianza y la media estimadas. La sobredispersión es frecuente en la práctica y puede afectar la precisión de las estimaciones de los errores estándar de los coeficientes, por lo que es fundamental diagnosticarla y tratarla adecuadamente.\nOtra situación en la que el modelo de Poisson puede no ser adecuado es cuando hay un número excesivo de ceros en los datos, es decir, cuando la frecuencia observada de ceros es mayor de lo que predice el modelo. Esto se debe a que el \\(ln(0)\\) no está definido. De manera particular, es posible que el mecanismo aleatorio que dio origen a los datos de conteo muestre una mayor concentración para algún valor específico, que puede ser el cero (como ocurre con algunas variables vinculadas a salud) o cualquier otro valor positivo. Esto implica que dicho valor tiene una mayor probabilidad de ocurrencia que la especificada por la distribución Poisson o cualquier otra distribución.\nEn el siguiente capítulo de esta unidad, exploraremos cómo detectar y manejar tanto la sobredispersión de los datos como el exceso de ceros, para asegurar que el modelo elegido sea el más adecuado para los datos analizados."
  },
  {
    "objectID": "unidad_5/02_reg_poisson.html#construcción-de-un-modelo-de-poisson-en-r",
    "href": "unidad_5/02_reg_poisson.html#construcción-de-un-modelo-de-poisson-en-r",
    "title": "Regresión de Poisson",
    "section": "Construcción de un modelo de Poisson en R",
    "text": "Construcción de un modelo de Poisson en R\nLa regresión de Poisson está implementada en R como parte de la familia de Modelos Lineales Generalizados (GLM). Esta técnica permite modelar variables de respuesta que representan conteos, asegurando que los valores predichos permanezcan dentro de límites razonables.\nUn criterio importante en la selección de la función de enlace dentro de los GLM es garantizar que los valores ajustados del modelo sean coherentes con la naturaleza de la variable respuesta. Para la distribución de Poisson, la función de enlace predeterminada es el logaritmo natural. Esta elección asegura que los recuentos ajustados sean siempre mayores o iguales a cero, lo cual es esencial en el contexto de datos de conteo.\nEn la práctica, esto significa que el modelo ajustado con errores de Poisson y una función de enlace logarítmica linealiza la relación entre la variable respuesta (conteos) y las variables independientes, permitiendo una interpretación clara y coherente de los resultados.\nComo es de imaginar utilizaremos la misma función general glm(), cambiando los argumentos en familia y enlace. La sintaxis básica de esta función, contenida en el paquete stats de R, es:\n\nglm(formula, family = poisson(link = \"log\"), data = datos)\n\nDado que el enlace logarítmico es el predeterminado para la familia de Poisson, podemos omitir su especificación:\n\nglm(formula, family = poisson, data = datos)\n\ndonde:\n\n\nformula: al igual que en los casos anteriores, describe la fórmula del modelo a ajustar con la estructura:\n\n\\[\nvariable\\_dependiente \\sim variable\\_indepen_1 + variable\\_indepen_2 +\\dots+ variable\\_indepen_n\n\\]\n\nfamily: indica la familia de distribuciones utilizadas y su función de enlace.\ndata: especifica el nombre de la base de datos (dataframe) que contiene las variables del modelo.\n\nPara obtener un resumen de los resultados del modelo se utiliza la función summary():\n\nsummary(modelo)\n\nEl resumen del objeto de regresión de Poisson incluye:\n\nCall: fórmula del modelo\nDeviance Residuals: muestra la distribución de los residuos (mediana, mínimo, máximo y percentilos 25-75) obtenidos en la última iteración\nCoefficients: Incluye los coeficientes del intercepto y de las variables independientes, junto con los errores estándar, el valor \\(z\\) (estadístico de Wald) y el \\(p\\)-valor correspondiente.\nDispersion parameter for poisson family taken to be 1: indica que el modelo asume el supuesto de equidispersión (media igual a varianza).\nNull deviance: devianza del modelo nulo, que solo incluye al intercepto.\nResidual deviance: devianza del modelo ajustado.\nAIC: criterio de información de Akaike.\nNumber of Fisher Scoring iterations: cantidad de iteraciones realizadas.\n\nEl objeto de regresión creado con glm() pertenece a las clases \"glm\" y \"lm\" y está compuesto por varios componentes que pueden ser accedidos usando el nombre del modelo seguido del signo $. Algunos de los componentes más relevantes son:\n\nmodelo$coefficients: Vector de coeficientes del modelo, también accesible mediante coef(modelo).\nmodelo$residuals: Vector de residuos obtenidos en la última iteración.\nmodelo$fitted.values: Valores ajustados medios, obtenidos mediante la transformación de los predictores lineales usando la inversa de la función de enlace.\nmodelo$family: Familia de distribuciones utilizada en la construcción del modelo.\nmodelo$deviance: Devianza del modelo ajustado.\nmodelo$aic: Criterio de información de Akaike (AIC).\nmodelo$null.deviance: Devianza del modelo nulo."
  },
  {
    "objectID": "unidad_5/02_reg_poisson.html#ejemplo-práctico-en-r",
    "href": "unidad_5/02_reg_poisson.html#ejemplo-práctico-en-r",
    "title": "Regresión de Poisson",
    "section": "Ejemplo práctico en R",
    "text": "Ejemplo práctico en R\nPara ilustrar las funciones y la metodología de análisis en R, utilizaremos el archivo de datos “cohorte_ocupacional.txt”, que contiene resultados de un estudio de cohortes que investiga la asociación entre las muertes respiratorias y la exposición al arsénico en la industria, ajustando por varios otros factores de riesgo.\nComenzaremos activando los paquetes necesarios:\n\n# chequeo de supuestos\nlibrary(easystats)\n\n# tablas regresión\nlibrary(gtsummary)\n\n# manejo de datos\nlibrary(tidyverse)\n\nCargamos la base de datos y exploramos su estructura:\n\n### Carga datos\ndatos &lt;- read_csv2(\"datos/cohorte_ocupacional.txt\")\n\n### Explora datos\nglimpse(datos)\n\nRows: 114\nColumns: 6\n$ muertes      &lt;dbl&gt; 2, 0, 0, 0, 2, 1, 0, 0, 0, 1, 1, 0, 2, 1, 0, 0, 7, 0, 0, …\n$ persona_anio &lt;dbl&gt; 30753, 4858, 4782, 3373, 39816, 6561, 1903, 125, 9368, 19…\n$ grupo_edad   &lt;chr&gt; \"40-49\", \"40-49\", \"40-49\", \"40-49\", \"40-49\", \"40-49\", \"40…\n$ periodo      &lt;chr&gt; \"1938-1949\", \"1938-1949\", \"1938-1949\", \"1938-1949\", \"1938…\n$ comienzo     &lt;chr&gt; \"1925 y post\", \"1925 y post\", \"1925 y post\", \"1925 y post…\n$ arsenico     &lt;chr&gt; \"&lt;1 año\", \"1-4 años\", \"5-14 años\", \"15+ años\", \"&lt;1 año\", …\n\n\nLas variables de estudio son:\n\nmuertes: Número de muertes por persona-años (persona_anio) en cada categoría. Nuestra variable de interés es la tasa de incidencia de mortalidad.\ngrupo_edad: Grupo de edad de los sujetos.\nperiodo: Período de empleo de los sujetos.\ncomienzo: Año de inicio del empleo.\narsenico: Nivel de exposición al arsénico durante el período de estudio.\n\nLas variables grupo_edad, periodo, comienzo y arsenico son de tipo carácter y debemos convertirlas a factor. Podemos hacer esto manualmente para cada variable, o usar el comando across() que permite modificar varias columnas a la vez:\n\ndatos &lt;- datos |&gt; \n  mutate(across(.cols = grupo_edad:arsenico, \n                .fns = ~ as.factor(.x)))\n\nRevisemos como quedaron los niveles de cada factor:\n\n# grupo etario\nlevels(datos$grupo_edad)\n\n[1] \"40-49\" \"50-59\" \"60-69\" \"70-79\"\n\n# periodo\nlevels(datos$periodo)\n\n[1] \"1938-1949\" \"1950-1959\" \"1960-1969\" \"1970-1977\"\n\n# comienzo\nlevels(datos$comienzo)\n\n[1] \"&lt; 1925\"      \"1925 y post\"\n\n# arsenico\nlevels(datos$arsenico)\n\n[1] \"&lt;1 año\"    \"1-4 años\"  \"15+ años\"  \"5-14 años\"\n\n\nAhora vamos a explorar los datos de la base organizando la información de persona-años por edad y período:\n\n## Creo tabla\ndatos  |&gt;  \n  count(periodo, grupo_edad, wt = persona_anio) |&gt; \n  pivot_wider(names_from = grupo_edad, values_from = n) |&gt;\n  \n  # formato de tabla\n  tibble()\n\n# A tibble: 4 × 5\n  periodo   `40-49` `50-59` `60-69` `70-79`\n  &lt;fct&gt;       &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1 1938-1949   92171   64211   40056   15072\n2 1950-1959  149493  102234   48960   18511\n3 1960-1969  161234  136627   75552   27240\n4 1970-1977   90730  115040   79366   33410\n\n\nAhora hacemos lo mismo para el número de muertes y calculamos la incidencia por 10000 personas-año para cada celda:\n\n## Creo objeto para personas-años\npersonas_anios &lt;- datos |&gt; \n  count(periodo, grupo_edad,\n        wt = persona_anio,\n        name = \"p.a\")\n\n### Creo tabla\ndatos |&gt; \n  count(periodo, grupo_edad, wt = muertes) |&gt; \n  left_join(personas_anios) |&gt; \n  mutate(incidencia = round(n/p.a*10000,2)) |&gt; \n  select(-n, -p.a) |&gt; \n  \n  tibble()\n\n# A tibble: 16 × 3\n   periodo   grupo_edad incidencia\n   &lt;fct&gt;     &lt;fct&gt;           &lt;dbl&gt;\n 1 1938-1949 40-49            0.54\n 2 1938-1949 50-59            1.71\n 3 1938-1949 60-69            3.5 \n 4 1938-1949 70-79            2.65\n 5 1950-1959 40-49            0.33\n 6 1950-1959 50-59            2.35\n 7 1950-1959 60-69            4.9 \n 8 1950-1959 70-79            6.48\n 9 1960-1969 40-49            0.43\n10 1960-1969 50-59            2.05\n11 1960-1969 60-69            5.82\n12 1960-1969 70-79            5.51\n13 1970-1977 40-49            0.44\n14 1970-1977 50-59            1.48\n15 1970-1977 60-69            4.41\n16 1970-1977 70-79            8.08\n\n\nPodemos visualizar la incidencia de muertes ajustadas por periodo y grupo de edad mediante un gráfico. Aunque el código exacto se omite por razones de extensión, los interesados pueden solicitarlo al equipo docente.\n\n\n\n\n\n\n\n\nEn el gráfico anterior, podemos observar que el grupo de mayor edad tiende a tener una incidencia más alta, lo que es consistente con la expectativa de que el riesgo de muerte aumenta con la edad. Sin embargo, para obtener una comprensión más precisa de esta relación, es crucial ajustar por otras covariables. La regresión de Poisson nos permitirá examinar cómo la exposición al arsénico y otros factores afectan la mortalidad, controlando simultáneamente por la edad y otros posibles factores de confusión.\nOffset\nDado que en este ejemplo nos interesa modelar la tasa de incidencia vamos a necesitar incorporar dentro del modelo un termino de desplazamiento (en inglés “offset”).\nCreamos el modelo saturado para trazar iteraciones backward:\n\nmodelo &lt;- glm(muertes ~ periodo + grupo_edad + arsenico, \n              offset = log(persona_anio),\n              family = poisson,\n              data = datos)\n\nEl término offset = log(persona_anio) en el modelo de regresión de Poisson permite que la variable persona_anio actúe como el denominador en el cálculo de las tasas de mortalidad. La transformación logarítmica es necesaria porque la función de enlace del modelo de Poisson es el logaritmo.\nPara obtener un resumen del modelo ajustado, utilizamos la función summary(), que proporciona una visión detallada de los resultados del modelo:\n\nsummary(modelo)\n\n\nCall:\nglm(formula = muertes ~ periodo + grupo_edad + arsenico, family = poisson, \n    data = datos, offset = log(persona_anio))\n\nCoefficients:\n                  Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)       -10.6363     0.2756 -38.594  &lt; 2e-16 ***\nperiodo1950-1959    0.4087     0.2126   1.923   0.0545 .  \nperiodo1960-1969    0.4413     0.2022   2.183   0.0290 *  \nperiodo1970-1977    0.3181     0.2063   1.542   0.1231    \ngrupo_edad50-59     1.4565     0.2458   5.925 3.12e-09 ***\ngrupo_edad60-69     2.3527     0.2392   9.837  &lt; 2e-16 ***\ngrupo_edad70-79     2.6024     0.2578  10.096  &lt; 2e-16 ***\narsenico1-4 años    0.7949     0.1582   5.024 5.06e-07 ***\narsenico15+ años    1.0468     0.1775   5.896 3.72e-09 ***\narsenico5-14 años   0.6070     0.2059   2.948   0.0032 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 376.02  on 113  degrees of freedom\nResidual deviance: 116.69  on 104  degrees of freedom\nAIC: 355.47\n\nNumber of Fisher Scoring iterations: 5\n\n\nEste primer modelo, que incluye todas las covariables, muestra que casi todos los niveles de las variables son significativos respecto a las categorías de referencia.\nBondad de ajuste\nPara evaluar la bondad de ajuste del modelo y verificar la presencia de sobredispersión, podemos utilizar la función check_overdispersion() del paquete performance. Esta función ayuda a identificar si los errores del modelo presentan una dispersión excesiva en comparación con la esperada bajo la distribución de Poisson:\n\ncheck_overdispersion(modelo)\n\n# Overdispersion test\n\n       dispersion ratio =   1.116\n  Pearson's Chi-Squared = 116.079\n                p-value =   0.197\n\n\nEl componente de Pearson's Chi-Squared se calcula a partir de la devianza del modelo. En las pruebas de bondad de ajuste, buscamos que el valor de p sea mayor a 0,05, lo cual indica que el modelo se ajusta adecuadamente a los datos.\nSelección de variables explicativas\nPara mejorar el modelo anterior y asegurarnos de que incluimos solo las variables significativas, procederemos a eliminar variables una por una en una primera ronda de iteración mediante un proceso backwards manual.\n\n# (-) arsénico\nmod1 &lt;- glm(muertes ~ periodo + grupo_edad, \n            offset = log(persona_anio),\n              family = poisson,\n              data = datos)\n\n# (-) grupo etario\nmod2 &lt;- glm(muertes ~ periodo + arsenico, \n            offset = log(persona_anio),\n              family = poisson,\n              data = datos)\n         \n# (-) periodo\nmod3 &lt;- glm(muertes ~ grupo_edad + arsenico, \n            offset = log(persona_anio),\n              family = poisson,\n              data = datos)     \n\nPara comparar los modelos podemos usar la función compare_perfomance() del paquete performance.\n\ncompare_performance(mod1, mod2, mod3, metrics = \"common\")\n\n# Comparison of Model Performance Indices\n\nName | Model | AIC (weights) | BIC (weights) | Nagelkerke's R2 |  RMSE\n----------------------------------------------------------------------\nmod1 |   glm | 396.6 (&lt;.001) | 415.8 (&lt;.001) |           0.877 | 2.208\nmod2 |   glm | 534.7 (&lt;.001) | 553.9 (&lt;.001) |           0.496 | 3.470\nmod3 |   glm | 355.0 (&gt;.999) | 374.2 (&gt;.999) |           0.926 | 1.595\n\n\nBuscamos el menor AIC, que en este caso es mod3 (modelo con variables grupo_edad y arsenico).\nEn la segunda fase de iteración partimos de este modelo y quitamos otra variable, lo que signica en nuestro ejemplo construir regresiones simples:\n\n# (-) arsénico\nmod4.1 &lt;- glm(muertes ~ grupo_edad, \n              offset = log(persona_anio),\n              family = poisson,\n              data = datos) \n\n# (-) grupo etario\nmod4.2 &lt;- glm(muertes ~ arsenico, \n              offset = log(persona_anio),\n              family = poisson,\n              data = datos) \n\nVolvemos a comparar:\n\ncompare_performance(mod3, mod4.1, mod4.2, metrics = \"common\")\n\n# Comparison of Model Performance Indices\n\nName   | Model | AIC (weights) | BIC (weights) | Nagelkerke's R2 |  RMSE\n------------------------------------------------------------------------\nmod3   |   glm | 355.0 (&gt;.999) | 374.2 (&gt;.999) |           0.926 | 1.595\nmod4.1 |   glm | 394.5 (&lt;.001) | 405.4 (&lt;.001) |           0.871 | 2.181\nmod4.2 |   glm | 537.9 (&lt;.001) | 548.8 (&lt;.001) |           0.451 | 3.416\n\n\nEl mejor modelo continúa siendo mod3, por lo que vamos a probar su bondad de ajuste:\n\ncheck_overdispersion(mod3)\n\n# Overdispersion test\n\n       dispersion ratio =   1.128\n  Pearson's Chi-Squared = 120.713\n                p-value =   0.172\n\n\nEl p valor del test confirma un buen ajuste. Procedemos a chequear significancia de las variables explicativas:\n\nsummary(mod3)\n\n\nCall:\nglm(formula = muertes ~ grupo_edad + arsenico, family = poisson, \n    data = datos, offset = log(persona_anio))\n\nCoefficients:\n                  Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)       -10.2972     0.2237 -46.032  &lt; 2e-16 ***\ngrupo_edad50-59     1.4624     0.2454   5.960 2.53e-09 ***\ngrupo_edad60-69     2.3502     0.2380   9.873  &lt; 2e-16 ***\ngrupo_edad70-79     2.5987     0.2564  10.136  &lt; 2e-16 ***\narsenico1-4 años    0.8041     0.1577   5.099 3.42e-07 ***\narsenico15+ años    0.9976     0.1758   5.673 1.40e-08 ***\narsenico5-14 años   0.5957     0.2059   2.893  0.00381 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 376.02  on 113  degrees of freedom\nResidual deviance: 122.25  on 107  degrees of freedom\nAIC: 355.04\n\nNumber of Fisher Scoring iterations: 5\n\n\nTodos los niveles de las variables son significativos respecto a sus niveles de referencia:\n\ntbl_regression(mod3, exponentiate = T)\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\nIRR\n95% CI\np-value\n\n\n\ngrupo_edad\n\n\n\n\n\n    40-49\n—\n—\n\n\n\n    50-59\n4.32\n2.72, 7.16\n&lt;0.001\n\n\n    60-69\n10.5\n6.73, 17.2\n&lt;0.001\n\n\n    70-79\n13.4\n8.27, 22.7\n&lt;0.001\n\n\narsenico\n\n\n\n\n\n    &lt;1 año\n—\n—\n\n\n\n    1-4 años\n2.23\n1.63, 3.02\n&lt;0.001\n\n\n    15+ años\n2.71\n1.90, 3.79\n&lt;0.001\n\n\n    5-14 años\n1.81\n1.19, 2.67\n0.004\n\n\n\nAbbreviations: CI = Confidence Interval, IRR = Incidence Rate Ratio\n\n\n\n\n\nTraducidos a riesgo, mediante la exponenciación de los coeficientes, observamos que los diferentes categorías de años de exposición al arsénico no varían entre sí. Esto nos hace pensar que puede valer la pena agruparlo en solo dos niveles:\n\n## Recategoriza arsénico\ndatos &lt;- datos %&gt;% \n  mutate(arsenico_cat = if_else(\n    arsenico == \"&lt;1 año\", \"&lt;1 año\", \"1+ años\") |&gt; \n      as.factor())\n\n## Modelo con arsénico recategorizado\nmod5 &lt;- glm(muertes ~ grupo_edad + arsenico_cat,\n                offset = log(persona_anio), \n               family = poisson, \n               data = datos)\n\n## Compara modelos\ncompare_performance(mod3, mod5, metrics = \"common\")\n\n# Comparison of Model Performance Indices\n\nName | Model | AIC (weights) | BIC (weights) | Nagelkerke's R2 |  RMSE\n----------------------------------------------------------------------\nmod3 |   glm | 355.0 (0.350) | 374.2 (0.034) |           0.926 | 1.595\nmod5 |   glm | 353.8 (0.650) | 367.5 (0.966) |           0.924 | 1.613\n\n\nEl mod5 tiene mejor performance que el modelo anterior. Chequeamos sobredispersión:\n\ncheck_overdispersion(mod5)\n\n# Overdispersion test\n\n       dispersion ratio =   1.147\n  Pearson's Chi-Squared = 125.038\n                p-value =    0.14\n\n\nEl ajuste continúa siendo bueno.\nChequeamos significancia:\n\nsummary(mod5)\n\n\nCall:\nglm(formula = muertes ~ grupo_edad + arsenico_cat, family = poisson, \n    data = datos, offset = log(persona_anio))\n\nCoefficients:\n                    Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)         -10.3112     0.2233 -46.169  &lt; 2e-16 ***\ngrupo_edad50-59       1.4702     0.2453   5.994 2.04e-09 ***\ngrupo_edad60-69       2.3661     0.2372   9.976  &lt; 2e-16 ***\ngrupo_edad70-79       2.6238     0.2548  10.297  &lt; 2e-16 ***\narsenico_cat1+ años   0.8109     0.1210   6.699 2.09e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 376.02  on 113  degrees of freedom\nResidual deviance: 125.02  on 109  degrees of freedom\nAIC: 353.8\n\nNumber of Fisher Scoring iterations: 5\n\n\nLas variables incluídas son significativas.\nEn esta etapa, aceptaríamos el mod5 como el mejor modelo, ya que tiene el AIC más pequeño entre todos los modelos que hemos probado.\n\ntbl_regression(mod5, exponentiate = T)\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\nIRR\n95% CI\np-value\n\n\n\ngrupo_edad\n\n\n\n\n\n    40-49\n—\n—\n\n\n\n    50-59\n4.35\n2.74, 7.21\n&lt;0.001\n\n\n    60-69\n10.7\n6.85, 17.4\n&lt;0.001\n\n\n    70-79\n13.8\n8.51, 23.2\n&lt;0.001\n\n\narsenico_cat\n\n\n\n\n\n    &lt;1 año\n—\n—\n\n\n\n    1+ años\n2.25\n1.77, 2.85\n&lt;0.001\n\n\n\nAbbreviations: CI = Confidence Interval, IRR = Incidence Rate Ratio\n\n\n\n\n\nInterpretación de los resultados\nConcluimos que las personas expuestas al arsénico durante al menos un año tienen un riesgo 2,25 veces mayor de muerte por enfermedad respiratoria en comparación con aquellas no expuestas. Para entender cómo se obtiene este cálculo, consideremos que en los modelos de Poisson, la estructura de compensación (offset) definida como log(persona_anio) transforma el resultado en el logaritmo de la densidad de incidencia.\nEl término offset = log(persona_anio) en el modelo de Poisson convierte el recuento de eventos en una densidad de incidencia, ajustada por el tiempo de seguimiento. Esto permite que el modelo estime tasas de incidencia de acuerdo con el tiempo de exposición.\nLos coeficientes del modelo de Poisson se interpretan en términos de tasas de incidencia. En particular, el coeficiente asociado a la variable de exposición al arsénico se transforma mediante la exponenciación para obtener el riesgo relativo. Si el coeficiente de la variable arsenico es \\(\\beta\\), el riesgo relativo (RR) se calcula como \\(e^\\beta\\). En nuestro caso, si \\(e^\\beta = 2,25\\), esto indica que las personas expuestas al arsénico durante al menos un año tienen un riesgo 2,25 veces mayor de muerte por enfermedad respiratoria en comparación con las no expuestas.\nDensidad de incidencia\nPara calcular la densidad de incidencia esperada en una población específica, utilizamos el modelo ajustado. Por ejemplo, para una población de 100.000 personas de entre 40 y 49 años expuestas al arsénico durante menos de un año, podemos usar el modelo mod5. En R, esto se hace creando un nuevo conjunto de datos con las características deseadas y utilizando la función predict() para obtener la tasa de incidencia ajustada:\n\n## Genera nuevos datos\nnewdata &lt;- tibble(grupo_edad = \"40-49\",\n                  arsenico_cat = \"&lt;1 año\",\n                  persona_anio = 100000)\n\n## Predice datos\npredict(object = mod5, newdata = newdata, type = \"response\") \n\n       1 \n3.325736 \n\n\nEsta población tendría una densidad de incidencia estimada de 33,26 por 100.000 personas-año.\nRiesgo relativo\nEn un estudio de casos y controles, como los vistos en la Unidad 4, el OR se utiliza para comparar la prevalencia de exposición entre casos y controles. En un estudio de cohortes, este valor es igual a la relación entre las probabilidades de contraer una enfermedad entre el grupo expuesto y el no expuesto. La razón de los riesgos para los dos grupos se denomina entonces “razón de riesgo” o riesgo relativo (RR).\nEn un estudio de cohorte real, los sujetos no siempre tienen la misma duración de seguimiento. El riesgo relativo ignora la duración del seguimiento. Por tanto, no es una buena medida de comparación del riesgo entre los dos grupos.\nRazón de densidad de incidencia\nEn este ejercicio, todos los sujetos agrupan sus tiempos de seguimiento y este número se denomina “tiempo-persona”, que luego se utiliza como denominador del evento, lo que da como resultado una “densidad de incidencia”.\nComparar la densidad de incidencia entre dos grupos de sujetos por su estado de exposición es más justo que comparar los riesgos crudos. La relación entre las densidades de incidencia de dos grupos se denomina razón de densidades de incidencia (RDI), que es una forma mejorada de riesgo relativo.\nEn el mod5, para calcular la razón de densidad de incidencia entre los sujetos expuestos al arsénico durante uno o más años frente a los expuestos durante menos de un año, podemos dividir la incidencia entre los primeros por la del segundo grupo.\n\n## Genera nuevos datos\nnewdata &lt;- tibble(grupo_edad = rep(\"40-49\", 2),\n                  arsenico_cat = c(\"&lt;1 año\", \"1+ años\"),\n                  persona_anio = rep(100000, 2))\n\n## Densidad de incidencia\ndi &lt;- predict(object = mod5, newdata = newdata, type = \"response\") \n\n## Razón densidad de incidencia\nrdi.arsenico &lt;- di[2] / di[1]\n\nrdi.arsenico\n\n       2 \n2.249864 \n\n\nEl código anterior comienza agregando una nueva fila al dataframe newdata que tiene lo mismo que la primera fila, excepto que la variable arsenico_cat es “1+ años”.\nA continuación, se calculan las respuestas o densidades de incidencia de las dos condiciones.\nLa RDI se obtiene luego de la división de las densidades de incidencia para arsenico_cat = “&lt;1 año” con arsenico_cat = \"1+ años\". Una forma más corta de obtenerla es mediante el coeficiente de la variable específica arsenico, utilizando R base o el paquete gtsummary.\n\ntbl_regression(mod5, exponentiate = T,                \n               include = \"arsenico_cat\")\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\nIRR\n95% CI\np-value\n\n\n\narsenico_cat\n\n\n\n\n\n    &lt;1 año\n—\n—\n\n\n\n    1+ años\n2.25\n1.77, 2.85\n&lt;0.001\n\n\n\nAbbreviations: CI = Confidence Interval, IRR = Incidence Rate Ratio\n\n\n\n\n\nTambién podemos mostrar la salida mediante un gráfico usando el ecosistema de paquetes easystats, tal como vimos en la unidad anterior:\n\nmodel_parameters(mod5, exponentiate = T) |&gt; \n  plot()\n\n\n\n\n\n\n\n\nDe Irala, Martínez-González, y Guillén Grima (2001)\nGordis (2017)\nHernández-Ávila (2011)\nKim (2019)\nKleinbaum et al. (1988)\nNolasco (2016)\nPérez Hoyos (2001)\nSalinas-Rodríguez, Manrique-Espinoza, y Sosa-Rubí (2009)\n(Wickham et al. 2019; Lüdecke et al. 2022; Sjoberg et al. 2021)"
  },
  {
    "objectID": "unidad_6/01_est_experimentales.html",
    "href": "unidad_6/01_est_experimentales.html",
    "title": "Estudios Experimentales",
    "section": "",
    "text": "En esta última unidad abordaremos los diseños experimentales. En epidemiología, un estudio experimental implica que el investigador manipula activamente las condiciones del estudio con el objetivo de evaluar el efecto de una intervención sobre las observaciones realizadas. La aleatorización constituye una etapa clave, ya que permite asignar la exposición de manera intencional y al azar. En cuanto a su temporalidad, estos estudios son de carácter longitudinal y prospectivo.\nLos estudios experimentales presentan una alta capacidad para establecer relaciones causales. De hecho, los meta-análisis de ensayos clínicos aleatorizados constituyen el nivel más alto de evidencia científica. Sin embargo, su implementación suele estar limitada por consideraciones éticas, legales y operativas. Por eso, su aplicación más frecuente se da en la evaluación de fármacos y otras intervenciones sanitarias, especialmente en el contexto de ensayos clínicos. También se utilizan en investigaciones comunitarias orientadas a evaluar la efectividad de programas de salud pública.\nAunque el rasgo distintivo de un estudio experimental es la introducción de una intervención asignada por el investigador, para que se lo considere científicamente válido deben cumplirse las siguientes condiciones:\n\nLa única razón por la que los sujetos reciben la intervención es el cumplimiento del protocolo del estudio.\nEl estudio incluye un grupo control que no recibe la intervención bajo análisis.\nLa asignación de los sujetos al grupo de intervención se realiza mediante un mecanismo aleatorio."
  },
  {
    "objectID": "unidad_6/01_est_experimentales.html#introducción",
    "href": "unidad_6/01_est_experimentales.html#introducción",
    "title": "Estudios Experimentales",
    "section": "",
    "text": "En esta última unidad abordaremos los diseños experimentales. En epidemiología, un estudio experimental implica que el investigador manipula activamente las condiciones del estudio con el objetivo de evaluar el efecto de una intervención sobre las observaciones realizadas. La aleatorización constituye una etapa clave, ya que permite asignar la exposición de manera intencional y al azar. En cuanto a su temporalidad, estos estudios son de carácter longitudinal y prospectivo.\nLos estudios experimentales presentan una alta capacidad para establecer relaciones causales. De hecho, los meta-análisis de ensayos clínicos aleatorizados constituyen el nivel más alto de evidencia científica. Sin embargo, su implementación suele estar limitada por consideraciones éticas, legales y operativas. Por eso, su aplicación más frecuente se da en la evaluación de fármacos y otras intervenciones sanitarias, especialmente en el contexto de ensayos clínicos. También se utilizan en investigaciones comunitarias orientadas a evaluar la efectividad de programas de salud pública.\nAunque el rasgo distintivo de un estudio experimental es la introducción de una intervención asignada por el investigador, para que se lo considere científicamente válido deben cumplirse las siguientes condiciones:\n\nLa única razón por la que los sujetos reciben la intervención es el cumplimiento del protocolo del estudio.\nEl estudio incluye un grupo control que no recibe la intervención bajo análisis.\nLa asignación de los sujetos al grupo de intervención se realiza mediante un mecanismo aleatorio."
  },
  {
    "objectID": "unidad_6/01_est_experimentales.html#tipos-de-estudios-clínicos-experimentales-ece",
    "href": "unidad_6/01_est_experimentales.html#tipos-de-estudios-clínicos-experimentales-ece",
    "title": "Estudios Experimentales",
    "section": "Tipos de estudios clínicos experimentales (ECE)",
    "text": "Tipos de estudios clínicos experimentales (ECE)\nExisten distintas clasificaciones de estudios experimentales según diversos autores, aunque en esencia apuntan a lo mismo. Por ejemplo, Hernández-Ávila (2011) propone la siguiente clasificación:\n\n\n\n\nTipo de estudio\nObjetivos\nDuración\n\n\n\nLaboratorio\nPrueba hipótesis etiológicas y estima respuestas biológicas y/o de comportamiento agudas. Sugiere eficacia de una intervención para modificar factores de riesgo en una población\nHoras o días\n\n\nEnsayo clínico\nPrueba hipótesis etiológicas y estima efectos de salud a largo plazo. Prueba eficacia de intervenciones que modifican el estado de salud Sugiere factibilidad de intervenciones\nDías hasta años\n\n\nIntervención comunitaria\nIdentifica personas de alto riesgo. Prueba eficacia y efectividad de intervenciones clínicas/sociales que modifican el estado de salud dentro de poblaciones particulares Sugiere políticas y programas de salud pública\nNo menos de 6 meses\n\n\n\n\n\nOtras clasificaciones más amplias incluyen estudios donde no se cumplen estrictamente las tres condiciones mencionadas anteriormente, que apuntan a descartar que los efectos observados puedan deberse a factores desconocidos o no controlados. Estos estudios, denominados cuasi-experimentales, también implican una intervención, pero carecen del rigor metodológico necesario para asegurar la validez interna. Por lo tanto, presentan limitaciones para atribuir el efecto observado exclusivamente a la intervención. Esta clasificación puede resumirse de la siguiente forma:\n\n\n\n\nTipo de estudio\nSubtipo\nDescripción general\n\n\n\nEstudios cuasi-experimentales\nNo controlados\nNo se incluye grupo de comparación\n\n\n\nCon controles históricos\nComparación con datos del pasado\n\n\n\nControlados no aleatorizados\nComparación con grupo contemporáneo, pero sin asignación aleatoria\n\n\nEstudios controlados y aleatorizados\nEnsayo de prevención primaria (ensayo de campo)\nEvalúan intervenciones preventivas en poblaciones sanas\n\n\n\nEnsayo de intervención comunitaria\nEvalúan programas a nivel grupal o comunitario\n\n\n\nEnsayo clínico de grupos paralelos\nCada grupo recibe una intervención distinta de forma simultánea\n\n\n\nEnsayo clínico cruzado\nCada sujeto recibe más de una intervención en distintos períodos\n\n\n\nEnsayo clínico factorial\nEvalúa dos o más intervenciones simultáneamente en distintas combinaciones\n\n\n\n\n\nEn esta unidad nos enfocaremos principalmente en los ensayos clínicos controlados, pero pueden consultar la bibliografía para explorar los otros diseños. Como en todo estudio epidemiológico, es fundamental adoptar medidas que minimicen el riesgo de sesgos. Habitualmente se consideran 5 principios para evitar sesgos:\n\nUso de grupo control\nAleatorización\nCegamiento\nConsentimiento antes de aleatorizar\nAnálisis por intención de tratar\n\nUso de grupo control\nAl igual que en los estudios de cohortes, los estudios de intervención deben incluir un grupo control con el cual comparar el grupo tratado. Habitualmente se utilizan dos grupos, aunque puede haber más. El grupo control puede recibir un placebo o un tratamiento activo alternativo, como la terapia existente.\nAleatorización\nSu objetivo principal es garantizar que las diferencias observadas entre grupos al final del estudio no se deban a factores distintos a la intervención. La aleatorización se refiere a la asignación mediante el azar de las unidades de investigación (individuos) a los distintos tratamientos a evaluar, con la finalidad de compararlos según las variables resultado definidas.\nCegamiento\nEs la política de mantener a alguien sin saber qué tratamiento se ha administrado. Existen distintas categorías de cegamiento:\n\nEn estudios simple ciego, el sujeto ignora el tratamiento recibido.\nEn estudios doble ciego, tanto el sujeto como quien evalúa los resultados desconocen la asignación.\nEn estudios triple ciego, incluso quien analiza los datos (por ejemplo, un/a estadístico/a) también lo desconoce.\n\nEl cegamiento reduce el sesgo del observador, aunque no siempre es posible. Un ejemplo claro es cuando se compara un tratamiento con radiación con un tratamiento quirúrgico para el cáncer de mama. Por otro lado, es fundamental que la asignación del tratamiento esté codificada y que alguien pueda descifrar el código en momentos de problemas médicos durante el estudio y cuando se requieran resultados finales.\nConsentimiento antes de la aleatorización\nPara evitar sesgos en la composición de los grupos, el consentimiento informado y la elegibilidad de los sujetos para cada tratamiento deben verificarse antes de asignar aleatoriamente a los participantes.\nAnálisis por intención de tratar\nDurante el estudio de intervención, los sujetos pueden suspender o modificar su tratamiento asignado por muchas razones, que incluyen enfermedad, toxicidad, migración o simplemente por error. Este enfoque analiza a los sujetos según el tratamiento asignado, independientemente de su adherencia. Esto evita sesgos debidos a interrupciones o cambios de tratamiento, y refleja mejor lo que ocurre en la práctica clínica. No obstante, puede subestimar la diferencia real entre tratamientos. En algunos casos, un análisis ITT completo no es factible."
  },
  {
    "objectID": "unidad_6/01_est_experimentales.html#análisis-de-los-estudios-experimentales",
    "href": "unidad_6/01_est_experimentales.html#análisis-de-los-estudios-experimentales",
    "title": "Estudios Experimentales",
    "section": "Análisis de los estudios experimentales",
    "text": "Análisis de los estudios experimentales\nEn un estudio experimental, se realiza un seguimiento de una cohorte. Lo más habitual en ensayos clínicos controlados es medir la incidencia de un evento en ambos grupos durante un período definido. Los eventos se categorizan de manera dicotómica (presencia/ausencia) y se presentan como la proporción de sujetos que presentan el desenlace (por ejemplo, infarto, recurrencia, muerte). Al tratarse de una cohorte, es posible calcular el riesgo relativo (RR), la diferencia de riesgos o la reducción relativa del riesgo. También se utiliza el número necesario a tratar (NNT), que indica cuántos pacientes deben recibir la intervención para evitar un caso adicional.\nUna metodología frecuentemente utilizada es el análisis de supervivencia, donde el tiempo hasta la ocurrencia del evento cobra un rol central. Como ya se vio en la unidad anterior, esta técnica también puede aplicarse a estudios de cohortes. A continuación avanzaremos en su comprensión.\n\nEscuela Nacional de Sanidad (ENS). Instituto de Salud Carlos III. Ministerio de Ciencias e Innovación. Madrid (2009)\nWoodward (2005)"
  },
  {
    "objectID": "unidad_6/03_metodos_analisis_sup.html",
    "href": "unidad_6/03_metodos_analisis_sup.html",
    "title": "Métodos para el análisis de supervivencia",
    "section": "",
    "text": "En el análisis de supervivencia, se busca modelar una variable aleatoria continua y no negativa, denotada por \\(T\\), que representa el “tiempo hasta un evento determinado” para cada individuo del estudio. A continuación, se describen sus cuatro funciones fundamentales.\n\n\nComo toda variable aleatoria continua, \\(T\\) posee una función de densidad de probabilidad, \\(f(t)\\), que ya presentamos en la primera parte. Esta función puede interpretarse como la probabilidad de que un individuo experimente el evento en un intervalo instantáneo de tiempo. Asociada a ella, también se encuentra la función de distribución acumulada \\(F(t)\\), que representa la probabilidad de que el evento ocurra en un intervalo determinado: [\\(t_1\\), \\(t_2\\)].\n\n\n\nLa función de supervivencia, \\(S(t)\\), indica la probabilidad de que un individuo no experimente el evento hasta al menos el tiempo \\(t\\), es decir, que “sobreviva” hasta ese momento.\n\n\n\nEn el contexto del análisis de supervivencia, suele mantenerse el término en inglés hazard, aunque su traducción aproximada es “riesgo”, ya que asume ciertas particularidades. Esta función permite analizar el riesgo instantáneo de que un individuo experimente el evento entre el tiempo \\(t\\) y \\(t+\\varepsilon\\), dado que sobrevivió hasta el tiempo \\(t\\). Por ejemplo, para responder a preguntas como: ¿cuál es el riesgo de un paciente de fallecer después de una cirugía a corazón abierto?, deberemos conocer esta función de riesgo.\nSe simboliza como \\(h(t)\\) o \\(\\lambda(t)\\) y la expresión es:\n\\[\nh(t) = \\lim_{n\\rightarrow \\infty}((t&lt;T&lt;t+\\varepsilon)|T&gt;t)/\\varepsilon\n\\]\nEsta expresión puede interpretarse, aproximadamente, como la probabilidad de morir en el intervalo (\\(t\\), \\(t + \\Delta t\\)], dado que se está vivo al tiempo \\(t\\), o bien de forma general como la probabilidad de presentar el evento en dicho intervalo. Podemos interpretarla como una medida de intensidad de muerte al tiempo \\(t\\), o una medida de muerte potencial al tiempo \\(t\\), o en forma general, como una medida de intensidad de eventos a dicho tiempo.\nLa función de riesgo recibe otros nombres como: tasa de fallo, función o tasa de incidencia, fuerza de mortalidad condicional o simplemente fuerza de mortalidad. Es importante resaltar que a pesar de llevar el nombre de riesgo; \\(h(t)\\) es una tasa y no una probabilidad, es por eso que a veces se prefiere usar el término hazard. Su unidad es \\(tiempo^{-1}\\) y puede asumir cualquier valor real mayor que cero.\nUna de sus principales utilidades es guiar la elección de un modelo paramétrico adecuado para los datos de supervivencia. En muchos contextos, el investigador tiene una noción previa sobre cómo varía el riesgo a lo largo del tiempo, lo que puede facilitar la elección de una función específica para \\(h(t)\\). Para entender un poco mejor esto último observemos detenidamente la siguiente figura:\n\n\n\n\n\n\n\n\nFigura 1: Funciones de riesgo\n\n\n\n\n\n\nEl gráfico A muestra una función de riesgo constante en el tiempo. Por ejemplo: riesgo de fracturas de escolares de 5to grado durante un año.\nEl gráfico B muestra una función de riesgo que decrece en el tiempo. Por ejemplo: el riesgo post-quirúrgico, que es mayor en los primeros días post cirugía y luego cae con el tiempo.\nEl gráfico C muestra una función de riesgo que se incrementa con el tiempo. Por ejemplo: si estudiamos el “tiempo hasta la solidificación de una fractura ósea”, en un primer momento el “riesgo de solidificación” de la fractura es nulo, y a medida que transcurre el tiempo se incrementa.\nLos gráficos D-F presentan patrones más complejos de riesgo que pueden representar situaciones específicas.\n\nPara cerrar diremos que la función de riesgo marca la dinámica del proceso estudiado, al dar sus valores una adecuada aproximación a la tasa de incidencia del evento de interés.\n\n\n\nA partir de la función de riesgo (\\(h(t)\\)) es posible calcular la función de riesgo acumulado, que se simboliza como \\(\\Lambda(t)\\) o \\(H(t)\\). Esta función mide el riesgo de ocurrencia de un evento hasta un determinado tiempo \\(t\\). Matemáticamente implica la suma de todos los riesgos en todos los tiempos hasta el tiempo \\(t\\).\nLa expresión matemática es:\n\\[\nH(t)=\\int_0^t h(u)du\n\\]\nPor ejemplo, permite responder preguntas como: ¿Cuál es el riesgo acumulado de morir en el primer año luego del diagnóstico de SIDA? ¿Y en los primeros dos años?\nLas funciones fundamentales del análisis de supervivencia están estrechamente relacionadas entre sí y son matemáticamente equivalentes. A continuación se resumen sus principales relaciones, omitiremos aquí dichas deducciones, pero hagan un acto de fe y crean en las siguientes equivalencias:\n\\(S(t)=1-F(t)\\)\n\\(h(t)=-\\frac{dln(S(t))}{dt}\\)\n\\(h(t)=\\frac{f(t)}{S(t)}\\)\n\\(h(t)=\\frac{f(t)}{1-F(t)}\\)\n\\(H(t)=-ln(S(t))\\)\n\\(S(t)=exp(-H(t))\\)\nUna vez que estimamos una de ellas, será posible obtener cualquiera de las demás."
  },
  {
    "objectID": "unidad_6/03_metodos_analisis_sup.html#funciones-básicas",
    "href": "unidad_6/03_metodos_analisis_sup.html#funciones-básicas",
    "title": "Métodos para el análisis de supervivencia",
    "section": "",
    "text": "En el análisis de supervivencia, se busca modelar una variable aleatoria continua y no negativa, denotada por \\(T\\), que representa el “tiempo hasta un evento determinado” para cada individuo del estudio. A continuación, se describen sus cuatro funciones fundamentales.\n\n\nComo toda variable aleatoria continua, \\(T\\) posee una función de densidad de probabilidad, \\(f(t)\\), que ya presentamos en la primera parte. Esta función puede interpretarse como la probabilidad de que un individuo experimente el evento en un intervalo instantáneo de tiempo. Asociada a ella, también se encuentra la función de distribución acumulada \\(F(t)\\), que representa la probabilidad de que el evento ocurra en un intervalo determinado: [\\(t_1\\), \\(t_2\\)].\n\n\n\nLa función de supervivencia, \\(S(t)\\), indica la probabilidad de que un individuo no experimente el evento hasta al menos el tiempo \\(t\\), es decir, que “sobreviva” hasta ese momento.\n\n\n\nEn el contexto del análisis de supervivencia, suele mantenerse el término en inglés hazard, aunque su traducción aproximada es “riesgo”, ya que asume ciertas particularidades. Esta función permite analizar el riesgo instantáneo de que un individuo experimente el evento entre el tiempo \\(t\\) y \\(t+\\varepsilon\\), dado que sobrevivió hasta el tiempo \\(t\\). Por ejemplo, para responder a preguntas como: ¿cuál es el riesgo de un paciente de fallecer después de una cirugía a corazón abierto?, deberemos conocer esta función de riesgo.\nSe simboliza como \\(h(t)\\) o \\(\\lambda(t)\\) y la expresión es:\n\\[\nh(t) = \\lim_{n\\rightarrow \\infty}((t&lt;T&lt;t+\\varepsilon)|T&gt;t)/\\varepsilon\n\\]\nEsta expresión puede interpretarse, aproximadamente, como la probabilidad de morir en el intervalo (\\(t\\), \\(t + \\Delta t\\)], dado que se está vivo al tiempo \\(t\\), o bien de forma general como la probabilidad de presentar el evento en dicho intervalo. Podemos interpretarla como una medida de intensidad de muerte al tiempo \\(t\\), o una medida de muerte potencial al tiempo \\(t\\), o en forma general, como una medida de intensidad de eventos a dicho tiempo.\nLa función de riesgo recibe otros nombres como: tasa de fallo, función o tasa de incidencia, fuerza de mortalidad condicional o simplemente fuerza de mortalidad. Es importante resaltar que a pesar de llevar el nombre de riesgo; \\(h(t)\\) es una tasa y no una probabilidad, es por eso que a veces se prefiere usar el término hazard. Su unidad es \\(tiempo^{-1}\\) y puede asumir cualquier valor real mayor que cero.\nUna de sus principales utilidades es guiar la elección de un modelo paramétrico adecuado para los datos de supervivencia. En muchos contextos, el investigador tiene una noción previa sobre cómo varía el riesgo a lo largo del tiempo, lo que puede facilitar la elección de una función específica para \\(h(t)\\). Para entender un poco mejor esto último observemos detenidamente la siguiente figura:\n\n\n\n\n\n\n\n\nFigura 1: Funciones de riesgo\n\n\n\n\n\n\nEl gráfico A muestra una función de riesgo constante en el tiempo. Por ejemplo: riesgo de fracturas de escolares de 5to grado durante un año.\nEl gráfico B muestra una función de riesgo que decrece en el tiempo. Por ejemplo: el riesgo post-quirúrgico, que es mayor en los primeros días post cirugía y luego cae con el tiempo.\nEl gráfico C muestra una función de riesgo que se incrementa con el tiempo. Por ejemplo: si estudiamos el “tiempo hasta la solidificación de una fractura ósea”, en un primer momento el “riesgo de solidificación” de la fractura es nulo, y a medida que transcurre el tiempo se incrementa.\nLos gráficos D-F presentan patrones más complejos de riesgo que pueden representar situaciones específicas.\n\nPara cerrar diremos que la función de riesgo marca la dinámica del proceso estudiado, al dar sus valores una adecuada aproximación a la tasa de incidencia del evento de interés.\n\n\n\nA partir de la función de riesgo (\\(h(t)\\)) es posible calcular la función de riesgo acumulado, que se simboliza como \\(\\Lambda(t)\\) o \\(H(t)\\). Esta función mide el riesgo de ocurrencia de un evento hasta un determinado tiempo \\(t\\). Matemáticamente implica la suma de todos los riesgos en todos los tiempos hasta el tiempo \\(t\\).\nLa expresión matemática es:\n\\[\nH(t)=\\int_0^t h(u)du\n\\]\nPor ejemplo, permite responder preguntas como: ¿Cuál es el riesgo acumulado de morir en el primer año luego del diagnóstico de SIDA? ¿Y en los primeros dos años?\nLas funciones fundamentales del análisis de supervivencia están estrechamente relacionadas entre sí y son matemáticamente equivalentes. A continuación se resumen sus principales relaciones, omitiremos aquí dichas deducciones, pero hagan un acto de fe y crean en las siguientes equivalencias:\n\\(S(t)=1-F(t)\\)\n\\(h(t)=-\\frac{dln(S(t))}{dt}\\)\n\\(h(t)=\\frac{f(t)}{S(t)}\\)\n\\(h(t)=\\frac{f(t)}{1-F(t)}\\)\n\\(H(t)=-ln(S(t))\\)\n\\(S(t)=exp(-H(t))\\)\nUna vez que estimamos una de ellas, será posible obtener cualquiera de las demás."
  },
  {
    "objectID": "unidad_6/03_metodos_analisis_sup.html#modelos-paramétricos",
    "href": "unidad_6/03_metodos_analisis_sup.html#modelos-paramétricos",
    "title": "Métodos para el análisis de supervivencia",
    "section": "Modelos paramétricos",
    "text": "Modelos paramétricos\nEn esencia, cualquier distribución definida sobre los números reales no negativos puede utilizarse para modelar el tiempo hasta la ocurrencia de un evento. Sin embargo, en la práctica, solo un pequeño conjunto de distribuciones es ampliamente utilizado en análisis de supervivencia. Las más frecuentes son la exponencial, la Weibull y la lognormal.\n\nDistribución exponencial\nAl asumir que la variable aleatoria T posee una distribución exponencial, su \\(f(t)\\) será:\n\\[\nf(t) = \\alpha e^{-\\alpha t} \\quad (\\alpha&gt;0)\n\\]\nUtilizando las relaciones entre las funciones básicas de supervivencia se deducen:\n\\(S(t)=e^{-\\alpha t}\\)\n\\(h(t)=\\frac{f(t)}{S(t)}=\\frac{\\alpha e^{-\\alpha t}}{e^{-\\alpha t}}\\)\n\\(H(t) = -lnS(t)=\\alpha t\\)\nComo podemos ver, al asumir una distribución exponencial para la variable “tiempo hasta que el evento se produzca”, el riesgo es constante en el tiempo y el riesgo acumulado es una función lineal del tiempo:\n\n\n\n\n\n\n\n\nFigura 2: Funciones de supervivencia, riesgo y riesgo acumulado para la distribución exponencial.\n\n\n\n\n\nAsumir una distribución exponencial para los tiempos de sobrevida es similar a suponer una distribución normal para otras variables: se trata de una elección que facilita el modelado. Sin embargo, en la práctica es una situación muy rara. Como mencionamos anteriormente, una posible excepción sería el riesgo de fractura en escolares de 5º grado a lo largo de un año escolar. Pero no existen demasiadas situaciones en salud donde el riesgo sea constante, en la mayoría de las situaciones el riesgo varía en el tiempo, usualmente incrementándose con la edad del individuo.\nUno de los parámetros clave en el análisis de supervivencia es la mediana de los tiempos de supervivencia, es decir, el tiempo en el cual el 50% de los individuos ha experimentado el evento. Para el caso exponencial:\n\\(S(t)=e^{-\\alpha t}=0,5\\)\n\\(-\\alpha t=ln(0,5)\\)\n\\(\\alpha t=ln(2)\\)\n\\(T_{mediano}=\\frac{ln(2)}{\\alpha}\\)\nA pesar de su simplicidad, la suposición de un riesgo constante limita seriamente la aplicabilidad del modelo exponencial en contextos de salud.\n\n\nDistribución Weibull\nLa distribución Weibull es una generalización de la exponencial, ampliamente utilizada en contextos biomédicos. Su función de densidad es:\n\\[\nf(t)=\\gamma\\alpha^\\gamma t^{\\gamma-1}exp(-(\\alpha t)^\\gamma) \\qquad (\\alpha&gt;0, \\gamma&gt;0)\n\\]\nA partir de esta expresión, se deducen:\n\\(S(t)=exp(-(\\alpha t)^\\gamma)\\)\n\\(h(t) = \\gamma\\alpha^\\gamma t^{\\gamma-1}\\)\n\\(H(t)=-lnS(t)=(\\alpha t)^{\\gamma-1}\\)\nLa distribución Weibull tiene dos parámetros:\n\n\\(\\alpha\\), que regula la escala del modelo\n\\(\\gamma\\), que controla la forma de la función de riesgo:\n\nSi \\(\\gamma &lt; 1\\), la función de riesgo es decreciente\nSi \\(\\gamma&gt;1\\), la función de riesgo es creciente\nSi \\(\\gamma = 1\\), la función de riesgo es constante y equivale a una distribución exponencial\n\n\n\n\n\n\n\n\n\n\nFigura 3: Funciones de supervivencia, riesgo y riesgo acumulado para la distribución Weibull.\n\n\n\n\n\nPara encontrar el tiempo mediano de supervivencia, procedemos en forma análoga que en la distribución exponencial:\n\\(S(t)=exp(-(\\alpha t)^\\gamma)=0,5\\)\n\\(-(\\alpha t)^\\gamma=ln(0,5)\\)\n\\(T_{mediano}=\\frac{ln(2)^{1/\\gamma}}{\\alpha}\\)\nDejaremos de lado la distribución lognormal, para pasar ahora a los métodos no paramétricos de estimación."
  },
  {
    "objectID": "unidad_6/03_metodos_analisis_sup.html#métodos-no-paramétricos",
    "href": "unidad_6/03_metodos_analisis_sup.html#métodos-no-paramétricos",
    "title": "Métodos para el análisis de supervivencia",
    "section": "Métodos no paramétricos",
    "text": "Métodos no paramétricos\nA continuación, abordaremos dos métodos no paramétricos para estimar las funciones básicas de supervivencia. Ambos se basan en las denominadas tablas de vida, que describen el proceso de mortalidad de una cohorte hasta la desaparición del último de sus integrantes bajo la experiencia de mortalidad observada en un período determinado. Estas tablas finalizan con la muerte de todos los sujetos, y su principal diferencia radica en la velocidad con que se alcanza dicho final.\nLas tablas de vida se utilizan principalmente en salud pública para medir mortalidad y supervivencia, aunque también son aplicadas en estudios demográficos y actuariales, para analizar longevidad, fertilidad, migraciones, crecimiento poblacional, proyecciones y años de vida sin discapacidad. En resumen, es una presentación tabular del progreso de una cohorte a través del tiempo que transcurre.\nLas curvas de supervivencia pueden construirse mediante dos métodos: el método de Kaplan-Meier o el análisis actuarial de Cutler-Ederer. Ambos permiten estimar la supervivencia en presencia de censura. Además, existe un estimador no paramétrico de la función de riesgo acumulado, conocido como estimador de Nelson-Aalen, que no será abordado en este curso.\n\nMétodo actuarial\nEn este método, los tiempos de supervivencia se agrupan en intervalos, cuya duración puede variar según la frecuencia del evento de interés y puede no ser de la misma longitud. Una limitación de este enfoque es la menor precisión en muestras pequeñas. Sin embargo, en muestras grandes, su influencia sobre las estimaciones es reducida y permite estimar también la función de riesgo. Se asume que las observaciones censuradas se distribuyen homogéneamente dentro de cada intervalo.\nAntes de continuar con el ejemplo se sugiere ver la siguiente videoclase:\n➡️ Método Actuarial\nConsideremos el ejemplo de un estudio de supervivencia pos trasplante hepático en una cohorte de 300 pacientes, seguidos durante cinco años (2009-2014). La siguiente tabla (Tabla 1) presenta los datos iniciales:\n\n\n\n\nTabla 1: Información inicial para construir la tabla de supervivencia.\n\n\n\n\n\n\nAño ingreso\nAño\nVivos\nMuertes\nPérdidas\nMuertes por otras causas\nVivos al final del estudio\n\n\n\n\n2009\n2009\n60\n21\n4\n1\n18\n\n\n2009\n2010\n34\n6\n1\n0\n\n\n\n2009\n2011\n27\n4\n0\n1\n\n\n\n2009\n2012\n22\n2\n1\n0\n\n\n\n2009\n2013\n19\n1\n0\n0\n\n\n\n2010\n2010\n67\n23\n3\n2\n20\n\n\n2010\n2011\n39\n8\n2\n1\n\n\n\n2010\n2012\n28\n3\n1\n0\n\n\n\n2010\n2013\n24\n3\n0\n1\n\n\n\n2011\n2011\n53\n18\n3\n0\n23\n\n\n2011\n2012\n32\n5\n1\n0\n\n\n\n2011\n2013\n26\n2\n0\n1\n\n\n\n2012\n2012\n64\n22\n5\n2\n25\n\n\n2012\n2013\n35\n7\n2\n1\n\n\n\n2013\n2013\n56\n19\n2\n1\n34\n\n\n\n\n\n\n\n\n\n\nPara interpretar la información de la tabla, es necesario considerar que la misma presenta la evolución anual de los pacientes que fueron admitidos en el estudio año a año. En 2009 se incorporaron 60 pacientes, de los cuales 21 fallecieron, cuatro se perdieron en el seguimiento (se ignora si sobrevivieron o fallecieron) y uno murió por causas ajenas al trasplante. Estos últimos cinco pacientes corresponden a observaciones censuradas. Para el año 2010, 34 pacientes del grupo original y se sumaron 67 nuevos participantes al estudio.\nCada paciente que ingresa durante cada uno de los años del estudio contribuye con información a lo que ocurre durante el primer año de supervivencia:\n\nUn total de 300 pacientes ingresaron vivos al estudio: \\(60 + 67 + 53 + 64 + 56 = 300\\)\nDurante el primer año de seguimiento murieron 103 pacientes: \\(21 + 23 + 18 + 22 + 19 = 103\\)\n\nPara estimar la probabilidad de morir en un período, se divide el número de muertes en cada período por el número de individuos en riesgo al comienzo del periodo de observación.\nPero; ¿qué hacemos con los sujetos que se perdieron, es decir los individuos censurados? Al clasificar los datos en intervalos de tiempo discretos (como hemos hecho en nuestra tabla), no podremos decir en qué momento se han perdido exactamente: ¿diremos que han sido individuos en riesgo durante todo el año (o período? Lo que el método actuarial asume es una solución “salomónica”: las observaciones censuradas, que se contabilizan como media persona-año:\n\\[\nP(morir)=\\frac{muertes\\; en \\; el \\; período}{indiv.en\\;riesgo\\;al\\;comienzo\\;del\\;período-\\frac{indiv.censurados}{2}}\n\\]\nRecordemos que los individuos censurados serán quienes están vivos al final del periodo, los que se han perdido y quienes han muerto por causas ajenas al estudio. En nuestra nueva tabla (Tabla 2), esta probabilidad se notará como \\(q_i\\)\nPor lo tanto, para el primer periodo:\n\\[\nq_i = \\frac{103}{300-\\frac{23+34}{2}}=0,379\n\\]\nLa probabilidad de supervivencia condicional, \\(p_i\\), que denota la probabilidad de sobrevivir a un determinado periodo, se calcula como:\n\\[\np_i = 1 - q_i\n\\]\nLa probabilidad acumulada de supervivencia \\(S(t)\\), que muestra la probabilidad de sobrevivir hasta un tiempo determinado, considerando que se ha sobrevidido a los tiempos anteriores se obtiene multiplicando las probabilidades de supervivencia en los períodos anteriores. Es decir que la probabilidad de sobrevivir al tercer año a partir del trasplante será:\n\\[\nS(3) = 0,621 * 0,789 * 0,868 = 0,425~(42,5\\%)\n\\]\nEsto indica que el 42,5% de los pacientes sobreviven al menos tres años luego del trasplante:\n\n\n\n\nTabla 2: Tabla de supervivencia construida por el método actuarial.\n\n\n\n\n\n\nPeriodo\nVivos\nMuertes\nPérdidas/muertes otras causas\nVivos al final del estudio\nq\np\nS(t)\n\n\n\n\n1\n300\n103\n23\n34\n0.379\n0.621\n0.621\n\n\n2\n140\n26\n8\n25\n0.211\n0.789\n0.490\n\n\n3\n81\n9\n3\n23\n0.132\n0.868\n0.425\n\n\n4\n46\n5\n2\n20\n0.143\n0.857\n0.364\n\n\n5\n19\n1\n0\n18\n0.100\n0.900\n0.368\n\n\n\n\n\n\n\n\n\n\nPuede luego graficarse \\(S(t)\\) vs periodo (Figura 4). Sobre el gráfico se pueden ubicar los cuartiles y en particular, la mediana de los tiempos de supervivencia, que indica el momento donde la supervivencia es del 50%.\n\n\n\n\n\n\n\n\nFigura 4: Función de supervivencia estimada a partir de la observación de 300 personas post transplante hepático.\n\n\n\n\n\n\n\nMétodo de Kaplan-Meier\nEl método de Kaplan-Meier (KM) utiliza una lógica similar al actuarial, pero permite trabajar con datos individuales y periodos más cortos, siendo particularmente útil cuando el tamaño muestral es reducido. La probabilidad de supervivencia al tiempo \\(t\\) se estima considerando que es independiente de la supervivencia a otros tiempos, entonces la probabilidad de alcanzar el tiempo \\(t\\) es el producto de alcanzar los tiempos anteriores:\n\\[S_{KM}(t)=\\prod_{i:ti&lt;t}(1-d_i/n_1) \\]\ndonde \\(d_i\\) es el número de eventos y \\(n_i\\) el número de personas expuestas en el tiempo \\(t_i\\).\nLa característica distintiva es que la proporción acumulada que sobrevive se calcula para el tiempo de supervivencia individual de cada paciente y no se agrupan los tiempos de supervivencia en intervalos.\nAntes de continuar con el ejemplo se sugiere ver la siguiente videoclase:\n➡️ Método de Kaplan-Meier\nPara nuestro ejemplo consideraremos una cohorte cerrada de 10 pacientes con dos años de seguimiento. El evento es muerte por enfermedad cardiovascular (ECV). En la siguiente figura se muestra el esquema del seguimiento, donde “C” indica censura y “M” el evento:\n\n\n\n\n\n\n\n\nFigura 5: Cohorte cerrada de 10 pacientes en seguimiento.\n\n\n\n\n\nLa información de esta figura se resume en la Tabla 3. Es importante señalar que la base de datos para análisis de supervivencia debe tener la siguiente estructura: cada fila es un individuo en seguimiento, debe consignarse el tiempo inicial y el tiempo final del seguimiento (en este caso, el tiempo inicial es el mismo para todos porque se trata de una cohorte cerrada, pero esto no es lo más frecuente), el tiempo de seguimiento \\(t\\) (se obtiene de la diferencia tiempo final y tiempo inicial) y la variable status, que indica si el paciente sufrió el evento (status:1) o es un paciente censurado (status:0).\n\n\n\n\nTabla 3: Datos de 10 pacientes en seguimiento para ECV.\n\n\n\n\n\n\nPaciente\nTiempo inicial\nTiempo final\nt (meses)\nstatus\n\n\n\n\n1\n0\n1\n1\n1\n\n\n2\n0\n17\n17\n1\n\n\n3\n0\n20\n20\n1\n\n\n4\n0\n9\n9\n1\n\n\n5\n0\n24\n24\n0\n\n\n6\n0\n16\n16\n0\n\n\n7\n0\n2\n2\n0\n\n\n8\n0\n13\n13\n1\n\n\n9\n0\n10\n10\n0\n\n\n10\n0\n3\n3\n1\n\n\n\n\n\n\n\n\n\n\nPara la construcción de la tabla de supervivencia, lo primero que debemos hacer es ordenar los pacientes de acuerdo a su tiempo.\nEntonces, la Tabla 3, se transforma en la Tabla 4:\n\n\n\n\nTabla 4: Pacientes en seguimientos por ECV, ordenados según tiempo de seguimiento.\n\n\n\n\n\n\nPaciente\ntiempo\nstatus\n\n\n\n\n1\n1\n1\n\n\n7\n2\n0\n\n\n10\n3\n1\n\n\n4\n9\n1\n\n\n9\n10\n0\n\n\n8\n13\n1\n\n\n6\n16\n0\n\n\n2\n17\n1\n\n\n3\n20\n1\n\n\n5\n24\n0\n\n\n\n\n\n\n\n\n\n\nComencemos con la construcción de la tabla de supervivencia (Tabla 5), teniendo en cuenta que aquí conocemos exactamente el tiempo en el que un individuo estuvo en riesgo de morir por ECV. Esta tabla se construye a partir de los individuos que tuvieron el evento.\n\n\n\n\nTabla 5: Tabla de supervivencia según KM para pacientes con ECV.\n\n\n\n\n\n\nTiempo\nIndividuos en riesgo\nEventos\nProb. evento\nProb. supervivencia\nS(t)\n\n\n\n\n0\n10\n0\n0.000\n1.000\n1.000\n\n\n1\n10\n1\n0.100\n0.900\n0.900\n\n\n3\n8\n1\n0.125\n0.875\n0.787\n\n\n9\n7\n1\n0.143\n0.857\n0.675\n\n\n13\n5\n1\n0.200\n0.800\n0.540\n\n\n17\n3\n1\n0.333\n0.667\n0.360\n\n\n20\n2\n1\n0.500\n0.500\n0.180\n\n\n\n\n\n\n\n\n\n\nUna vez obtenida la \\(S(t)\\), se grafica:\n\n\n\n\n\n\n\n\nFigura 6: Curva de supervivencia estimada por Kaplan-Meier.\n\n\n\n\n\nLas curvas de supervivencia de Kaplan-Meier son representaciones escalonadas que describen cómo evoluciona la supervivencia acumulada a lo largo del tiempo. Cada descenso en la curva —es decir, cada “escalón”— se produce cuando ocurre un evento (por ejemplo, una muerte), lo que refleja una disminución en la probabilidad de supervivencia acumulada hasta ese instante.\nResumiendo, para construir una curva de supervivencia se deben dar los siguientes pasos:\n\nOrdenar los tiempos de supervivencia (o tiempos de observación) en forma ascendente.\nConstruir una tabla que contenga: una columna con los tiempos observados para cada participante (\\(t_i\\)) y una columna con el estado del individuo al final del seguimiento (status).\nCalcular la probabilidad de supervivencia en cada tiempo (\\(s_i / n_i\\)), es decir, el cociente entre el número de individuos que sobreviven (\\(s_i\\)) y el número de individuos en riesgo de sufrir el evento en ese instante (\\(n_i\\)). Esta columna sólo tendrá valores en los momentos en que ocurre un evento. Es importante señalar que, en el denominador (\\(n_i\\)), se incluyen también los individuos que van a experimentar el evento en ese período, ya que entran al periodo vivos.\nObtener la supervivencia acumulada, multiplicando en cada periodo de tiempo los cocientes de supervivencia (\\(s_i\\)/\\(n_i\\)) por los productos acumulados de los tiempos anteriores.\nRepresentar gráficamente la curva. La línea comienza en 1 (100% de supervivencia) y se mantiene constante hasta que ocurre el primer evento, momento en el que desciende un escalón. Este patrón se repite a medida que ocurren nuevos eventos. Por ejemplo, si la supervivencia cae a 0,90 tras el primer evento, ese valor se mantendrá hasta el siguiente.\n\nAlgunos investigadores optan por representar también curvas de incidencia acumulada, que refleja la probabilidad de experimentar el evento a lo largo del tiempo. Esta también puede calcularse a partir de la tabla de vida generada mediante el método de Kaplan-Meier. Por ejemplo, en la Tabla 5, bastaría con usar la columna “Probabilidad del evento” y obtener la probabilidad acumulada multiplicando las probabilidades de distintos tiempos.\nHasta aquí aprendimos a calcular la \\(S(t)\\) a partir de nuestros datos, ya sea por el método actuarial o por el de KM. Todo esto que hicimos “a mano” aquí, las funciones del lenguaje R lo hacen y nos facilitan el trabajo.\nAhora podríamos plantearnos distintos interrogantes, según el estudio, como por ejemplo:\n\n¿La curva será una buena estimación tanto para hombres como para mujeres? Es decir: ¿la variable sexo afecta al tiempo de supervivencia?\nO también: ¿el tratamiento afecta el tiempo de supervivencia? Para ello será necesario comparar curvas de supervivencia.\n\nResponder estas preguntas requiere comparar curvas de supervivencia entre grupos, lo cual veremos a continuación."
  },
  {
    "objectID": "unidad_6/03_metodos_analisis_sup.html#comparación-de-curvas-de-supervivencia",
    "href": "unidad_6/03_metodos_analisis_sup.html#comparación-de-curvas-de-supervivencia",
    "title": "Métodos para el análisis de supervivencia",
    "section": "Comparación de curvas de supervivencia",
    "text": "Comparación de curvas de supervivencia\nPara responder a las preguntas anteriores, la estrategia de KM es estratificar según la variable de interés (sexo en el primer ejemplo, tratamiento en el segundo) y estimar la \\(S(t)\\) para cada estrato. De esta forma podemos comparar las curvas gráficamente.\nPor ejemplo, la siguiente figura, nos muestra las curvas de supervivencia para dos grupos de pacientes que recibieron distintos tratamientos (A y B). Si observamos las medianas de los tiempos de supervivencia (líneas punteadas), podemos ver que los pacientes que siguieron el tratamiento \\(B\\) tienen mayores tiempos de supervivencia que los que recibieron el tratamiento \\(A\\) (Mediana tiempos de supervivencia \\(A \\approx 7 \\;meses\\); Mediana tiempos de supervivencia \\(B \\approx 20 \\;meses\\))\n\n\n\n\n\n\n\n\nFigura 7: Curvas de KM estratificadas por tipo de tratamiento.\n\n\n\n\n\nSi bien gráficamente podemos tener una idea, lo correcto es hacer una comparación en términos estadísticos. Por lo tanto, existen varios test que nos permiten comparar curvas de supervivencia. Uno de los más usados por los paquetes estadísticos es el log-rank test.\n\nLog-rank test\n\nLa hipótesis nula (\\(H_0\\)) plantea que las supervivencias de los grupos que se comparan (2 o más) son iguales.\nLa hipótesis alternativa (\\(H_1\\)) sostiene que al menos uno de los grupos tiene una supervivencia diferente.\n\nEl estadístico de contraste utilizado es la chi-cuadrado (\\(\\chi^2\\)) con \\(k-1\\) grados de libertad, donde \\(k\\) representa la cantidad de grupos (es decir, el número de curvas que se comparan). Esta prueba evalúa si la incidencia de eventos es similar en los distintos estratos.\nLa interpretación del valor \\(p\\) es similar a la de otros tests de hipótesis: típicamente, un valor \\(p &lt; 0,05\\) se considera evidencia suficiente para rechazar la hipótesis nula, indicando diferencias estadísticamente significativas entre los grupos.\nEn la práctica médica, es frecuente comparar tratamientos o cohortes utilizando como criterio la supervivencia a cinco años. Sin embargo, esta estrategia puede ser engañosa. Es posible que dos curvas muy diferentes reflejen la misma supervivencia en ese único punto temporal. La Figura 8 ilustra claramente este problema.\n\n\n\n\n\n\n\n\nFigura 8: Curvas de supervivencia que exhiben la misma supervivencia a cinco años para diferentes estudios.\n\n\n\n\n\nEn la Figura 8, ambos grupos presentan una supervivencia del 50% a los 5 años. No obstante, se advierte que el grupo B experimenta una caída más temprana en la probabilidad de supervivencia, lo que indica un peor pronóstico desde los primeros momentos del seguimiento.\nLos tests diseñados específicamente para comparar curvas de supervivencia, como el log-rank, consideran todo el trayecto temporal de las curvas, y no únicamente un punto aislado. Esto permite detectar diferencias persistentes en el tiempo.\nEl test de log-rank (también conocido como test de Mantel-Haenszel) es el más adecuado cuando el evento es poco frecuente o cuando las curvas son aproximadamente paralelas.\nSi las curvas se cruzan en el tiempo —por ejemplo, si un grupo presenta mejor supervivencia inicialmente, pero luego el otro lo supera— conviene emplear el test de Wilcoxon (o de Breslow), que otorga mayor peso a las diferencias tempranas. Un tercer método menos utilizado es el test de Tarone-Ware, que aplica una ponderación intermedia donde se le da distinto peso a las diferencias según ocurran más precoz o más tardíamente a lo largo del seguimiento."
  },
  {
    "objectID": "unidad_6/03_metodos_analisis_sup.html#hazard-y-riesgo-acumulado",
    "href": "unidad_6/03_metodos_analisis_sup.html#hazard-y-riesgo-acumulado",
    "title": "Métodos para el análisis de supervivencia",
    "section": "Hazard y Riesgo acumulado",
    "text": "Hazard y Riesgo acumulado\nYa hemos estimado \\(S(t)\\), ahora nos falta abordar las otras funciones básicas de supervivencia\nLa expresión compleja de \\(h(t)\\), a los efectos de las Tablas de Supervivencia, puede reemplazarse por:\n\\[\\lambda(t)=\\frac{Número \\; de \\; \\;eventos\\;observados\\;en\\;cada\\;intervalo}{personas\\;en\\;riesgo\\;al\\;inicio*amplitud\\;intervalo}\\]\nDe la misma forma que construimos \\(S(t)\\), podemos estimar \\(h(t)\\) (o un software puede hacerlo por nosotros). La Tabla 6, nos muestra las estimaciones para el ejemplo utilizado para KM.\n\n\n\n\nTabla 6: Estimaciones de h(t).\n\n\n\n\n\n\nTiempo\nIndividuos en riesgo\nEventos\nProb. evento\nProb. supervivencia\nlambda(t)\n\n\n\n\n0\n10\n0\n0.000\n1.000\n0.000\n\n\n1\n10\n1\n0.100\n0.900\n0.100\n\n\n2\n8\n1\n0.125\n0.875\n0.625\n\n\n6\n7\n1\n0.143\n0.857\n0.238\n\n\n\n\n\n\n\n\n\n\n¿Qué expresa \\(h(t)\\) o \\(\\lambda(t)\\)?\nObservemos que la función de riesgo \\(h(t)\\) o \\(\\lambda(t)\\) es una medida de la probabilidad de que se produzca el suceso estudiado entre los que quedan sin haber sufrido todavía tal suceso.\nEs realmente una función que evalúa, puntualmente, en un período de tiempo determinado, la probabilidad de que un individuo de los que todavía no han sufrido tal suceso lo sufra precisamente en ese período de tiempo. Es, por lo tanto, realmente, una función que mide el riesgo en un período de tiempo concreto.\nDe forma similar, la expresión para el riesgo acumulado:\n\\[\nH(t)=\\int_0^t h(u)du\n\\]\npuede aproximarse como:\n\\[\n\\Lambda(t)=\\sum\\lambda(t)*intervalo\n\\] No mostramos aquí los cálculos correspondientes para no agobiarlos más, pero pueden intentarlo si están motivados."
  },
  {
    "objectID": "unidad_6/03_metodos_analisis_sup.html#intervalos-de-confianza-y-error-estándar-para-la-supervivencia",
    "href": "unidad_6/03_metodos_analisis_sup.html#intervalos-de-confianza-y-error-estándar-para-la-supervivencia",
    "title": "Métodos para el análisis de supervivencia",
    "section": "Intervalos de confianza y error estándar para la supervivencia",
    "text": "Intervalos de confianza y error estándar para la supervivencia\nSi se desea calcular un intervalo de confianza (IC) para la probabilidad de supervivencia estimada en un tiempo determinado, es necesario considerar su error estándar (SE). El mismo se calcula en función de la supervivencia acumulada hasta ese momento, y representa la incertidumbre asociada a la estimación.\nUna fórmula común para el error estándar de la estimación de supervivencia acumulada (\\(S_t\\)) en un tiempo dado es el producto de la supervivencia estimada para ese tiempo por la raíz de la suma de los cocientes entre el número de fallecidos en cada momento y el producto de supervivientes y pacientes en riesgo en ese tiempo:\n\\[\nSE_{S_t}=S_t\\sqrt{\\sum\\frac{n_i-s_i}{n_is_i}}\n\\]\nPara calcular el 95% IC, podríamos pensar en usar la fórmula que vimos inicialmente:\n\\[\n95\\%~IC_{S_t} =  S_t \\pm 1,96 SE \\qquad donde~1,96~es~el~valor~Z~para~un~alfa~bilateral~del~5\\%\n\\]\nCon esta fórmula, en algunos casos se podría obtener, un límite inferior negativo o un límite superior mayor que 1. Ambos resultados carecen de sentido en un contexto de probabilidades.\nPara evitar estos problemas, se utiliza una transformación logarítmica que estabiliza la varianza. La fórmula ajustada para el error estándar transformado es:\n\\[\nSE_t=\\sqrt{\\frac{1}{(ln[S])^2}\\times\\sum\\frac{n_i-s_i}{n_is_i}}\n\\]\nY a partir de este \\(SE_t\\), calcular el intervalo de confianza al 95%, según la expresión:\n\\[\n95\\%~IC = SE_t * exp(\\pm1,96 SE_t)\n\\]\nAunque todos estos cálculos pueden realizarse manualmente, las funciones del lenguaje R ya los incluyen al utilizar modelos de supervivencia. El propósito aquí es mostrar la lógica matemática detrás de dichas funciones."
  },
  {
    "objectID": "unidad_6/03_metodos_analisis_sup.html#algunas-otras-medidas-de-supervivencia",
    "href": "unidad_6/03_metodos_analisis_sup.html#algunas-otras-medidas-de-supervivencia",
    "title": "Métodos para el análisis de supervivencia",
    "section": "Algunas otras medidas de supervivencia",
    "text": "Algunas otras medidas de supervivencia\nEn la literatura científica, muchas veces nos encontramos con otras medidas de supervivencia, que puntualizaremos a continuación:\n\nSupervivencia a 5 años: es el número de personas aún vivas después de 5 años del diagnóstico.\nSupervivencia media: tener en cuenta sólo aquellos individuos de los que se tienen datos completos y se conoce con exactitud su situación clínica. Esta se calcula por:\n\n\\[\nS \\;media=\\frac{\\sum tiempo\\;hasta\\;alcanzar\\;el\\;evento}{numero\\;total\\;de\\;individuos\\;que\\;tuvieron\\;el\\;evento}\n\\]\n\nTasa de supervivencia: Se obtiene según la fórmula\n\n\\[\nTasa\\;de\\;supervivencia= \\frac{nro.\\;de\\;individuos\\;que\\;sobreviven\\;hasta\\;un\\;tiempo\\;t}{nro.\\;total\\;de\\;individuos}  \n\\]\n\nTasa de Incidencia: Puede calcularse también una tasa de incidencia del evento de interés\n\n\\[\nTasa\\;de\\;incidencia=\\frac{numero\\; de\\; eventos}{\\sum tiempo-persona \\; en\\;seguimiento}  \n\\]"
  },
  {
    "objectID": "unidad_6/03_metodos_analisis_sup.html#ejemplo-práctico-en-r",
    "href": "unidad_6/03_metodos_analisis_sup.html#ejemplo-práctico-en-r",
    "title": "Métodos para el análisis de supervivencia",
    "section": "Ejemplo práctico en R",
    "text": "Ejemplo práctico en R\nA continuación, veremos un ejemplo para aprender a realizar un análisis de supervivencia en R, utilizando datos de pacientes con leucemia mieloide crónica (LMC) almacenados en el archivo “tmo.txt”.\nHasta hace pocos años, el trasplante de médula ósea (TMO) a partir de un donante compatible era el único tratamiento disponible para la LMC. Sin embargo, este procedimiento presenta diversos desafíos: una de las complicaciones más frecuentes es la enfermedad injerto contra huésped, que afecta entre el 25 y el 30 % de los pacientes trasplantados. También pueden presentarse efectos adversos vinculados a tratamientos previos como la quimioterapia o la radioterapia. El pronóstico de los pacientes sometidos a TMO se asocia además, con la fase de la enfermedad en el momento del trasplante: aquellos trasplantados durante la fase crónica inicial suelen presentar una mayor probabilidad de supervivencia.\nEl Centro de Trasplante de Médula Ósea del Instituto Nacional del Cáncer de Brasil, entre 1986 y 1998 realizó 96 trasplantes de médula ósea para tratamiento de LMC. El análisis de los datos de esta cohorte tuvo como objetivo identificar factores que pudieran estar relacionados con el pronóstico, ya sea para orientar intervenciones profilácticas o terapéuticas que mejoren los resultados, o para evaluar la conveniencia de alternativas terapéuticas en subgrupos de alto riesgo.\nExisten numerosas librerías para el análisis de supervivencia, nosotros aquí usaremos los paquetes survival (Therneau 2024) y survminer(Kassambara, Kosinski, y Biecek 2024), este último provee funciones que facilitan el análisis y la visualización de datos de supervivencia. Comenzamos cargando los paquetes:\n\nlibrary(survival)\nlibrary(survminer)\nlibrary(tidyverse)\n\nA continuación cargamos los datos y exploramos su estructura:\n\ndatos &lt;- read_csv2(\"datos/tmo.txt\")\n\nLas variables presentes en el conjunto de datos son las siguientes:\n\nid: identificador único del paciente\nsexo: sexo biológico (M = masculino, F = femenino)\nedad: edad en años al momento del trasplante\nstatus: estado al final del seguimiento (1 = fallecido, 0 = censurado)\ntiempo: tiempo hasta la fecha de óbito o censura\ndeag: presencia de enfermedad injerto-huesped aguda\ndecr: presencia de enfermedad injerto-huesped crónica\nfase: fase de la LMC (aguda, crónica, crisis blástica)\n\nEl evento de interés es “muerte pos-trasplante”. Comenzaremos por utilizar la función Surv(), que toma como argumentos time y event y devuelve un objeto de supervivencia que se utiliza como insumo para los análisis posteriores. La función survfit() toma ese objeto y construye las curvas de supervivencia correspondientes.\nNos centraremos en la construcción de objetos de supervivencia a partir de datos con censura a la derecha, ya que es el tipo más común en estudios longitudinales. En este tipo de censura, solo necesitamos dos argumentos para Surv(). Para explorar otras formas de censura (por ejemplo, censura por intervalo o truncamiento), se recomienda consultar la documentación del paquete survival.\nGeneramos el objeto de supervivencia usando las variables tiempo y status:\n\nglobal &lt;- Surv(time = datos$tiempo, event = datos$status)\n\n# Explorar el objeto de supervivencia\nglobal\n\n [1] 1000+   39   434    69   672    98  1000+  415   261    65  1000+ 1000+\n[13] 1000+  347   453   281  1000+  185  1000+   79  1000+ 1000+  475  1000+\n[25]  100   313  1000+  128   427    84   214   200   120    83  1000+  149 \n[37] 1000+   70  1000+ 1000+   54   101   162   216    63  1000+ 1000+ 1000+\n[49] 1000+ 1000+  571+   71  1000+   31   907+  210    32   865+  139   980+\n[61]  320+   78    40   753+  846+  774+  522    48   587+   76   468+  434 \n[73]  452+  487+  586   536+   89+  425   531+  371   489+  445+  243+  362+\n[85]  370   263   383+   74   229+  236+  104   110+   54+   54+   32   342+\n\n\nAl llamar el objeto, veremos que algunas observaciones aparecen con el símbolo +, indicando que han sido censuradas a la derecha.\nLa función survfit() crea curvas de supervivencia a partir de una fórmula. A continuación, ajustaremos un modelo Kaplan-Meier para estimar la curva de supervivencia global:\n\nKM &lt;- survfit(global ~ 1)\n\n# Explorar el modelo de supervivencia\nKM\n\nCall: survfit(formula = global ~ 1)\n\n      n events median 0.95LCL 0.95UCL\n[1,] 96     49    453     370      NA\n\n\nAlgunos de los argumentos opcionales de la función son los siguientes:\n\nconf.int: permite modificar el nivel de confianza (por ejemplo, conf.int = 0.90)\nconf.type: define el método de construcción del intervalo:\n\n\"log\" (por defecto): usa transformación logarítmica: \\(g(t) = log(t)\\)\n\"log-log\": usa \\(g(t) = log(-log(t))\\)\n\"plain\": intervalo lineal sin transformación.\n\n\nEl objeto KM contiene información detallada: número de observaciones, cantidad de eventos observados, mediana del tiempo de supervivencia y su intervalo de confianza. Además, al tratarse de un objeto de tipo lista, podemos acceder a información adicional con los siguiente comandos:\n\n# devuelve las estimaciones de Kaplan-Meier a cada t_i\nKM$surv\n\n [1] 0.9895833 0.9687500 0.9583333 0.9479167 0.9375000 0.9270833 0.9164272\n [8] 0.9057711 0.8951149 0.8844588 0.8738027 0.8631466 0.8524904 0.8418343\n[15] 0.8311782 0.8205220 0.8098659 0.8098659 0.7990677 0.7882695 0.7774713\n[22] 0.7666731 0.7666731 0.7557206 0.7447681 0.7338156 0.7228632 0.7119107\n[29] 0.7009582 0.6900057 0.6790533 0.6681008 0.6571483 0.6571483 0.6571483\n[36] 0.6571483 0.6456194 0.6340905 0.6225616 0.6110327 0.6110327 0.6110327\n[43] 0.5990516 0.5990516 0.5868261 0.5746005 0.5746005 0.5621092 0.5496179\n[50] 0.5371266 0.5121440 0.5121440 0.5121440 0.4990121 0.4990121 0.4855252\n[57] 0.4855252 0.4855252 0.4712451 0.4712451 0.4712451 0.4712451 0.4555369\n[64] 0.4555369 0.4392677 0.4392677 0.4392677 0.4392677 0.4392677 0.4392677\n[71] 0.4392677 0.4392677\n\n# {t_i}\nKM$time \n\n [1]   31   32   39   40   48   54   63   65   69   70   71   74   76   78   79\n[16]   83   84   89   98  100  101  104  110  120  128  139  149  162  185  200\n[31]  210  214  216  229  236  243  261  263  281  313  320  342  347  362  370\n[46]  371  383  415  425  427  434  445  452  453  468  475  487  489  522  531\n[61]  536  571  586  587  672  753  774  846  865  907  980 1000\n\n# {Y_i}\nKM$n.risk \n\n [1] 96 95 93 92 91 90 87 86 85 84 83 82 81 80 79 78 77 76 75 74 73 72 71 70 69\n[26] 68 67 66 65 64 63 62 61 60 59 58 57 56 55 54 53 52 51 50 49 48 47 46 45 44\n[51] 43 41 40 39 38 37 36 35 34 33 32 31 30 29 28 27 26 25 24 23 22 21\n\n# {d_i}\nKM$n.event \n\n [1] 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0 0 0 1 1\n[39] 1 1 0 0 1 0 1 1 0 1 1 1 2 0 0 1 0 1 0 0 1 0 0 0 1 0 1 0 0 0 0 0 0 0\n\n# error estándar de las estimaciones de  K-M a {t_i}\nKM$std.err \n\n [1] 0.01047135 0.01833089 0.02128141 0.02392372 0.02635231 0.02862321\n [7] 0.03086977 0.03301123 0.03506847 0.03705709 0.03898920 0.04087437\n[13] 0.04272043 0.04453385 0.04632006 0.04808377 0.04982901 0.04982901\n[19] 0.05160533 0.05336878 0.05512246 0.05686918 0.05686918 0.05866126\n[25] 0.06045058 0.06223956 0.06403050 0.06582557 0.06762684 0.06943634\n[31] 0.07125603 0.07308784 0.07493368 0.07493368 0.07493368 0.07493368\n[37] 0.07699571 0.07907601 0.08117706 0.08330139 0.08330139 0.08330139\n[43] 0.08562288 0.08562288 0.08807070 0.09055225 0.09055225 0.09318156\n[49] 0.09585329 0.09857177 0.10416729 0.10416729 0.10416729 0.10735729\n[55] 0.10735729 0.11079864 0.11079864 0.11079864 0.11475018 0.11475018\n[61] 0.11475018 0.11475018 0.11965379 0.11965379 0.12505911 0.12505911\n[67] 0.12505911 0.12505911 0.12505911 0.12505911 0.12505911 0.12505911\n\n# estimaciones puntuales inferiores (alternativamente, $upper)\nKM$lower\n\n [1] 0.9694806 0.9345627 0.9191827 0.9044952 0.8903077 0.8765055 0.8626241\n [8] 0.8490226 0.8356578 0.8224974 0.8095165 0.7966952 0.7840176 0.7714705\n[15] 0.7590431 0.7467260 0.7345115 0.7345115 0.7221992 0.7099816 0.6978531\n[22] 0.6858088 0.6858088 0.6736413 0.6615542 0.6495440 0.6376072 0.6257411\n[29] 0.6139430 0.6022106 0.5905417 0.5789346 0.5673875 0.5673875 0.5673875\n[36] 0.5673875 0.5551850 0.5430523 0.5309875 0.5189890 0.5189890 0.5189890\n[43] 0.5065029 0.5065029 0.4937914 0.4811581 0.4811581 0.4682787 0.4554811\n[50] 0.4427639 0.4175656 0.4175656 0.4175656 0.4043229 0.4043229 0.3907508\n[57] 0.3907508 0.3907508 0.3763321 0.3763321 0.3763321 0.3763321 0.3603082\n[64] 0.3603082 0.3437786 0.3437786 0.3437786 0.3437786 0.3437786 0.3437786\n[71] 0.3437786 0.3437786\n\n\nTambién puede obtenerse un resumen completo mediante:\n\nsummary(KM)\n\nCall: survfit(formula = global ~ 1)\n\n time n.risk n.event survival std.err lower 95% CI upper 95% CI\n   31     96       1    0.990  0.0104        0.969        1.000\n   32     95       2    0.969  0.0178        0.935        1.000\n   39     93       1    0.958  0.0204        0.919        0.999\n   40     92       1    0.948  0.0227        0.904        0.993\n   48     91       1    0.938  0.0247        0.890        0.987\n   54     90       1    0.927  0.0265        0.877        0.981\n   63     87       1    0.916  0.0283        0.863        0.974\n   65     86       1    0.906  0.0299        0.849        0.966\n   69     85       1    0.895  0.0314        0.836        0.959\n   70     84       1    0.884  0.0328        0.822        0.951\n   71     83       1    0.874  0.0341        0.810        0.943\n   74     82       1    0.863  0.0353        0.797        0.935\n   76     81       1    0.852  0.0364        0.784        0.927\n   78     80       1    0.842  0.0375        0.771        0.919\n   79     79       1    0.831  0.0385        0.759        0.910\n   83     78       1    0.821  0.0395        0.747        0.902\n   84     77       1    0.810  0.0404        0.735        0.893\n   98     75       1    0.799  0.0412        0.722        0.884\n  100     74       1    0.788  0.0421        0.710        0.875\n  101     73       1    0.777  0.0429        0.698        0.866\n  104     72       1    0.767  0.0436        0.686        0.857\n  120     70       1    0.756  0.0443        0.674        0.848\n  128     69       1    0.745  0.0450        0.662        0.838\n  139     68       1    0.734  0.0457        0.650        0.829\n  149     67       1    0.723  0.0463        0.638        0.820\n  162     66       1    0.712  0.0469        0.626        0.810\n  185     65       1    0.701  0.0474        0.614        0.800\n  200     64       1    0.690  0.0479        0.602        0.791\n  210     63       1    0.679  0.0484        0.591        0.781\n  214     62       1    0.668  0.0488        0.579        0.771\n  216     61       1    0.657  0.0492        0.567        0.761\n  261     57       1    0.646  0.0497        0.555        0.751\n  263     56       1    0.634  0.0501        0.543        0.740\n  281     55       1    0.623  0.0505        0.531        0.730\n  313     54       1    0.611  0.0509        0.519        0.719\n  347     51       1    0.599  0.0513        0.507        0.709\n  370     49       1    0.587  0.0517        0.494        0.697\n  371     48       1    0.575  0.0520        0.481        0.686\n  415     46       1    0.562  0.0524        0.468        0.675\n  425     45       1    0.550  0.0527        0.455        0.663\n  427     44       1    0.537  0.0529        0.443        0.652\n  434     43       2    0.512  0.0533        0.418        0.628\n  453     39       1    0.499  0.0536        0.404        0.616\n  475     37       1    0.486  0.0538        0.391        0.603\n  522     34       1    0.471  0.0541        0.376        0.590\n  586     30       1    0.456  0.0545        0.360        0.576\n  672     28       1    0.439  0.0549        0.344        0.561\n\n\nPara graficar la función de supervivencia usaremos la función ggsurvplot() del paquete survminer:\n\nggsurvplot(KM, data = datos)\n\n\n\n\n\n\n\n\nComo se esperaba, la gráfica nos muestra las probabilidades decrecientes de supervivencia a medida que pasa el tiempo. El sombreado alrededor de la curva representa el intervalo de confianza para cada estimación puntual. Más adelante exploraremos cómo personalizar estos gráficos para mejorar su presentación e interpretación.\n\nComparación de curvas de supervivencia\nIntentaremos ahora verificar los factores pronósticos para el tiempo de supervivencia pos-trasplante. Recordemos que el evento a considerar es “muerte pos-trasplante”. Como herramienta exploratoria, utilizaremos las curvas de supervivencia estimadas por el método de Kaplan Meier según las covariables: sexo, Enfermedad injerto crónica (decr), Enfermedad injerto aguda (deag) y fase. Esta parte es equivalente al análisis bivariado que realizamos en regresión lineal y logística. Recordemos que la variable que representa los tiempos de supervivencia es tiempo y la que representa el evento o la censura es status.\nComenzaremos evaluando las curvas de Kaplan Meier en forma visual, y las completaremos con la información del log-rank test, que nos permite comparar dos funciones de supervivencia.\nDefinimos la función de supervivencia para sexo, usando el objeto de supervivencia creado anteriormente:\n\nkmsex &lt;- survfit(global ~ sexo, data = datos)\n\nGraficamos la curva de supervivencia:\n\nggsurvplot(kmsex, data = datos)\n\n\n\n\n\n\n\n\nPara comparar ambas funciones de supervivencia se debe ejecutar la función survdiff():\n\nlogrank &lt;- survdiff(global ~ sexo, data = datos)\n\nlogrank\n\nCall:\nsurvdiff(formula = global ~ sexo, data = datos)\n\n        N Observed Expected (O-E)^2/E (O-E)^2/V\nsexo=F 39       17     19.4     0.305     0.508\nsexo=M 57       32     29.6     0.201     0.508\n\n Chisq= 0.5  on 1 degrees of freedom, p= 0.5 \n\n\nPodemos resumir toda la información en un único gráfico, e incluso sumarle información adicional, de acuerdo a cómo personalicemos dicho gráfico:\n\nggsurvplot(\n  kmsex, \n  data = datos, \n1  size = 1,\n2  palette = c(\"#A36267\", \"#515A79\"),\n3  conf.int = TRUE,\n4  pval = TRUE,\n5  risk.table = TRUE,\n6  risk.table.col = \"strata\",\n7  legend.labs = c(\"Femenino\", \"Masculino\"),\n8  risk.table.height = 0.25,\n9  ggtheme = theme_bw()\n)\n\n\n1\n\nCambia el grosor de la línea\n\n2\n\nPersonaliza las paletas de colores\n\n3\n\nAgrega intervalo de confianza\n\n4\n\nAgrega p-valor\n\n5\n\nAgrega tabla de riesgo\n\n6\n\nTabla de riesgos con diferente color según grupo\n\n7\n\nCambia etiqueta de la leyenda\n\n8\n\nÚtil para cambiar cuando se tienen múltiples grupos\n\n9\n\nCambia el tema de ggplot2\n\n\n\n\n\n\n\n\n\n\n\nHagamos lo mismo para las restantes variables del ejemplo:\nEnfermedad injerto aguda:\n\n# Función de supervivencia\nkmdeag &lt;- survfit(global ~ deag, data = datos)\n\n# Generar curvas de supervivencia\nggsurvplot(\n  kmdeag, \n  data = datos, \n  size = 1,                  \n  palette = c(\"#A36267\", \"#515A79\"),\n  conf.int = TRUE,          \n  pval = TRUE,              \n  risk.table = TRUE,        \n  risk.table.col = \"strata\",\n  legend.labs = c(\"NO\", \"SI\"),    \n  risk.table.height = 0.25, \n  ggtheme = theme_bw()      \n)\n\n\n\n\n\n\n\n# Test de log-rank\nlogrank &lt;- survdiff(global ~ deag, data = datos)\n\nlogrank\n\nCall:\nsurvdiff(formula = global ~ deag, data = datos)\n\n         N Observed Expected (O-E)^2/E (O-E)^2/V\ndeag=no 61       23     36.2      4.79      18.8\ndeag=si 35       26     12.8     13.50      18.8\n\n Chisq= 18.8  on 1 degrees of freedom, p= 1e-05 \n\n\nEnfermedad injerto crónica:\n\n# Función de supervivencia\nkmdecr &lt;- survfit(global ~ decr, data = datos)\n\n# Generar curvas de supervivencia\nggsurvplot(\n  kmdecr, \n  data = datos, \n  size = 1,                  \n  palette = c(\"#A36267\", \"#515A79\"),\n  conf.int = TRUE,          \n  pval = TRUE,              \n  risk.table = TRUE,        \n  risk.table.col = \"strata\",\n  legend.labs = c(\"NO\", \"SI\"),    \n  risk.table.height = 0.25, \n  ggtheme = theme_bw()      \n)\n\n\n\n\n\n\n\n# Test de log-rank\nlogrank &lt;- survdiff(global ~ decr, data = datos)\n\nlogrank\n\nCall:\nsurvdiff(formula = global ~ decr, data = datos)\n\n         N Observed Expected (O-E)^2/E (O-E)^2/V\ndecr=no 45       31     15.7     14.94      22.6\ndecr=si 51       18     33.3      7.04      22.6\n\n Chisq= 22.6  on 1 degrees of freedom, p= 2e-06 \n\n\nFase de la leucemia:\n\n# Función de supervivencia\nkmfase &lt;- survfit(global ~ fase, data = datos)\n\n# Generar curvas de supervivencia\nggsurvplot(\n  kmfase, \n  data = datos, \n  size = 1,                  \n  palette = c(\"#A36267\", \"#515A79\", \"#E99973\"),\n  conf.int = TRUE,          \n  pval = TRUE,              \n  risk.table = TRUE,        \n  risk.table.col = \"strata\",\n  legend.labs = c(\"aguda\", \"cb\", \"crónica\"),    \n  risk.table.height = 0.25, \n  ggtheme = theme_bw()      \n)\n\n\n\n\n\n\n\n# Test de log-rank\nlogrank &lt;- survdiff(global ~ fase, data = datos)\n\nlogrank\n\nCall:\nsurvdiff(formula = global ~ fase, data = datos)\n\n              N Observed Expected (O-E)^2/E (O-E)^2/V\nfase=aguda   20       14     8.73      3.19       3.9\nfase=cb       6        6     1.50     13.49      14.3\nfase=crónica 70       29    38.77      2.46      12.0\n\n Chisq= 19.7  on 2 degrees of freedom, p= 5e-05 \n\n\n¿Qué podemos concluir a partir de las curvas de KM? Observando cada gráfico detenidamente, vemos que:\n\nPara la variable sexo, las curvas se superponen bastante. Es decir que parece razonable suponer que no existe diferencia en la supervivencia pos-trasplante de hombres y mujeres. Dado la forma de las curvas, parecería más correcto emplear el test de Wilcoxon. De todas formas, el valor de ambos test indica lo que ya habíamos predicho observando el gráfico: que no hay diferencia en las curvas de supervivencia entre hombres y mujeres.\nSe observa diferencia en las curvas de supervivencia de los pacientes según hayan padecido o no enfermedad injerto contra huésped aguda (deag), con menor supervivencia para quienes la padecieron.\nSe observa diferencia en las curvas de supervivencia de los pacientes según hayan padecido o no enfermedad injerto contra huésped crónica (decr), con menor supervivencia para quienes no la padecieron.\nSe observa diferencia en las curvas de supervivencia de los pacientes según la fase de la enfermedad en la que se encontraban al momento del trasplante. Observen que la variable fase tiene 3 categorías. El valor del log Rank test nos indica que hay diferencia en la supervivencia de las categorías, pero no nos dice si las 3 son distintas o si sólo una difiere de las otras dos. De la observación del gráfico podemos concluir que los pacientes que sufren crisis blástica (cb) tienen una supervivencia menor, de hecho aproximadamente a los 200 días fallecen todos.\n\nRecordamos que KM estratifica por variables cualitativas, por lo tanto edad no puede ser evaluada de esta forma. Podría recodificarse si, para el evento en cuestión, el marco teórico señalara alguna edad de corte. Volveremos sobre este ejemplo cuando abordemos el modelo de regresión de Cox.\n\n\nFunción de riesgo\nEn algunas situaciones, dependiendo del evento que estemos considerando, puede resultar más fácil de interpretar la función de riesgo en lugar de la función de supervivencia. En nuestro ejemplo, el evento considerado es el “tiempo hasta la muerte pos-trasplante”, y en este contexto la función de supervivencia se interpreta fácilmente.\nSin embargo, en otros escenarios —por ejemplo, cuando el evento es la aparición de una infección, una recaída o una complicación— la interpretación de la función de supervivencia puede no ser tan directa. En esos casos, puede ser más útil representar la función de riesgo acumulado, que muestra la acumulación del riesgo del evento a lo largo del tiempo.\nUna vez definidos los objetos de supervivencia, podemos utilizar la opción fun = \"cumhaz\" dentro de la función ggsurvplot() para graficar la función de riesgo acumulado:\n\nggsurvplot(\n  kmsex, \n  data = datos, \n  fun= \"cumhaz\",\n  size = 1,                  \n  palette = c(\"#A36267\", \"#515A79\"),\n  conf.int = TRUE,          \n  pval = TRUE,              \n  risk.table = TRUE,        \n  risk.table.col = \"strata\",\n  legend.labs = c(\"Femenino\", \"Masculino\"),    \n  risk.table.height = 0.25, \n  ggtheme = theme_bw()      \n)\n\n\n\n\n\n\n\n\nCon el argumento fun=\"cumhaz\" le indicamos a ggsurvplot() que grafique la función de riesgo acumulado. Este mismo procedimiento puede aplicarse a todos los objetos de supervivencia creados previamente.\n\nHernández-Ávila (2011)\nWoodward (2005)\nRoyo-Bordonada et al. (2009)\nEscuela Nacional de Sanidad (ENS). Instituto de Salud Carlos III. Ministerio de Ciencias e Innovación. Madrid (2009)"
  }
]